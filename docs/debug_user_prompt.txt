User question: How does language model computate?

Context:
Source: https://www.youtube.com/watch?v=hMEViRcF7o0
Content: {"id_": "2828c59d-de56-4757-995d-762affc15b03", "embedding": null, "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b4725299-3007-4d37-a4d4-57084d9bd646", "node_type": "4", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "ac9b80bd0b6f3398147387091bd45f29e388ddcb5744c3ad62826a953f6404cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "312a7c23-383e-4c34-94b0-bd73e848b015", "node_type": "1", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "5127c249671f80c292232049a4970c7e97368a493807d77d4ed3948cbfa6c406", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "576d605a-960d-43e3-8c32-37804371cdc6", "node_type": "1", "metadata": {}, "hash": "787d29076ab163c143efd1a4d2f83127c664c5c39742ecabf175092d9e901703", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Okay. You can easily show that existing language models as they are right now out there in the wild are are easily a you're easily able to drive them to be universal computers in a classical sense. Okay. ", "mimetype": "text/plain", "start_char_idx": 13603, "end_char_idx": 13807, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.584

Source: https://www.youtube.com/watch?v=hMEViRcF7o0
Content: {"id_": "bc5d4a35-2955-4ced-a561-b6ef5793d0af", "embedding": null, "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b4725299-3007-4d37-a4d4-57084d9bd646", "node_type": "4", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "ac9b80bd0b6f3398147387091bd45f29e388ddcb5744c3ad62826a953f6404cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "058195df-6cd7-4d04-aec0-0c07d7405177", "node_type": "1", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "2d6af4157d7164005fd1bf34f50b6348c7a7929e1ff8a4bcc72e60b8e929de4b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6376fdd2-a16e-4584-a8f5-46358b097314", "node_type": "1", "metadata": {}, "hash": "418cb870ad713644e253f3c4b40c0564e1d0da60e9aeacebc73a848e2bd4129e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Um, okay. So, part one, uh, language model as a computer. Uh, and this actually often strikes people as as sort of a strange thing to say because we all know where language modeling came from, right? It was just this you know endeavor to sort of ex you know capture human natural language behavior usually in the form of text which you know over the years wound up being kind of crystallized into this very simple idea of building a conditional probabilistic model that has a you know expresses a conditional distribution over the very next symbol or token or word or maybe a sentence uh given some finite uh length context to it. And you might ask yourself what does this have to do with a general purpose computer? uh and and to answer that I'm going to sort of share uh when this became clear to me uh and for me I started to see this connection between language models and computers right at the dawn of this uh so-called chain of thought uh phenomenon um okay so you all know the story but but I I just have to set something up here so at that time we were looking at these you know GSMAK problems these math word problems the kinds of word problems you got like in elementary school or junior high things like you know Janie got two apples from her mother twice as many bananas this vertical, how many fruit did you get? And you know, when we're trying to get the models to respond six okay um and then there was this kind of remarkable discovery that if you just simply took these questionans answer pairs say in GSMAK and just interposed between the question and answer just a natural language expressed semicoherent reasoning chain like Janie got twice as you know got two apples from her mother twice as many bananas from her uncle. How many fruit did she get? Well, she got 2 * 2 equals four bananas from her uncle plus two um fruit from her mother. That equals six fruit. The answer is six. So, we just simply had this reasoning chain between the question and the answer. And frankly, we were stunned. This this was like a step change in the performance of these models. They went from like teens, you know, accuracy up into the 60s up into the 70s almost overnight. Uh quite shocking. Okay. Um however that's not the real lesson. Um so the real lesson comes from actually understanding what were we doing before this. Okay. And what we were doing before this is good oldfashioned machine learning. You literally take the GSMAK data set and you treat it like you know like a supervised learning data set. You have a bunch of question answer pairs. You know here's a question here's an answer d and you wire up your big neural net or your language model and just by you know gradient descent on log likelihood of the answer. you just try to optimize the probability of eliciting the correct answer on the training set and then test on a hold out validation set. Okay. ", "mimetype": "text/plain", "start_char_idx": 2123, "end_char_idx": 5002, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.561

Source: https://www.youtube.com/watch?v=hMEViRcF7o0
Content: {"id_": "47ec3cf4-c489-418a-9aa2-62a616180262", "embedding": null, "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b4725299-3007-4d37-a4d4-57084d9bd646", "node_type": "4", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "ac9b80bd0b6f3398147387091bd45f29e388ddcb5744c3ad62826a953f6404cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1a7b07b2-2604-409c-92b4-15579bb5525c", "node_type": "1", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "96af285a16dd6499975ebd2889dc6e2f38123e71a61dca9537a609cee502d926", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9f486925-28c7-4602-8ccf-05d61589d892", "node_type": "1", "metadata": {}, "hash": "f1250eeedabe940c4915f4d85b7b599b9eca58f01816f25f01de7db0fed5c7cc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Okay? Well, guess what is the go-to solution in large language model reasoning to this day? Um I I'll come back to that but let me share sort of some outcome of this experiment. So you you can ask some of these simple sort of path reachability problems to language models. Um now it turns out I'm going to make it even simpler is that I can simplify the problems to embed some special structure. So it turns out in this instance I I I didn't tell you but I embedded a special structure which is there is a total linear ordering on the variables where it turns out that first variable x1 if you assume it has the ver the value true it will force the next variable to have some value and that will force the next variable and force the next variable in a linear chain that connects the start variable to the end. So you can solve this problem just by reading off the forced assignments one by one and it will answer the question for you. So that's what you see on the right here. It's just a sequence of forced assignments that ends up with forcing X6 to be false. Therefore, we know the answer to the question is no. All right. Now, uh what do the language models do with problems like this? And these are I've anonymized the models. Sorry. And this is actually these experiments are back in February, so it's a generation or two behind already. Anyway, but when you ask uh the models, you know, questions like this, they're pretty mediocre. They were about 70% accurate, which you know, chance is 50%. You know, it's something, but these aren't super hard problems. Okay, but look on the right. Okay, the interesting thing on the right is the right is showing the the response length, and it's really impressive. It's it's responding to these questions with like response lengths of 2,000 or more tokens. Okay? And you read the responses, it's like total walkabout. like it might catch a few of the forced assignments and then it's just trying stuff, making mistakes, backing up, I don't know, just meandering and then 70% of the time landing on a correct answer and, you know, 30% of the time landing on an incorrect answer. It's it's kind of weird. ", "mimetype": "text/plain", "start_char_idx": 42389, "end_char_idx": 44540, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.529

Source: https://www.youtube.com/watch?v=hMEViRcF7o0
Content: {"id_": "0fd62a80-1d98-4cbc-9e30-ca941743dee2", "embedding": null, "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b4725299-3007-4d37-a4d4-57084d9bd646", "node_type": "4", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "ac9b80bd0b6f3398147387091bd45f29e388ddcb5744c3ad62826a953f6404cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "680d6f6f-6a59-4229-9a8e-7dff243cb135", "node_type": "1", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "9b20fa2a03d0897d3b37215023f07a1f45b989072cabe38dea5f80f8ed0741c8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e48f1fd0-e0d5-47f7-9b13-f3d5cd7e73bf", "node_type": "1", "metadata": {}, "hash": "7f50bc79bf67e4be5ead4f94e670f9787784a41a25f46749be840cd79bd0fc34", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This is good news. it means there'll always be jobs well at least for language models. Um okay so uh so where I want to go from here is is I just want to share a little bit more you know down to earth eventually um some of the challenges that I've been trying to face and kind of overcome and trying to you know get these models to elicit more and more trustable more and more reliable formal system behaviors. Okay. Uh so here is the first lesson I learned the hard way which is um if you're thinking about something it's like very concretely like a logical reasoning system. When you think about logic what you're interested in is a you know given a set of premises or assumptions a sequence of conclusions uh and there's a a sense in which we want that sequence of conclusions to be logically correct right to correctly follow from the premises. Um the logical correctness of a reasoning chain it turns out has nothing to do in principle with whether or not the reasoning chain is algorithmically realizable. That is algorithmic realizability and logical correctness correctness are just orthogonal concepts. Okay. And and by not sort of you know being aware of this you can get yourself in trouble. So um here's okay actually here's another way to explain it. So certainly uh you know uh uh for example right if I can prove certain programs correct right so I can give you a training set of here's a program and here's an argument that proves that program is correct and here's another program here's an argument that proves that program is correct I can give you a finite set of this these things and you could try to train your machine learning model to prove correctness of new programs okay the upshot of this is it will never work why will it never work because that is an undecidable property Right? Proving correctness of arbitrary programs is not something that can be decided in general. You will have to restrict like formal verification has must have limited scope. Okay. But you will never see this as a machine learning researcher. All you're going to see is I have a data set. I have my ambition. I fit my model to that data. Awesome on the training data. And it's like ah, you know, it's failing out of distribution and again and again, right? And this will never stop. it cannot work. You will always fail out of distribution this way. Okay. ", "mimetype": "text/plain", "start_char_idx": 37570, "end_char_idx": 39932, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.489

Source: https://www.youtube.com/watch?v=hMEViRcF7o0
Content: {"id_": "1761997e-a602-4bd2-927f-120ca2c90e72", "embedding": null, "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b4725299-3007-4d37-a4d4-57084d9bd646", "node_type": "4", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "ac9b80bd0b6f3398147387091bd45f29e388ddcb5744c3ad62826a953f6404cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f42e086e-2d30-4367-a77a-2e81b7c2a097", "node_type": "1", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "02a55f21a2921b06f2a5f1fa9ee6073531dfdb5b992642cc5507bc92599f1441", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "680d6f6f-6a59-4229-9a8e-7dff243cb135", "node_type": "1", "metadata": {}, "hash": "8872caa19121900baa9892112ab22a440ecae5e3c4237379e81e503f75e58066", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "All right. So um now I want to move to the second part of the talk which is I want to start to pivot more to sort of a machine learning stance which is um at least play along. Imagine you're trying to like you know train the LLM or alter the weights in your language model to somehow be a more reliable execution engine or reasoning engine. Now first thing to observe is by doing so you are not changing the ultimate computational ability of the model. It was a universal computer at the start. Hopefully, it's still one at the end. Okay. It is not about changing the computational ability of the model. It is about changing the behavior of the model to kind of be more usable by us, to be more directable by us, to be more sort of programmable by us where you know the behavior it emits is kind of the behavior we were looking for or hoping for or was at least correct even if we don't understand the algorithm. Okay. ", "mimetype": "text/plain", "start_char_idx": 32543, "end_char_idx": 33462, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.464

Answer the question based on the context above.
{
  "success": true,
  "response": "The provided text is a summary of a conversation about the challenges and potential solutions in machine learning. The conversation touches upon the importance of understanding the underlying principles of reasoning and the potential pitfalls of over-reliance on logic. It also highlights the need for a more robust and reliable system for evaluating and verifying the correctness of machine learning models. The conversation also touches on the need to be aware of the limitations of machine learning and to consider the potential impact of changes in the underlying computational capabilities of the model.",
  "thread_id": "mkml",
  "generated_queries": [
    "language model computate"
  ],
  "retrieval_count": 5,
  "error": null,
  "project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9"
}
----------------------------------------------------------------

User question: How does language model computate?

Context:
Source: https://www.youtube.com/watch?v=hMEViRcF7o0
Content: {"id_": "2828c59d-de56-4757-995d-762affc15b03", "embedding": null, "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b4725299-3007-4d37-a4d4-57084d9bd646", "node_type": "4", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "ac9b80bd0b6f3398147387091bd45f29e388ddcb5744c3ad62826a953f6404cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "312a7c23-383e-4c34-94b0-bd73e848b015", "node_type": "1", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "5127c249671f80c292232049a4970c7e97368a493807d77d4ed3948cbfa6c406", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "576d605a-960d-43e3-8c32-37804371cdc6", "node_type": "1", "metadata": {}, "hash": "787d29076ab163c143efd1a4d2f83127c664c5c39742ecabf175092d9e901703", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Okay. You can easily show that existing language models as they are right now out there in the wild are are easily a you're easily able to drive them to be universal computers in a classical sense. Okay. ", "mimetype": "text/plain", "start_char_idx": 13603, "end_char_idx": 13807, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.584

Source: https://www.youtube.com/watch?v=hMEViRcF7o0
Content: {"id_": "bc5d4a35-2955-4ced-a561-b6ef5793d0af", "embedding": null, "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b4725299-3007-4d37-a4d4-57084d9bd646", "node_type": "4", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "ac9b80bd0b6f3398147387091bd45f29e388ddcb5744c3ad62826a953f6404cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "058195df-6cd7-4d04-aec0-0c07d7405177", "node_type": "1", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "2d6af4157d7164005fd1bf34f50b6348c7a7929e1ff8a4bcc72e60b8e929de4b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6376fdd2-a16e-4584-a8f5-46358b097314", "node_type": "1", "metadata": {}, "hash": "418cb870ad713644e253f3c4b40c0564e1d0da60e9aeacebc73a848e2bd4129e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Um, okay. So, part one, uh, language model as a computer. Uh, and this actually often strikes people as as sort of a strange thing to say because we all know where language modeling came from, right? It was just this you know endeavor to sort of ex you know capture human natural language behavior usually in the form of text which you know over the years wound up being kind of crystallized into this very simple idea of building a conditional probabilistic model that has a you know expresses a conditional distribution over the very next symbol or token or word or maybe a sentence uh given some finite uh length context to it. And you might ask yourself what does this have to do with a general purpose computer? uh and and to answer that I'm going to sort of share uh when this became clear to me uh and for me I started to see this connection between language models and computers right at the dawn of this uh so-called chain of thought uh phenomenon um okay so you all know the story but but I I just have to set something up here so at that time we were looking at these you know GSMAK problems these math word problems the kinds of word problems you got like in elementary school or junior high things like you know Janie got two apples from her mother twice as many bananas this vertical, how many fruit did you get? And you know, when we're trying to get the models to respond six okay um and then there was this kind of remarkable discovery that if you just simply took these questionans answer pairs say in GSMAK and just interposed between the question and answer just a natural language expressed semicoherent reasoning chain like Janie got twice as you know got two apples from her mother twice as many bananas from her uncle. How many fruit did she get? Well, she got 2 * 2 equals four bananas from her uncle plus two um fruit from her mother. That equals six fruit. The answer is six. So, we just simply had this reasoning chain between the question and the answer. And frankly, we were stunned. This this was like a step change in the performance of these models. They went from like teens, you know, accuracy up into the 60s up into the 70s almost overnight. Uh quite shocking. Okay. Um however that's not the real lesson. Um so the real lesson comes from actually understanding what were we doing before this. Okay. And what we were doing before this is good oldfashioned machine learning. You literally take the GSMAK data set and you treat it like you know like a supervised learning data set. You have a bunch of question answer pairs. You know here's a question here's an answer d and you wire up your big neural net or your language model and just by you know gradient descent on log likelihood of the answer. you just try to optimize the probability of eliciting the correct answer on the training set and then test on a hold out validation set. Okay. ", "mimetype": "text/plain", "start_char_idx": 2123, "end_char_idx": 5002, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.561

Source: https://www.youtube.com/watch?v=hMEViRcF7o0
Content: {"id_": "47ec3cf4-c489-418a-9aa2-62a616180262", "embedding": null, "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b4725299-3007-4d37-a4d4-57084d9bd646", "node_type": "4", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "ac9b80bd0b6f3398147387091bd45f29e388ddcb5744c3ad62826a953f6404cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1a7b07b2-2604-409c-92b4-15579bb5525c", "node_type": "1", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "96af285a16dd6499975ebd2889dc6e2f38123e71a61dca9537a609cee502d926", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9f486925-28c7-4602-8ccf-05d61589d892", "node_type": "1", "metadata": {}, "hash": "f1250eeedabe940c4915f4d85b7b599b9eca58f01816f25f01de7db0fed5c7cc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Okay? Well, guess what is the go-to solution in large language model reasoning to this day? Um I I'll come back to that but let me share sort of some outcome of this experiment. So you you can ask some of these simple sort of path reachability problems to language models. Um now it turns out I'm going to make it even simpler is that I can simplify the problems to embed some special structure. So it turns out in this instance I I I didn't tell you but I embedded a special structure which is there is a total linear ordering on the variables where it turns out that first variable x1 if you assume it has the ver the value true it will force the next variable to have some value and that will force the next variable and force the next variable in a linear chain that connects the start variable to the end. So you can solve this problem just by reading off the forced assignments one by one and it will answer the question for you. So that's what you see on the right here. It's just a sequence of forced assignments that ends up with forcing X6 to be false. Therefore, we know the answer to the question is no. All right. Now, uh what do the language models do with problems like this? And these are I've anonymized the models. Sorry. And this is actually these experiments are back in February, so it's a generation or two behind already. Anyway, but when you ask uh the models, you know, questions like this, they're pretty mediocre. They were about 70% accurate, which you know, chance is 50%. You know, it's something, but these aren't super hard problems. Okay, but look on the right. Okay, the interesting thing on the right is the right is showing the the response length, and it's really impressive. It's it's responding to these questions with like response lengths of 2,000 or more tokens. Okay? And you read the responses, it's like total walkabout. like it might catch a few of the forced assignments and then it's just trying stuff, making mistakes, backing up, I don't know, just meandering and then 70% of the time landing on a correct answer and, you know, 30% of the time landing on an incorrect answer. It's it's kind of weird. ", "mimetype": "text/plain", "start_char_idx": 42389, "end_char_idx": 44540, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.529

Source: https://www.youtube.com/watch?v=hMEViRcF7o0
Content: {"id_": "0fd62a80-1d98-4cbc-9e30-ca941743dee2", "embedding": null, "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b4725299-3007-4d37-a4d4-57084d9bd646", "node_type": "4", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "ac9b80bd0b6f3398147387091bd45f29e388ddcb5744c3ad62826a953f6404cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "680d6f6f-6a59-4229-9a8e-7dff243cb135", "node_type": "1", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "9b20fa2a03d0897d3b37215023f07a1f45b989072cabe38dea5f80f8ed0741c8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e48f1fd0-e0d5-47f7-9b13-f3d5cd7e73bf", "node_type": "1", "metadata": {}, "hash": "7f50bc79bf67e4be5ead4f94e670f9787784a41a25f46749be840cd79bd0fc34", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This is good news. it means there'll always be jobs well at least for language models. Um okay so uh so where I want to go from here is is I just want to share a little bit more you know down to earth eventually um some of the challenges that I've been trying to face and kind of overcome and trying to you know get these models to elicit more and more trustable more and more reliable formal system behaviors. Okay. Uh so here is the first lesson I learned the hard way which is um if you're thinking about something it's like very concretely like a logical reasoning system. When you think about logic what you're interested in is a you know given a set of premises or assumptions a sequence of conclusions uh and there's a a sense in which we want that sequence of conclusions to be logically correct right to correctly follow from the premises. Um the logical correctness of a reasoning chain it turns out has nothing to do in principle with whether or not the reasoning chain is algorithmically realizable. That is algorithmic realizability and logical correctness correctness are just orthogonal concepts. Okay. And and by not sort of you know being aware of this you can get yourself in trouble. So um here's okay actually here's another way to explain it. So certainly uh you know uh uh for example right if I can prove certain programs correct right so I can give you a training set of here's a program and here's an argument that proves that program is correct and here's another program here's an argument that proves that program is correct I can give you a finite set of this these things and you could try to train your machine learning model to prove correctness of new programs okay the upshot of this is it will never work why will it never work because that is an undecidable property Right? Proving correctness of arbitrary programs is not something that can be decided in general. You will have to restrict like formal verification has must have limited scope. Okay. But you will never see this as a machine learning researcher. All you're going to see is I have a data set. I have my ambition. I fit my model to that data. Awesome on the training data. And it's like ah, you know, it's failing out of distribution and again and again, right? And this will never stop. it cannot work. You will always fail out of distribution this way. Okay. ", "mimetype": "text/plain", "start_char_idx": 37570, "end_char_idx": 39932, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.489

Source: https://www.youtube.com/watch?v=hMEViRcF7o0
Content: {"id_": "1761997e-a602-4bd2-927f-120ca2c90e72", "embedding": null, "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b4725299-3007-4d37-a4d4-57084d9bd646", "node_type": "4", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "ac9b80bd0b6f3398147387091bd45f29e388ddcb5744c3ad62826a953f6404cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f42e086e-2d30-4367-a77a-2e81b7c2a097", "node_type": "1", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "02a55f21a2921b06f2a5f1fa9ee6073531dfdb5b992642cc5507bc92599f1441", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "680d6f6f-6a59-4229-9a8e-7dff243cb135", "node_type": "1", "metadata": {}, "hash": "8872caa19121900baa9892112ab22a440ecae5e3c4237379e81e503f75e58066", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "All right. So um now I want to move to the second part of the talk which is I want to start to pivot more to sort of a machine learning stance which is um at least play along. Imagine you're trying to like you know train the LLM or alter the weights in your language model to somehow be a more reliable execution engine or reasoning engine. Now first thing to observe is by doing so you are not changing the ultimate computational ability of the model. It was a universal computer at the start. Hopefully, it's still one at the end. Okay. It is not about changing the computational ability of the model. It is about changing the behavior of the model to kind of be more usable by us, to be more directable by us, to be more sort of programmable by us where you know the behavior it emits is kind of the behavior we were looking for or hoping for or was at least correct even if we don't understand the algorithm. Okay. ", "mimetype": "text/plain", "start_char_idx": 32543, "end_char_idx": 33462, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.464

Source: 
Content: Context information is below.
---------------------
project_id: 5c5377ad-2c93-4118-be74-a62e80025cd9
source_url: https://www.youtube.com/watch?v=hMEViRcF7o0
video_id: 52cc9229-9228-431a-94da-bb9b17cc2981
embedding_model: ollama
id: 7c0a9d46-bbc4-42f3-ae95-954c01445493

Okay. You can easily show that existing language models as they are right now out there in the wild are are easily a you're easily able to drive them to be universal computers in a classical sense. Okay.

project_id: 5c5377ad-2c93-4118-be74-a62e80025cd9
source_url: https://www.youtube.com/watch?v=hMEViRcF7o0
video_id: 52cc9229-9228-431a-94da-bb9b17cc2981
embedding_model: ollama
id: 7c0a9d46-bbc4-42f3-ae95-954c01445493

Um, okay. So, part one, uh, language model as a computer. Uh, and this actually often strikes people as as sort of a strange thing to say because we all know where language modeling came from, right? It was just this you know endeavor to sort of ex you know capture human natural language behavior usually in the form of text which you know over the years wound up being kind of crystallized into this very simple idea of building a conditional probabilistic model that has a you know expresses a conditional distribution over the very next symbol or token or word or maybe a sentence uh given some finite uh length context to it. And you might ask yourself what does this have to do with a general purpose computer? uh and and to answer that I'm going to sort of share uh when this became clear to me uh and for me I started to see this connection between language models and computers right at the dawn of this uh so-called chain of thought uh phenomenon um okay so you all know the story but but I I just have to set something up here so at that time we were looking at these you know GSMAK problems these math word problems the kinds of word problems you got like in elementary school or junior high things like you know Janie got two apples from her mother twice as many bananas this vertical, how many fruit did you get? And you know, when we're trying to get the models to respond six okay um and then there was this kind of remarkable discovery that if you just simply took these questionans answer pairs say in GSMAK and just interposed between the question and answer just a natural language expressed semicoherent reasoning chain like Janie got twice as you know got two apples from her mother twice as many bananas from her uncle. How many fruit did she get? Well, she got 2 * 2 equals four bananas from her uncle plus two um fruit from her mother. That equals six fruit. The answer is six. So, we just simply had this reasoning chain between the question and the answer. And frankly, we were stunned. This this was like a step change in the performance of these models. They went from like teens, you know, accuracy up into the 60s up into the 70s almost overnight. Uh quite shocking. Okay. Um however that's not the real lesson. Um so the real lesson comes from actually understanding what were we doing before this. Okay. And what we were doing before this is good oldfashioned machine learning. You literally take the GSMAK data set and you treat it like you know like a supervised learning data set. You have a bunch of question answer pairs. You know here's a question here's an answer d and you wire up your big neural net or your language model and just by you know gradient descent on log likelihood of the answer. you just try to optimize the probability of eliciting the correct answer on the training set and then test on a hold out validation set. Okay.
---------------------
Given the context information and not prior knowledge, answer the query.
Query: language model computate
Answer: 
Relevance score: 0.580

Answer the question based on the context above.

----------------------------------------------------------------

User question: How does language model computate?

Context:
Source: https://www.youtube.com/watch?v=hMEViRcF7o0
Content: {"id_": "2828c59d-de56-4757-995d-762affc15b03", "embedding": null, "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b4725299-3007-4d37-a4d4-57084d9bd646", "node_type": "4", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "ac9b80bd0b6f3398147387091bd45f29e388ddcb5744c3ad62826a953f6404cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "312a7c23-383e-4c34-94b0-bd73e848b015", "node_type": "1", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "5127c249671f80c292232049a4970c7e97368a493807d77d4ed3948cbfa6c406", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "576d605a-960d-43e3-8c32-37804371cdc6", "node_type": "1", "metadata": {}, "hash": "787d29076ab163c143efd1a4d2f83127c664c5c39742ecabf175092d9e901703", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Okay. You can easily show that existing language models as they are right now out there in the wild are are easily a you're easily able to drive them to be universal computers in a classical sense. Okay. ", "mimetype": "text/plain", "start_char_idx": 13603, "end_char_idx": 13807, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.594

Source: https://www.youtube.com/watch?v=hMEViRcF7o0
Content: {"id_": "bc5d4a35-2955-4ced-a561-b6ef5793d0af", "embedding": null, "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b4725299-3007-4d37-a4d4-57084d9bd646", "node_type": "4", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "ac9b80bd0b6f3398147387091bd45f29e388ddcb5744c3ad62826a953f6404cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "058195df-6cd7-4d04-aec0-0c07d7405177", "node_type": "1", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "2d6af4157d7164005fd1bf34f50b6348c7a7929e1ff8a4bcc72e60b8e929de4b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6376fdd2-a16e-4584-a8f5-46358b097314", "node_type": "1", "metadata": {}, "hash": "418cb870ad713644e253f3c4b40c0564e1d0da60e9aeacebc73a848e2bd4129e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Um, okay. So, part one, uh, language model as a computer. Uh, and this actually often strikes people as as sort of a strange thing to say because we all know where language modeling came from, right? It was just this you know endeavor to sort of ex you know capture human natural language behavior usually in the form of text which you know over the years wound up being kind of crystallized into this very simple idea of building a conditional probabilistic model that has a you know expresses a conditional distribution over the very next symbol or token or word or maybe a sentence uh given some finite uh length context to it. And you might ask yourself what does this have to do with a general purpose computer? uh and and to answer that I'm going to sort of share uh when this became clear to me uh and for me I started to see this connection between language models and computers right at the dawn of this uh so-called chain of thought uh phenomenon um okay so you all know the story but but I I just have to set something up here so at that time we were looking at these you know GSMAK problems these math word problems the kinds of word problems you got like in elementary school or junior high things like you know Janie got two apples from her mother twice as many bananas this vertical, how many fruit did you get? And you know, when we're trying to get the models to respond six okay um and then there was this kind of remarkable discovery that if you just simply took these questionans answer pairs say in GSMAK and just interposed between the question and answer just a natural language expressed semicoherent reasoning chain like Janie got twice as you know got two apples from her mother twice as many bananas from her uncle. How many fruit did she get? Well, she got 2 * 2 equals four bananas from her uncle plus two um fruit from her mother. That equals six fruit. The answer is six. So, we just simply had this reasoning chain between the question and the answer. And frankly, we were stunned. This this was like a step change in the performance of these models. They went from like teens, you know, accuracy up into the 60s up into the 70s almost overnight. Uh quite shocking. Okay. Um however that's not the real lesson. Um so the real lesson comes from actually understanding what were we doing before this. Okay. And what we were doing before this is good oldfashioned machine learning. You literally take the GSMAK data set and you treat it like you know like a supervised learning data set. You have a bunch of question answer pairs. You know here's a question here's an answer d and you wire up your big neural net or your language model and just by you know gradient descent on log likelihood of the answer. you just try to optimize the probability of eliciting the correct answer on the training set and then test on a hold out validation set. Okay. ", "mimetype": "text/plain", "start_char_idx": 2123, "end_char_idx": 5002, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.590

Source: https://www.youtube.com/watch?v=hMEViRcF7o0
Content: {"id_": "47ec3cf4-c489-418a-9aa2-62a616180262", "embedding": null, "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b4725299-3007-4d37-a4d4-57084d9bd646", "node_type": "4", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "ac9b80bd0b6f3398147387091bd45f29e388ddcb5744c3ad62826a953f6404cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1a7b07b2-2604-409c-92b4-15579bb5525c", "node_type": "1", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "96af285a16dd6499975ebd2889dc6e2f38123e71a61dca9537a609cee502d926", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9f486925-28c7-4602-8ccf-05d61589d892", "node_type": "1", "metadata": {}, "hash": "f1250eeedabe940c4915f4d85b7b599b9eca58f01816f25f01de7db0fed5c7cc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Okay? Well, guess what is the go-to solution in large language model reasoning to this day? Um I I'll come back to that but let me share sort of some outcome of this experiment. So you you can ask some of these simple sort of path reachability problems to language models. Um now it turns out I'm going to make it even simpler is that I can simplify the problems to embed some special structure. So it turns out in this instance I I I didn't tell you but I embedded a special structure which is there is a total linear ordering on the variables where it turns out that first variable x1 if you assume it has the ver the value true it will force the next variable to have some value and that will force the next variable and force the next variable in a linear chain that connects the start variable to the end. So you can solve this problem just by reading off the forced assignments one by one and it will answer the question for you. So that's what you see on the right here. It's just a sequence of forced assignments that ends up with forcing X6 to be false. Therefore, we know the answer to the question is no. All right. Now, uh what do the language models do with problems like this? And these are I've anonymized the models. Sorry. And this is actually these experiments are back in February, so it's a generation or two behind already. Anyway, but when you ask uh the models, you know, questions like this, they're pretty mediocre. They were about 70% accurate, which you know, chance is 50%. You know, it's something, but these aren't super hard problems. Okay, but look on the right. Okay, the interesting thing on the right is the right is showing the the response length, and it's really impressive. It's it's responding to these questions with like response lengths of 2,000 or more tokens. Okay? And you read the responses, it's like total walkabout. like it might catch a few of the forced assignments and then it's just trying stuff, making mistakes, backing up, I don't know, just meandering and then 70% of the time landing on a correct answer and, you know, 30% of the time landing on an incorrect answer. It's it's kind of weird. ", "mimetype": "text/plain", "start_char_idx": 42389, "end_char_idx": 44540, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.559

Source: https://www.youtube.com/watch?v=hMEViRcF7o0
Content: {"id_": "0fd62a80-1d98-4cbc-9e30-ca941743dee2", "embedding": null, "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b4725299-3007-4d37-a4d4-57084d9bd646", "node_type": "4", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "ac9b80bd0b6f3398147387091bd45f29e388ddcb5744c3ad62826a953f6404cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "680d6f6f-6a59-4229-9a8e-7dff243cb135", "node_type": "1", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "9b20fa2a03d0897d3b37215023f07a1f45b989072cabe38dea5f80f8ed0741c8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e48f1fd0-e0d5-47f7-9b13-f3d5cd7e73bf", "node_type": "1", "metadata": {}, "hash": "7f50bc79bf67e4be5ead4f94e670f9787784a41a25f46749be840cd79bd0fc34", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This is good news. it means there'll always be jobs well at least for language models. Um okay so uh so where I want to go from here is is I just want to share a little bit more you know down to earth eventually um some of the challenges that I've been trying to face and kind of overcome and trying to you know get these models to elicit more and more trustable more and more reliable formal system behaviors. Okay. Uh so here is the first lesson I learned the hard way which is um if you're thinking about something it's like very concretely like a logical reasoning system. When you think about logic what you're interested in is a you know given a set of premises or assumptions a sequence of conclusions uh and there's a a sense in which we want that sequence of conclusions to be logically correct right to correctly follow from the premises. Um the logical correctness of a reasoning chain it turns out has nothing to do in principle with whether or not the reasoning chain is algorithmically realizable. That is algorithmic realizability and logical correctness correctness are just orthogonal concepts. Okay. And and by not sort of you know being aware of this you can get yourself in trouble. So um here's okay actually here's another way to explain it. So certainly uh you know uh uh for example right if I can prove certain programs correct right so I can give you a training set of here's a program and here's an argument that proves that program is correct and here's another program here's an argument that proves that program is correct I can give you a finite set of this these things and you could try to train your machine learning model to prove correctness of new programs okay the upshot of this is it will never work why will it never work because that is an undecidable property Right? Proving correctness of arbitrary programs is not something that can be decided in general. You will have to restrict like formal verification has must have limited scope. Okay. But you will never see this as a machine learning researcher. All you're going to see is I have a data set. I have my ambition. I fit my model to that data. Awesome on the training data. And it's like ah, you know, it's failing out of distribution and again and again, right? And this will never stop. it cannot work. You will always fail out of distribution this way. Okay. ", "mimetype": "text/plain", "start_char_idx": 37570, "end_char_idx": 39932, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.497

Source: https://www.youtube.com/watch?v=hMEViRcF7o0
Content: {"id_": "1761997e-a602-4bd2-927f-120ca2c90e72", "embedding": null, "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b4725299-3007-4d37-a4d4-57084d9bd646", "node_type": "4", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "ac9b80bd0b6f3398147387091bd45f29e388ddcb5744c3ad62826a953f6404cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f42e086e-2d30-4367-a77a-2e81b7c2a097", "node_type": "1", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "02a55f21a2921b06f2a5f1fa9ee6073531dfdb5b992642cc5507bc92599f1441", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "680d6f6f-6a59-4229-9a8e-7dff243cb135", "node_type": "1", "metadata": {}, "hash": "8872caa19121900baa9892112ab22a440ecae5e3c4237379e81e503f75e58066", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "All right. So um now I want to move to the second part of the talk which is I want to start to pivot more to sort of a machine learning stance which is um at least play along. Imagine you're trying to like you know train the LLM or alter the weights in your language model to somehow be a more reliable execution engine or reasoning engine. Now first thing to observe is by doing so you are not changing the ultimate computational ability of the model. It was a universal computer at the start. Hopefully, it's still one at the end. Okay. It is not about changing the computational ability of the model. It is about changing the behavior of the model to kind of be more usable by us, to be more directable by us, to be more sort of programmable by us where you know the behavior it emits is kind of the behavior we were looking for or hoping for or was at least correct even if we don't understand the algorithm. Okay. ", "mimetype": "text/plain", "start_char_idx": 32543, "end_char_idx": 33462, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.469

Source: 
Content: Context information is below.
---------------------
project_id: 5c5377ad-2c93-4118-be74-a62e80025cd9
source_url: https://www.youtube.com/watch?v=hMEViRcF7o0
video_id: 52cc9229-9228-431a-94da-bb9b17cc2981
embedding_model: ollama
id: 7c0a9d46-bbc4-42f3-ae95-954c01445493

Okay. You can easily show that existing language models as they are right now out there in the wild are are easily a you're easily able to drive them to be universal computers in a classical sense. Okay.

project_id: 5c5377ad-2c93-4118-be74-a62e80025cd9
source_url: https://www.youtube.com/watch?v=hMEViRcF7o0
video_id: 52cc9229-9228-431a-94da-bb9b17cc2981
embedding_model: ollama
id: 7c0a9d46-bbc4-42f3-ae95-954c01445493

Um, okay. So, part one, uh, language model as a computer. Uh, and this actually often strikes people as as sort of a strange thing to say because we all know where language modeling came from, right? It was just this you know endeavor to sort of ex you know capture human natural language behavior usually in the form of text which you know over the years wound up being kind of crystallized into this very simple idea of building a conditional probabilistic model that has a you know expresses a conditional distribution over the very next symbol or token or word or maybe a sentence uh given some finite uh length context to it. And you might ask yourself what does this have to do with a general purpose computer? uh and and to answer that I'm going to sort of share uh when this became clear to me uh and for me I started to see this connection between language models and computers right at the dawn of this uh so-called chain of thought uh phenomenon um okay so you all know the story but but I I just have to set something up here so at that time we were looking at these you know GSMAK problems these math word problems the kinds of word problems you got like in elementary school or junior high things like you know Janie got two apples from her mother twice as many bananas this vertical, how many fruit did you get? And you know, when we're trying to get the models to respond six okay um and then there was this kind of remarkable discovery that if you just simply took these questionans answer pairs say in GSMAK and just interposed between the question and answer just a natural language expressed semicoherent reasoning chain like Janie got twice as you know got two apples from her mother twice as many bananas from her uncle. How many fruit did she get? Well, she got 2 * 2 equals four bananas from her uncle plus two um fruit from her mother. That equals six fruit. The answer is six. So, we just simply had this reasoning chain between the question and the answer. And frankly, we were stunned. This this was like a step change in the performance of these models. They went from like teens, you know, accuracy up into the 60s up into the 70s almost overnight. Uh quite shocking. Okay. Um however that's not the real lesson. Um so the real lesson comes from actually understanding what were we doing before this. Okay. And what we were doing before this is good oldfashioned machine learning. You literally take the GSMAK data set and you treat it like you know like a supervised learning data set. You have a bunch of question answer pairs. You know here's a question here's an answer d and you wire up your big neural net or your language model and just by you know gradient descent on log likelihood of the answer. you just try to optimize the probability of eliciting the correct answer on the training set and then test on a hold out validation set. Okay.
---------------------
Given the context information and not prior knowledge, answer the query.
Query: Language model computates
Answer: 
Relevance score: 0.580

Answer the question based on the context above.

----------------------------------------------------------------

User question: How does language model computate?

Context:
Source: https://www.youtube.com/watch?v=hMEViRcF7o0
Content: {"id_": "2828c59d-de56-4757-995d-762affc15b03", "embedding": null, "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b4725299-3007-4d37-a4d4-57084d9bd646", "node_type": "4", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "ac9b80bd0b6f3398147387091bd45f29e388ddcb5744c3ad62826a953f6404cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "312a7c23-383e-4c34-94b0-bd73e848b015", "node_type": "1", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "5127c249671f80c292232049a4970c7e97368a493807d77d4ed3948cbfa6c406", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "576d605a-960d-43e3-8c32-37804371cdc6", "node_type": "1", "metadata": {}, "hash": "787d29076ab163c143efd1a4d2f83127c664c5c39742ecabf175092d9e901703", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Okay. You can easily show that existing language models as they are right now out there in the wild are are easily a you're easily able to drive them to be universal computers in a classical sense. Okay. ", "mimetype": "text/plain", "start_char_idx": 13603, "end_char_idx": 13807, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.594

Source: https://www.youtube.com/watch?v=hMEViRcF7o0
Content: {"id_": "bc5d4a35-2955-4ced-a561-b6ef5793d0af", "embedding": null, "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b4725299-3007-4d37-a4d4-57084d9bd646", "node_type": "4", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "ac9b80bd0b6f3398147387091bd45f29e388ddcb5744c3ad62826a953f6404cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "058195df-6cd7-4d04-aec0-0c07d7405177", "node_type": "1", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "2d6af4157d7164005fd1bf34f50b6348c7a7929e1ff8a4bcc72e60b8e929de4b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6376fdd2-a16e-4584-a8f5-46358b097314", "node_type": "1", "metadata": {}, "hash": "418cb870ad713644e253f3c4b40c0564e1d0da60e9aeacebc73a848e2bd4129e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Um, okay. So, part one, uh, language model as a computer. Uh, and this actually often strikes people as as sort of a strange thing to say because we all know where language modeling came from, right? It was just this you know endeavor to sort of ex you know capture human natural language behavior usually in the form of text which you know over the years wound up being kind of crystallized into this very simple idea of building a conditional probabilistic model that has a you know expresses a conditional distribution over the very next symbol or token or word or maybe a sentence uh given some finite uh length context to it. And you might ask yourself what does this have to do with a general purpose computer? uh and and to answer that I'm going to sort of share uh when this became clear to me uh and for me I started to see this connection between language models and computers right at the dawn of this uh so-called chain of thought uh phenomenon um okay so you all know the story but but I I just have to set something up here so at that time we were looking at these you know GSMAK problems these math word problems the kinds of word problems you got like in elementary school or junior high things like you know Janie got two apples from her mother twice as many bananas this vertical, how many fruit did you get? And you know, when we're trying to get the models to respond six okay um and then there was this kind of remarkable discovery that if you just simply took these questionans answer pairs say in GSMAK and just interposed between the question and answer just a natural language expressed semicoherent reasoning chain like Janie got twice as you know got two apples from her mother twice as many bananas from her uncle. How many fruit did she get? Well, she got 2 * 2 equals four bananas from her uncle plus two um fruit from her mother. That equals six fruit. The answer is six. So, we just simply had this reasoning chain between the question and the answer. And frankly, we were stunned. This this was like a step change in the performance of these models. They went from like teens, you know, accuracy up into the 60s up into the 70s almost overnight. Uh quite shocking. Okay. Um however that's not the real lesson. Um so the real lesson comes from actually understanding what were we doing before this. Okay. And what we were doing before this is good oldfashioned machine learning. You literally take the GSMAK data set and you treat it like you know like a supervised learning data set. You have a bunch of question answer pairs. You know here's a question here's an answer d and you wire up your big neural net or your language model and just by you know gradient descent on log likelihood of the answer. you just try to optimize the probability of eliciting the correct answer on the training set and then test on a hold out validation set. Okay. ", "mimetype": "text/plain", "start_char_idx": 2123, "end_char_idx": 5002, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.590

Source: https://www.youtube.com/watch?v=hMEViRcF7o0
Content: {"id_": "47ec3cf4-c489-418a-9aa2-62a616180262", "embedding": null, "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b4725299-3007-4d37-a4d4-57084d9bd646", "node_type": "4", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "ac9b80bd0b6f3398147387091bd45f29e388ddcb5744c3ad62826a953f6404cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1a7b07b2-2604-409c-92b4-15579bb5525c", "node_type": "1", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "96af285a16dd6499975ebd2889dc6e2f38123e71a61dca9537a609cee502d926", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9f486925-28c7-4602-8ccf-05d61589d892", "node_type": "1", "metadata": {}, "hash": "f1250eeedabe940c4915f4d85b7b599b9eca58f01816f25f01de7db0fed5c7cc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Okay? Well, guess what is the go-to solution in large language model reasoning to this day? Um I I'll come back to that but let me share sort of some outcome of this experiment. So you you can ask some of these simple sort of path reachability problems to language models. Um now it turns out I'm going to make it even simpler is that I can simplify the problems to embed some special structure. So it turns out in this instance I I I didn't tell you but I embedded a special structure which is there is a total linear ordering on the variables where it turns out that first variable x1 if you assume it has the ver the value true it will force the next variable to have some value and that will force the next variable and force the next variable in a linear chain that connects the start variable to the end. So you can solve this problem just by reading off the forced assignments one by one and it will answer the question for you. So that's what you see on the right here. It's just a sequence of forced assignments that ends up with forcing X6 to be false. Therefore, we know the answer to the question is no. All right. Now, uh what do the language models do with problems like this? And these are I've anonymized the models. Sorry. And this is actually these experiments are back in February, so it's a generation or two behind already. Anyway, but when you ask uh the models, you know, questions like this, they're pretty mediocre. They were about 70% accurate, which you know, chance is 50%. You know, it's something, but these aren't super hard problems. Okay, but look on the right. Okay, the interesting thing on the right is the right is showing the the response length, and it's really impressive. It's it's responding to these questions with like response lengths of 2,000 or more tokens. Okay? And you read the responses, it's like total walkabout. like it might catch a few of the forced assignments and then it's just trying stuff, making mistakes, backing up, I don't know, just meandering and then 70% of the time landing on a correct answer and, you know, 30% of the time landing on an incorrect answer. It's it's kind of weird. ", "mimetype": "text/plain", "start_char_idx": 42389, "end_char_idx": 44540, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.559

Source: https://www.youtube.com/watch?v=hMEViRcF7o0
Content: {"id_": "0fd62a80-1d98-4cbc-9e30-ca941743dee2", "embedding": null, "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b4725299-3007-4d37-a4d4-57084d9bd646", "node_type": "4", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "ac9b80bd0b6f3398147387091bd45f29e388ddcb5744c3ad62826a953f6404cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "680d6f6f-6a59-4229-9a8e-7dff243cb135", "node_type": "1", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "9b20fa2a03d0897d3b37215023f07a1f45b989072cabe38dea5f80f8ed0741c8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e48f1fd0-e0d5-47f7-9b13-f3d5cd7e73bf", "node_type": "1", "metadata": {}, "hash": "7f50bc79bf67e4be5ead4f94e670f9787784a41a25f46749be840cd79bd0fc34", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This is good news. it means there'll always be jobs well at least for language models. Um okay so uh so where I want to go from here is is I just want to share a little bit more you know down to earth eventually um some of the challenges that I've been trying to face and kind of overcome and trying to you know get these models to elicit more and more trustable more and more reliable formal system behaviors. Okay. Uh so here is the first lesson I learned the hard way which is um if you're thinking about something it's like very concretely like a logical reasoning system. When you think about logic what you're interested in is a you know given a set of premises or assumptions a sequence of conclusions uh and there's a a sense in which we want that sequence of conclusions to be logically correct right to correctly follow from the premises. Um the logical correctness of a reasoning chain it turns out has nothing to do in principle with whether or not the reasoning chain is algorithmically realizable. That is algorithmic realizability and logical correctness correctness are just orthogonal concepts. Okay. And and by not sort of you know being aware of this you can get yourself in trouble. So um here's okay actually here's another way to explain it. So certainly uh you know uh uh for example right if I can prove certain programs correct right so I can give you a training set of here's a program and here's an argument that proves that program is correct and here's another program here's an argument that proves that program is correct I can give you a finite set of this these things and you could try to train your machine learning model to prove correctness of new programs okay the upshot of this is it will never work why will it never work because that is an undecidable property Right? Proving correctness of arbitrary programs is not something that can be decided in general. You will have to restrict like formal verification has must have limited scope. Okay. But you will never see this as a machine learning researcher. All you're going to see is I have a data set. I have my ambition. I fit my model to that data. Awesome on the training data. And it's like ah, you know, it's failing out of distribution and again and again, right? And this will never stop. it cannot work. You will always fail out of distribution this way. Okay. ", "mimetype": "text/plain", "start_char_idx": 37570, "end_char_idx": 39932, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.497

Source: https://www.youtube.com/watch?v=hMEViRcF7o0
Content: {"id_": "1761997e-a602-4bd2-927f-120ca2c90e72", "embedding": null, "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b4725299-3007-4d37-a4d4-57084d9bd646", "node_type": "4", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "ac9b80bd0b6f3398147387091bd45f29e388ddcb5744c3ad62826a953f6404cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f42e086e-2d30-4367-a77a-2e81b7c2a097", "node_type": "1", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "02a55f21a2921b06f2a5f1fa9ee6073531dfdb5b992642cc5507bc92599f1441", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "680d6f6f-6a59-4229-9a8e-7dff243cb135", "node_type": "1", "metadata": {}, "hash": "8872caa19121900baa9892112ab22a440ecae5e3c4237379e81e503f75e58066", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "All right. So um now I want to move to the second part of the talk which is I want to start to pivot more to sort of a machine learning stance which is um at least play along. Imagine you're trying to like you know train the LLM or alter the weights in your language model to somehow be a more reliable execution engine or reasoning engine. Now first thing to observe is by doing so you are not changing the ultimate computational ability of the model. It was a universal computer at the start. Hopefully, it's still one at the end. Okay. It is not about changing the computational ability of the model. It is about changing the behavior of the model to kind of be more usable by us, to be more directable by us, to be more sort of programmable by us where you know the behavior it emits is kind of the behavior we were looking for or hoping for or was at least correct even if we don't understand the algorithm. Okay. ", "mimetype": "text/plain", "start_char_idx": 32543, "end_char_idx": 33462, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.469

Source: 
Content: [NodeWithScore(node=TextNode(id_='2828c59d-de56-4757-995d-762affc15b03', embedding=None, metadata={'project_id': '5c5377ad-2c93-4118-be74-a62e80025cd9', 'source_url': 'https://www.youtube.com/watch?v=hMEViRcF7o0', 'video_id': '52cc9229-9228-431a-94da-bb9b17cc2981', 'embedding_model': 'ollama', 'id': '7c0a9d46-bbc4-42f3-ae95-954c01445493'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='b4725299-3007-4d37-a4d4-57084d9bd646', node_type='4', metadata={'project_id': '5c5377ad-2c93-4118-be74-a62e80025cd9', 'source_url': 'https://www.youtube.com/watch?v=hMEViRcF7o0', 'video_id': '52cc9229-9228-431a-94da-bb9b17cc2981', 'embedding_model': 'ollama', 'id': '7c0a9d46-bbc4-42f3-ae95-954c01445493'}, hash='ac9b80bd0b6f3398147387091bd45f29e388ddcb5744c3ad62826a953f6404cc'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='312a7c23-383e-4c34-94b0-bd73e848b015', node_type='1', metadata={'project_id': '5c5377ad-2c93-4118-be74-a62e80025cd9', 'source_url': 'https://www.youtube.com/watch?v=hMEViRcF7o0', 'video_id': '52cc9229-9228-431a-94da-bb9b17cc2981', 'embedding_model': 'ollama', 'id': '7c0a9d46-bbc4-42f3-ae95-954c01445493'}, hash='5127c249671f80c292232049a4970c7e97368a493807d77d4ed3948cbfa6c406'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='576d605a-960d-43e3-8c32-37804371cdc6', node_type='1', metadata={}, hash='787d29076ab163c143efd1a4d2f83127c664c5c39742ecabf175092d9e901703')}, metadata_template='{key}: {value}', metadata_separator='\n', text="Okay. You can easily show that existing language models as they are right now out there in the wild are are easily a you're easily able to drive them to be universal computers in a classical sense. Okay. ", mimetype='text/plain', start_char_idx=13603, end_char_idx=13807, metadata_seperator='\n', text_template='{metadata_str}\n\n{content}'), score=0.5935108), NodeWithScore(node=TextNode(id_='bc5d4a35-2955-4ced-a561-b6ef5793d0af', embedding=None, metadata={'project_id': '5c5377ad-2c93-4118-be74-a62e80025cd9', 'source_url': 'https://www.youtube.com/watch?v=hMEViRcF7o0', 'video_id': '52cc9229-9228-431a-94da-bb9b17cc2981', 'embedding_model': 'ollama', 'id': '7c0a9d46-bbc4-42f3-ae95-954c01445493'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='b4725299-3007-4d37-a4d4-57084d9bd646', node_type='4', metadata={'project_id': '5c5377ad-2c93-4118-be74-a62e80025cd9', 'source_url': 'https://www.youtube.com/watch?v=hMEViRcF7o0', 'video_id': '52cc9229-9228-431a-94da-bb9b17cc2981', 'embedding_model': 'ollama', 'id': '7c0a9d46-bbc4-42f3-ae95-954c01445493'}, hash='ac9b80bd0b6f3398147387091bd45f29e388ddcb5744c3ad62826a953f6404cc'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='058195df-6cd7-4d04-aec0-0c07d7405177', node_type='1', metadata={'project_id': '5c5377ad-2c93-4118-be74-a62e80025cd9', 'source_url': 'https://www.youtube.com/watch?v=hMEViRcF7o0', 'video_id': '52cc9229-9228-431a-94da-bb9b17cc2981', 'embedding_model': 'ollama', 'id': '7c0a9d46-bbc4-42f3-ae95-954c01445493'}, hash='2d6af4157d7164005fd1bf34f50b6348c7a7929e1ff8a4bcc72e60b8e929de4b'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='6376fdd2-a16e-4584-a8f5-46358b097314', node_type='1', metadata={}, hash='418cb870ad713644e253f3c4b40c0564e1d0da60e9aeacebc73a848e2bd4129e')}, metadata_template='{key}: {value}', metadata_separator='\n', text="Um, okay. So, part one, uh, language model as a computer. Uh, and this actually often strikes people as as sort of a strange thing to say because we all know where language modeling came from, right? It was just this you know endeavor to sort of ex you know capture human natural language behavior usually in the form of text which you know over the years wound up being kind of crystallized into this very simple idea of building a conditional probabilistic model that has a you know expresses a conditional distribution over the very next symbol or token or word or maybe a sentence uh given some finite uh length context to it. And you might ask yourself what does this have to do with a general purpose computer? uh and and to answer that I'm going to sort of share uh when this became clear to me uh and for me I started to see this connection between language models and computers right at the dawn of this uh so-called chain of thought uh phenomenon um okay so you all know the story but but I I just have to set something up here so at that time we were looking at these you know GSMAK problems these math word problems the kinds of word problems you got like in elementary school or junior high things like you know Janie got two apples from her mother twice as many bananas this vertical, how many fruit did you get? And you know, when we're trying to get the models to respond six okay um and then there was this kind of remarkable discovery that if you just simply took these questionans answer pairs say in GSMAK and just interposed between the question and answer just a natural language expressed semicoherent reasoning chain like Janie got twice as you know got two apples from her mother twice as many bananas from her uncle. How many fruit did she get? Well, she got 2 * 2 equals four bananas from her uncle plus two um fruit from her mother. That equals six fruit. The answer is six. So, we just simply had this reasoning chain between the question and the answer. And frankly, we were stunned. This this was like a step change in the performance of these models. They went from like teens, you know, accuracy up into the 60s up into the 70s almost overnight. Uh quite shocking. Okay. Um however that's not the real lesson. Um so the real lesson comes from actually understanding what were we doing before this. Okay. And what we were doing before this is good oldfashioned machine learning. You literally take the GSMAK data set and you treat it like you know like a supervised learning data set. You have a bunch of question answer pairs. You know here's a question here's an answer d and you wire up your big neural net or your language model and just by you know gradient descent on log likelihood of the answer. you just try to optimize the probability of eliciting the correct answer on the training set and then test on a hold out validation set. Okay. ", mimetype='text/plain', start_char_idx=2123, end_char_idx=5002, metadata_seperator='\n', text_template='{metadata_str}\n\n{content}'), score=0.5897209)]
Relevance score: 0.580

Answer the question based on the context above.

----------------------------------------------------------------

User question: How does language model computate?

Context:
Source: https://www.youtube.com/watch?v=hMEViRcF7o0
Content: {"id_": "2828c59d-de56-4757-995d-762affc15b03", "embedding": null, "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b4725299-3007-4d37-a4d4-57084d9bd646", "node_type": "4", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "ac9b80bd0b6f3398147387091bd45f29e388ddcb5744c3ad62826a953f6404cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "312a7c23-383e-4c34-94b0-bd73e848b015", "node_type": "1", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "5127c249671f80c292232049a4970c7e97368a493807d77d4ed3948cbfa6c406", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "576d605a-960d-43e3-8c32-37804371cdc6", "node_type": "1", "metadata": {}, "hash": "787d29076ab163c143efd1a4d2f83127c664c5c39742ecabf175092d9e901703", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Okay. You can easily show that existing language models as they are right now out there in the wild are are easily a you're easily able to drive them to be universal computers in a classical sense. Okay. ", "mimetype": "text/plain", "start_char_idx": 13603, "end_char_idx": 13807, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.594

Source: https://www.youtube.com/watch?v=hMEViRcF7o0
Content: {"id_": "bc5d4a35-2955-4ced-a561-b6ef5793d0af", "embedding": null, "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b4725299-3007-4d37-a4d4-57084d9bd646", "node_type": "4", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "ac9b80bd0b6f3398147387091bd45f29e388ddcb5744c3ad62826a953f6404cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "058195df-6cd7-4d04-aec0-0c07d7405177", "node_type": "1", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "2d6af4157d7164005fd1bf34f50b6348c7a7929e1ff8a4bcc72e60b8e929de4b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6376fdd2-a16e-4584-a8f5-46358b097314", "node_type": "1", "metadata": {}, "hash": "418cb870ad713644e253f3c4b40c0564e1d0da60e9aeacebc73a848e2bd4129e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Um, okay. So, part one, uh, language model as a computer. Uh, and this actually often strikes people as as sort of a strange thing to say because we all know where language modeling came from, right? It was just this you know endeavor to sort of ex you know capture human natural language behavior usually in the form of text which you know over the years wound up being kind of crystallized into this very simple idea of building a conditional probabilistic model that has a you know expresses a conditional distribution over the very next symbol or token or word or maybe a sentence uh given some finite uh length context to it. And you might ask yourself what does this have to do with a general purpose computer? uh and and to answer that I'm going to sort of share uh when this became clear to me uh and for me I started to see this connection between language models and computers right at the dawn of this uh so-called chain of thought uh phenomenon um okay so you all know the story but but I I just have to set something up here so at that time we were looking at these you know GSMAK problems these math word problems the kinds of word problems you got like in elementary school or junior high things like you know Janie got two apples from her mother twice as many bananas this vertical, how many fruit did you get? And you know, when we're trying to get the models to respond six okay um and then there was this kind of remarkable discovery that if you just simply took these questionans answer pairs say in GSMAK and just interposed between the question and answer just a natural language expressed semicoherent reasoning chain like Janie got twice as you know got two apples from her mother twice as many bananas from her uncle. How many fruit did she get? Well, she got 2 * 2 equals four bananas from her uncle plus two um fruit from her mother. That equals six fruit. The answer is six. So, we just simply had this reasoning chain between the question and the answer. And frankly, we were stunned. This this was like a step change in the performance of these models. They went from like teens, you know, accuracy up into the 60s up into the 70s almost overnight. Uh quite shocking. Okay. Um however that's not the real lesson. Um so the real lesson comes from actually understanding what were we doing before this. Okay. And what we were doing before this is good oldfashioned machine learning. You literally take the GSMAK data set and you treat it like you know like a supervised learning data set. You have a bunch of question answer pairs. You know here's a question here's an answer d and you wire up your big neural net or your language model and just by you know gradient descent on log likelihood of the answer. you just try to optimize the probability of eliciting the correct answer on the training set and then test on a hold out validation set. Okay. ", "mimetype": "text/plain", "start_char_idx": 2123, "end_char_idx": 5002, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.590

Source: https://www.youtube.com/watch?v=hMEViRcF7o0
Content: {"id_": "47ec3cf4-c489-418a-9aa2-62a616180262", "embedding": null, "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b4725299-3007-4d37-a4d4-57084d9bd646", "node_type": "4", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "ac9b80bd0b6f3398147387091bd45f29e388ddcb5744c3ad62826a953f6404cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1a7b07b2-2604-409c-92b4-15579bb5525c", "node_type": "1", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "96af285a16dd6499975ebd2889dc6e2f38123e71a61dca9537a609cee502d926", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9f486925-28c7-4602-8ccf-05d61589d892", "node_type": "1", "metadata": {}, "hash": "f1250eeedabe940c4915f4d85b7b599b9eca58f01816f25f01de7db0fed5c7cc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Okay? Well, guess what is the go-to solution in large language model reasoning to this day? Um I I'll come back to that but let me share sort of some outcome of this experiment. So you you can ask some of these simple sort of path reachability problems to language models. Um now it turns out I'm going to make it even simpler is that I can simplify the problems to embed some special structure. So it turns out in this instance I I I didn't tell you but I embedded a special structure which is there is a total linear ordering on the variables where it turns out that first variable x1 if you assume it has the ver the value true it will force the next variable to have some value and that will force the next variable and force the next variable in a linear chain that connects the start variable to the end. So you can solve this problem just by reading off the forced assignments one by one and it will answer the question for you. So that's what you see on the right here. It's just a sequence of forced assignments that ends up with forcing X6 to be false. Therefore, we know the answer to the question is no. All right. Now, uh what do the language models do with problems like this? And these are I've anonymized the models. Sorry. And this is actually these experiments are back in February, so it's a generation or two behind already. Anyway, but when you ask uh the models, you know, questions like this, they're pretty mediocre. They were about 70% accurate, which you know, chance is 50%. You know, it's something, but these aren't super hard problems. Okay, but look on the right. Okay, the interesting thing on the right is the right is showing the the response length, and it's really impressive. It's it's responding to these questions with like response lengths of 2,000 or more tokens. Okay? And you read the responses, it's like total walkabout. like it might catch a few of the forced assignments and then it's just trying stuff, making mistakes, backing up, I don't know, just meandering and then 70% of the time landing on a correct answer and, you know, 30% of the time landing on an incorrect answer. It's it's kind of weird. ", "mimetype": "text/plain", "start_char_idx": 42389, "end_char_idx": 44540, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.559

Source: https://www.youtube.com/watch?v=hMEViRcF7o0
Content: {"id_": "0fd62a80-1d98-4cbc-9e30-ca941743dee2", "embedding": null, "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b4725299-3007-4d37-a4d4-57084d9bd646", "node_type": "4", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "ac9b80bd0b6f3398147387091bd45f29e388ddcb5744c3ad62826a953f6404cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "680d6f6f-6a59-4229-9a8e-7dff243cb135", "node_type": "1", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "9b20fa2a03d0897d3b37215023f07a1f45b989072cabe38dea5f80f8ed0741c8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e48f1fd0-e0d5-47f7-9b13-f3d5cd7e73bf", "node_type": "1", "metadata": {}, "hash": "7f50bc79bf67e4be5ead4f94e670f9787784a41a25f46749be840cd79bd0fc34", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This is good news. it means there'll always be jobs well at least for language models. Um okay so uh so where I want to go from here is is I just want to share a little bit more you know down to earth eventually um some of the challenges that I've been trying to face and kind of overcome and trying to you know get these models to elicit more and more trustable more and more reliable formal system behaviors. Okay. Uh so here is the first lesson I learned the hard way which is um if you're thinking about something it's like very concretely like a logical reasoning system. When you think about logic what you're interested in is a you know given a set of premises or assumptions a sequence of conclusions uh and there's a a sense in which we want that sequence of conclusions to be logically correct right to correctly follow from the premises. Um the logical correctness of a reasoning chain it turns out has nothing to do in principle with whether or not the reasoning chain is algorithmically realizable. That is algorithmic realizability and logical correctness correctness are just orthogonal concepts. Okay. And and by not sort of you know being aware of this you can get yourself in trouble. So um here's okay actually here's another way to explain it. So certainly uh you know uh uh for example right if I can prove certain programs correct right so I can give you a training set of here's a program and here's an argument that proves that program is correct and here's another program here's an argument that proves that program is correct I can give you a finite set of this these things and you could try to train your machine learning model to prove correctness of new programs okay the upshot of this is it will never work why will it never work because that is an undecidable property Right? Proving correctness of arbitrary programs is not something that can be decided in general. You will have to restrict like formal verification has must have limited scope. Okay. But you will never see this as a machine learning researcher. All you're going to see is I have a data set. I have my ambition. I fit my model to that data. Awesome on the training data. And it's like ah, you know, it's failing out of distribution and again and again, right? And this will never stop. it cannot work. You will always fail out of distribution this way. Okay. ", "mimetype": "text/plain", "start_char_idx": 37570, "end_char_idx": 39932, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.497

Source: https://www.youtube.com/watch?v=hMEViRcF7o0
Content: {"id_": "1761997e-a602-4bd2-927f-120ca2c90e72", "embedding": null, "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b4725299-3007-4d37-a4d4-57084d9bd646", "node_type": "4", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "ac9b80bd0b6f3398147387091bd45f29e388ddcb5744c3ad62826a953f6404cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f42e086e-2d30-4367-a77a-2e81b7c2a097", "node_type": "1", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "02a55f21a2921b06f2a5f1fa9ee6073531dfdb5b992642cc5507bc92599f1441", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "680d6f6f-6a59-4229-9a8e-7dff243cb135", "node_type": "1", "metadata": {}, "hash": "8872caa19121900baa9892112ab22a440ecae5e3c4237379e81e503f75e58066", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "All right. So um now I want to move to the second part of the talk which is I want to start to pivot more to sort of a machine learning stance which is um at least play along. Imagine you're trying to like you know train the LLM or alter the weights in your language model to somehow be a more reliable execution engine or reasoning engine. Now first thing to observe is by doing so you are not changing the ultimate computational ability of the model. It was a universal computer at the start. Hopefully, it's still one at the end. Okay. It is not about changing the computational ability of the model. It is about changing the behavior of the model to kind of be more usable by us, to be more directable by us, to be more sort of programmable by us where you know the behavior it emits is kind of the behavior we were looking for or hoping for or was at least correct even if we don't understand the algorithm. Okay. ", "mimetype": "text/plain", "start_char_idx": 32543, "end_char_idx": 33462, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.469

Source: 
Content: [NodeWithScore(node=TextNode(id_='2828c59d-de56-4757-995d-762affc15b03', embedding=None, metadata={'project_id': '5c5377ad-2c93-4118-be74-a62e80025cd9', 'source_url': 'https://www.youtube.com/watch?v=hMEViRcF7o0', 'video_id': '52cc9229-9228-431a-94da-bb9b17cc2981', 'embedding_model': 'ollama', 'id': '7c0a9d46-bbc4-42f3-ae95-954c01445493'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='b4725299-3007-4d37-a4d4-57084d9bd646', node_type='4', metadata={'project_id': '5c5377ad-2c93-4118-be74-a62e80025cd9', 'source_url': 'https://www.youtube.com/watch?v=hMEViRcF7o0', 'video_id': '52cc9229-9228-431a-94da-bb9b17cc2981', 'embedding_model': 'ollama', 'id': '7c0a9d46-bbc4-42f3-ae95-954c01445493'}, hash='ac9b80bd0b6f3398147387091bd45f29e388ddcb5744c3ad62826a953f6404cc'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='312a7c23-383e-4c34-94b0-bd73e848b015', node_type='1', metadata={'project_id': '5c5377ad-2c93-4118-be74-a62e80025cd9', 'source_url': 'https://www.youtube.com/watch?v=hMEViRcF7o0', 'video_id': '52cc9229-9228-431a-94da-bb9b17cc2981', 'embedding_model': 'ollama', 'id': '7c0a9d46-bbc4-42f3-ae95-954c01445493'}, hash='5127c249671f80c292232049a4970c7e97368a493807d77d4ed3948cbfa6c406'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='576d605a-960d-43e3-8c32-37804371cdc6', node_type='1', metadata={}, hash='787d29076ab163c143efd1a4d2f83127c664c5c39742ecabf175092d9e901703')}, metadata_template='{key}: {value}', metadata_separator='\n', text="Okay. You can easily show that existing language models as they are right now out there in the wild are are easily a you're easily able to drive them to be universal computers in a classical sense. Okay. ", mimetype='text/plain', start_char_idx=13603, end_char_idx=13807, metadata_seperator='\n', text_template='{metadata_str}\n\n{content}'), score=0.5935108), NodeWithScore(node=TextNode(id_='bc5d4a35-2955-4ced-a561-b6ef5793d0af', embedding=None, metadata={'project_id': '5c5377ad-2c93-4118-be74-a62e80025cd9', 'source_url': 'https://www.youtube.com/watch?v=hMEViRcF7o0', 'video_id': '52cc9229-9228-431a-94da-bb9b17cc2981', 'embedding_model': 'ollama', 'id': '7c0a9d46-bbc4-42f3-ae95-954c01445493'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='b4725299-3007-4d37-a4d4-57084d9bd646', node_type='4', metadata={'project_id': '5c5377ad-2c93-4118-be74-a62e80025cd9', 'source_url': 'https://www.youtube.com/watch?v=hMEViRcF7o0', 'video_id': '52cc9229-9228-431a-94da-bb9b17cc2981', 'embedding_model': 'ollama', 'id': '7c0a9d46-bbc4-42f3-ae95-954c01445493'}, hash='ac9b80bd0b6f3398147387091bd45f29e388ddcb5744c3ad62826a953f6404cc'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='058195df-6cd7-4d04-aec0-0c07d7405177', node_type='1', metadata={'project_id': '5c5377ad-2c93-4118-be74-a62e80025cd9', 'source_url': 'https://www.youtube.com/watch?v=hMEViRcF7o0', 'video_id': '52cc9229-9228-431a-94da-bb9b17cc2981', 'embedding_model': 'ollama', 'id': '7c0a9d46-bbc4-42f3-ae95-954c01445493'}, hash='2d6af4157d7164005fd1bf34f50b6348c7a7929e1ff8a4bcc72e60b8e929de4b'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='6376fdd2-a16e-4584-a8f5-46358b097314', node_type='1', metadata={}, hash='418cb870ad713644e253f3c4b40c0564e1d0da60e9aeacebc73a848e2bd4129e')}, metadata_template='{key}: {value}', metadata_separator='\n', text="Um, okay. So, part one, uh, language model as a computer. Uh, and this actually often strikes people as as sort of a strange thing to say because we all know where language modeling came from, right? It was just this you know endeavor to sort of ex you know capture human natural language behavior usually in the form of text which you know over the years wound up being kind of crystallized into this very simple idea of building a conditional probabilistic model that has a you know expresses a conditional distribution over the very next symbol or token or word or maybe a sentence uh given some finite uh length context to it. And you might ask yourself what does this have to do with a general purpose computer? uh and and to answer that I'm going to sort of share uh when this became clear to me uh and for me I started to see this connection between language models and computers right at the dawn of this uh so-called chain of thought uh phenomenon um okay so you all know the story but but I I just have to set something up here so at that time we were looking at these you know GSMAK problems these math word problems the kinds of word problems you got like in elementary school or junior high things like you know Janie got two apples from her mother twice as many bananas this vertical, how many fruit did you get? And you know, when we're trying to get the models to respond six okay um and then there was this kind of remarkable discovery that if you just simply took these questionans answer pairs say in GSMAK and just interposed between the question and answer just a natural language expressed semicoherent reasoning chain like Janie got twice as you know got two apples from her mother twice as many bananas from her uncle. How many fruit did she get? Well, she got 2 * 2 equals four bananas from her uncle plus two um fruit from her mother. That equals six fruit. The answer is six. So, we just simply had this reasoning chain between the question and the answer. And frankly, we were stunned. This this was like a step change in the performance of these models. They went from like teens, you know, accuracy up into the 60s up into the 70s almost overnight. Uh quite shocking. Okay. Um however that's not the real lesson. Um so the real lesson comes from actually understanding what were we doing before this. Okay. And what we were doing before this is good oldfashioned machine learning. You literally take the GSMAK data set and you treat it like you know like a supervised learning data set. You have a bunch of question answer pairs. You know here's a question here's an answer d and you wire up your big neural net or your language model and just by you know gradient descent on log likelihood of the answer. you just try to optimize the probability of eliciting the correct answer on the training set and then test on a hold out validation set. Okay. ", mimetype='text/plain', start_char_idx=2123, end_char_idx=5002, metadata_seperator='\n', text_template='{metadata_str}\n\n{content}'), score=0.5897209)]
Relevance score: 0.580

Answer the question based on the context above.

----------------------------------------------------------------

User question: How does language model computate?

Context:
Source: https://www.youtube.com/watch?v=hMEViRcF7o0
Content: {"id_": "2828c59d-de56-4757-995d-762affc15b03", "embedding": null, "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b4725299-3007-4d37-a4d4-57084d9bd646", "node_type": "4", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "ac9b80bd0b6f3398147387091bd45f29e388ddcb5744c3ad62826a953f6404cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "312a7c23-383e-4c34-94b0-bd73e848b015", "node_type": "1", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "5127c249671f80c292232049a4970c7e97368a493807d77d4ed3948cbfa6c406", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "576d605a-960d-43e3-8c32-37804371cdc6", "node_type": "1", "metadata": {}, "hash": "787d29076ab163c143efd1a4d2f83127c664c5c39742ecabf175092d9e901703", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Okay. You can easily show that existing language models as they are right now out there in the wild are are easily a you're easily able to drive them to be universal computers in a classical sense. Okay. ", "mimetype": "text/plain", "start_char_idx": 13603, "end_char_idx": 13807, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.594

Source: https://www.youtube.com/watch?v=hMEViRcF7o0
Content: {"id_": "bc5d4a35-2955-4ced-a561-b6ef5793d0af", "embedding": null, "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b4725299-3007-4d37-a4d4-57084d9bd646", "node_type": "4", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "ac9b80bd0b6f3398147387091bd45f29e388ddcb5744c3ad62826a953f6404cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "058195df-6cd7-4d04-aec0-0c07d7405177", "node_type": "1", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "2d6af4157d7164005fd1bf34f50b6348c7a7929e1ff8a4bcc72e60b8e929de4b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6376fdd2-a16e-4584-a8f5-46358b097314", "node_type": "1", "metadata": {}, "hash": "418cb870ad713644e253f3c4b40c0564e1d0da60e9aeacebc73a848e2bd4129e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Um, okay. So, part one, uh, language model as a computer. Uh, and this actually often strikes people as as sort of a strange thing to say because we all know where language modeling came from, right? It was just this you know endeavor to sort of ex you know capture human natural language behavior usually in the form of text which you know over the years wound up being kind of crystallized into this very simple idea of building a conditional probabilistic model that has a you know expresses a conditional distribution over the very next symbol or token or word or maybe a sentence uh given some finite uh length context to it. And you might ask yourself what does this have to do with a general purpose computer? uh and and to answer that I'm going to sort of share uh when this became clear to me uh and for me I started to see this connection between language models and computers right at the dawn of this uh so-called chain of thought uh phenomenon um okay so you all know the story but but I I just have to set something up here so at that time we were looking at these you know GSMAK problems these math word problems the kinds of word problems you got like in elementary school or junior high things like you know Janie got two apples from her mother twice as many bananas this vertical, how many fruit did you get? And you know, when we're trying to get the models to respond six okay um and then there was this kind of remarkable discovery that if you just simply took these questionans answer pairs say in GSMAK and just interposed between the question and answer just a natural language expressed semicoherent reasoning chain like Janie got twice as you know got two apples from her mother twice as many bananas from her uncle. How many fruit did she get? Well, she got 2 * 2 equals four bananas from her uncle plus two um fruit from her mother. That equals six fruit. The answer is six. So, we just simply had this reasoning chain between the question and the answer. And frankly, we were stunned. This this was like a step change in the performance of these models. They went from like teens, you know, accuracy up into the 60s up into the 70s almost overnight. Uh quite shocking. Okay. Um however that's not the real lesson. Um so the real lesson comes from actually understanding what were we doing before this. Okay. And what we were doing before this is good oldfashioned machine learning. You literally take the GSMAK data set and you treat it like you know like a supervised learning data set. You have a bunch of question answer pairs. You know here's a question here's an answer d and you wire up your big neural net or your language model and just by you know gradient descent on log likelihood of the answer. you just try to optimize the probability of eliciting the correct answer on the training set and then test on a hold out validation set. Okay. ", "mimetype": "text/plain", "start_char_idx": 2123, "end_char_idx": 5002, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.590

Source: https://www.youtube.com/watch?v=hMEViRcF7o0
Content: {"id_": "47ec3cf4-c489-418a-9aa2-62a616180262", "embedding": null, "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b4725299-3007-4d37-a4d4-57084d9bd646", "node_type": "4", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "ac9b80bd0b6f3398147387091bd45f29e388ddcb5744c3ad62826a953f6404cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1a7b07b2-2604-409c-92b4-15579bb5525c", "node_type": "1", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "96af285a16dd6499975ebd2889dc6e2f38123e71a61dca9537a609cee502d926", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9f486925-28c7-4602-8ccf-05d61589d892", "node_type": "1", "metadata": {}, "hash": "f1250eeedabe940c4915f4d85b7b599b9eca58f01816f25f01de7db0fed5c7cc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Okay? Well, guess what is the go-to solution in large language model reasoning to this day? Um I I'll come back to that but let me share sort of some outcome of this experiment. So you you can ask some of these simple sort of path reachability problems to language models. Um now it turns out I'm going to make it even simpler is that I can simplify the problems to embed some special structure. So it turns out in this instance I I I didn't tell you but I embedded a special structure which is there is a total linear ordering on the variables where it turns out that first variable x1 if you assume it has the ver the value true it will force the next variable to have some value and that will force the next variable and force the next variable in a linear chain that connects the start variable to the end. So you can solve this problem just by reading off the forced assignments one by one and it will answer the question for you. So that's what you see on the right here. It's just a sequence of forced assignments that ends up with forcing X6 to be false. Therefore, we know the answer to the question is no. All right. Now, uh what do the language models do with problems like this? And these are I've anonymized the models. Sorry. And this is actually these experiments are back in February, so it's a generation or two behind already. Anyway, but when you ask uh the models, you know, questions like this, they're pretty mediocre. They were about 70% accurate, which you know, chance is 50%. You know, it's something, but these aren't super hard problems. Okay, but look on the right. Okay, the interesting thing on the right is the right is showing the the response length, and it's really impressive. It's it's responding to these questions with like response lengths of 2,000 or more tokens. Okay? And you read the responses, it's like total walkabout. like it might catch a few of the forced assignments and then it's just trying stuff, making mistakes, backing up, I don't know, just meandering and then 70% of the time landing on a correct answer and, you know, 30% of the time landing on an incorrect answer. It's it's kind of weird. ", "mimetype": "text/plain", "start_char_idx": 42389, "end_char_idx": 44540, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.559

Source: https://www.youtube.com/watch?v=hMEViRcF7o0
Content: {"id_": "0fd62a80-1d98-4cbc-9e30-ca941743dee2", "embedding": null, "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b4725299-3007-4d37-a4d4-57084d9bd646", "node_type": "4", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "ac9b80bd0b6f3398147387091bd45f29e388ddcb5744c3ad62826a953f6404cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "680d6f6f-6a59-4229-9a8e-7dff243cb135", "node_type": "1", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "9b20fa2a03d0897d3b37215023f07a1f45b989072cabe38dea5f80f8ed0741c8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e48f1fd0-e0d5-47f7-9b13-f3d5cd7e73bf", "node_type": "1", "metadata": {}, "hash": "7f50bc79bf67e4be5ead4f94e670f9787784a41a25f46749be840cd79bd0fc34", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This is good news. it means there'll always be jobs well at least for language models. Um okay so uh so where I want to go from here is is I just want to share a little bit more you know down to earth eventually um some of the challenges that I've been trying to face and kind of overcome and trying to you know get these models to elicit more and more trustable more and more reliable formal system behaviors. Okay. Uh so here is the first lesson I learned the hard way which is um if you're thinking about something it's like very concretely like a logical reasoning system. When you think about logic what you're interested in is a you know given a set of premises or assumptions a sequence of conclusions uh and there's a a sense in which we want that sequence of conclusions to be logically correct right to correctly follow from the premises. Um the logical correctness of a reasoning chain it turns out has nothing to do in principle with whether or not the reasoning chain is algorithmically realizable. That is algorithmic realizability and logical correctness correctness are just orthogonal concepts. Okay. And and by not sort of you know being aware of this you can get yourself in trouble. So um here's okay actually here's another way to explain it. So certainly uh you know uh uh for example right if I can prove certain programs correct right so I can give you a training set of here's a program and here's an argument that proves that program is correct and here's another program here's an argument that proves that program is correct I can give you a finite set of this these things and you could try to train your machine learning model to prove correctness of new programs okay the upshot of this is it will never work why will it never work because that is an undecidable property Right? Proving correctness of arbitrary programs is not something that can be decided in general. You will have to restrict like formal verification has must have limited scope. Okay. But you will never see this as a machine learning researcher. All you're going to see is I have a data set. I have my ambition. I fit my model to that data. Awesome on the training data. And it's like ah, you know, it's failing out of distribution and again and again, right? And this will never stop. it cannot work. You will always fail out of distribution this way. Okay. ", "mimetype": "text/plain", "start_char_idx": 37570, "end_char_idx": 39932, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.497

Source: https://www.youtube.com/watch?v=hMEViRcF7o0
Content: {"id_": "1761997e-a602-4bd2-927f-120ca2c90e72", "embedding": null, "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b4725299-3007-4d37-a4d4-57084d9bd646", "node_type": "4", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "ac9b80bd0b6f3398147387091bd45f29e388ddcb5744c3ad62826a953f6404cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f42e086e-2d30-4367-a77a-2e81b7c2a097", "node_type": "1", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "02a55f21a2921b06f2a5f1fa9ee6073531dfdb5b992642cc5507bc92599f1441", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "680d6f6f-6a59-4229-9a8e-7dff243cb135", "node_type": "1", "metadata": {}, "hash": "8872caa19121900baa9892112ab22a440ecae5e3c4237379e81e503f75e58066", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "All right. So um now I want to move to the second part of the talk which is I want to start to pivot more to sort of a machine learning stance which is um at least play along. Imagine you're trying to like you know train the LLM or alter the weights in your language model to somehow be a more reliable execution engine or reasoning engine. Now first thing to observe is by doing so you are not changing the ultimate computational ability of the model. It was a universal computer at the start. Hopefully, it's still one at the end. Okay. It is not about changing the computational ability of the model. It is about changing the behavior of the model to kind of be more usable by us, to be more directable by us, to be more sort of programmable by us where you know the behavior it emits is kind of the behavior we were looking for or hoping for or was at least correct even if we don't understand the algorithm. Okay. ", "mimetype": "text/plain", "start_char_idx": 32543, "end_char_idx": 33462, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.469

Source: 
Content: [NodeWithScore(node=TextNode(id_='2828c59d-de56-4757-995d-762affc15b03', embedding=None, metadata={'project_id': '5c5377ad-2c93-4118-be74-a62e80025cd9', 'source_url': 'https://www.youtube.com/watch?v=hMEViRcF7o0', 'video_id': '52cc9229-9228-431a-94da-bb9b17cc2981', 'embedding_model': 'ollama', 'id': '7c0a9d46-bbc4-42f3-ae95-954c01445493'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='b4725299-3007-4d37-a4d4-57084d9bd646', node_type='4', metadata={'project_id': '5c5377ad-2c93-4118-be74-a62e80025cd9', 'source_url': 'https://www.youtube.com/watch?v=hMEViRcF7o0', 'video_id': '52cc9229-9228-431a-94da-bb9b17cc2981', 'embedding_model': 'ollama', 'id': '7c0a9d46-bbc4-42f3-ae95-954c01445493'}, hash='ac9b80bd0b6f3398147387091bd45f29e388ddcb5744c3ad62826a953f6404cc'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='312a7c23-383e-4c34-94b0-bd73e848b015', node_type='1', metadata={'project_id': '5c5377ad-2c93-4118-be74-a62e80025cd9', 'source_url': 'https://www.youtube.com/watch?v=hMEViRcF7o0', 'video_id': '52cc9229-9228-431a-94da-bb9b17cc2981', 'embedding_model': 'ollama', 'id': '7c0a9d46-bbc4-42f3-ae95-954c01445493'}, hash='5127c249671f80c292232049a4970c7e97368a493807d77d4ed3948cbfa6c406'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='576d605a-960d-43e3-8c32-37804371cdc6', node_type='1', metadata={}, hash='787d29076ab163c143efd1a4d2f83127c664c5c39742ecabf175092d9e901703')}, metadata_template='{key}: {value}', metadata_separator='\n', text="Okay. You can easily show that existing language models as they are right now out there in the wild are are easily a you're easily able to drive them to be universal computers in a classical sense. Okay. ", mimetype='text/plain', start_char_idx=13603, end_char_idx=13807, metadata_seperator='\n', text_template='{metadata_str}\n\n{content}'), score=0.5935108), NodeWithScore(node=TextNode(id_='bc5d4a35-2955-4ced-a561-b6ef5793d0af', embedding=None, metadata={'project_id': '5c5377ad-2c93-4118-be74-a62e80025cd9', 'source_url': 'https://www.youtube.com/watch?v=hMEViRcF7o0', 'video_id': '52cc9229-9228-431a-94da-bb9b17cc2981', 'embedding_model': 'ollama', 'id': '7c0a9d46-bbc4-42f3-ae95-954c01445493'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='b4725299-3007-4d37-a4d4-57084d9bd646', node_type='4', metadata={'project_id': '5c5377ad-2c93-4118-be74-a62e80025cd9', 'source_url': 'https://www.youtube.com/watch?v=hMEViRcF7o0', 'video_id': '52cc9229-9228-431a-94da-bb9b17cc2981', 'embedding_model': 'ollama', 'id': '7c0a9d46-bbc4-42f3-ae95-954c01445493'}, hash='ac9b80bd0b6f3398147387091bd45f29e388ddcb5744c3ad62826a953f6404cc'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='058195df-6cd7-4d04-aec0-0c07d7405177', node_type='1', metadata={'project_id': '5c5377ad-2c93-4118-be74-a62e80025cd9', 'source_url': 'https://www.youtube.com/watch?v=hMEViRcF7o0', 'video_id': '52cc9229-9228-431a-94da-bb9b17cc2981', 'embedding_model': 'ollama', 'id': '7c0a9d46-bbc4-42f3-ae95-954c01445493'}, hash='2d6af4157d7164005fd1bf34f50b6348c7a7929e1ff8a4bcc72e60b8e929de4b'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='6376fdd2-a16e-4584-a8f5-46358b097314', node_type='1', metadata={}, hash='418cb870ad713644e253f3c4b40c0564e1d0da60e9aeacebc73a848e2bd4129e')}, metadata_template='{key}: {value}', metadata_separator='\n', text="Um, okay. So, part one, uh, language model as a computer. Uh, and this actually often strikes people as as sort of a strange thing to say because we all know where language modeling came from, right? It was just this you know endeavor to sort of ex you know capture human natural language behavior usually in the form of text which you know over the years wound up being kind of crystallized into this very simple idea of building a conditional probabilistic model that has a you know expresses a conditional distribution over the very next symbol or token or word or maybe a sentence uh given some finite uh length context to it. And you might ask yourself what does this have to do with a general purpose computer? uh and and to answer that I'm going to sort of share uh when this became clear to me uh and for me I started to see this connection between language models and computers right at the dawn of this uh so-called chain of thought uh phenomenon um okay so you all know the story but but I I just have to set something up here so at that time we were looking at these you know GSMAK problems these math word problems the kinds of word problems you got like in elementary school or junior high things like you know Janie got two apples from her mother twice as many bananas this vertical, how many fruit did you get? And you know, when we're trying to get the models to respond six okay um and then there was this kind of remarkable discovery that if you just simply took these questionans answer pairs say in GSMAK and just interposed between the question and answer just a natural language expressed semicoherent reasoning chain like Janie got twice as you know got two apples from her mother twice as many bananas from her uncle. How many fruit did she get? Well, she got 2 * 2 equals four bananas from her uncle plus two um fruit from her mother. That equals six fruit. The answer is six. So, we just simply had this reasoning chain between the question and the answer. And frankly, we were stunned. This this was like a step change in the performance of these models. They went from like teens, you know, accuracy up into the 60s up into the 70s almost overnight. Uh quite shocking. Okay. Um however that's not the real lesson. Um so the real lesson comes from actually understanding what were we doing before this. Okay. And what we were doing before this is good oldfashioned machine learning. You literally take the GSMAK data set and you treat it like you know like a supervised learning data set. You have a bunch of question answer pairs. You know here's a question here's an answer d and you wire up your big neural net or your language model and just by you know gradient descent on log likelihood of the answer. you just try to optimize the probability of eliciting the correct answer on the training set and then test on a hold out validation set. Okay. ", mimetype='text/plain', start_char_idx=2123, end_char_idx=5002, metadata_seperator='\n', text_template='{metadata_str}\n\n{content}'), score=0.5897209)]
Relevance score: 0.580

Answer the question based on the context above.

----------------------------------------------------------------

You are a helpful AI assistant that answers questions based on retrieved context.

            Guidelines:
            1. Use the provided context to answer the user's question
            2. Be concise and informative
            3. If the context doesn't contain relevant information, say so politely
            4. Cite sources when appropriate
            5. Provide a clear, helpful response

            Context will be provided with source URLs and relevance scores.


            User question: How does language model computate?

Context:
Source: https://www.youtube.com/watch?v=hMEViRcF7o0
Content: {"id_": "2828c59d-de56-4757-995d-762affc15b03", "embedding": null, "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b4725299-3007-4d37-a4d4-57084d9bd646", "node_type": "4", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "ac9b80bd0b6f3398147387091bd45f29e388ddcb5744c3ad62826a953f6404cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "312a7c23-383e-4c34-94b0-bd73e848b015", "node_type": "1", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "5127c249671f80c292232049a4970c7e97368a493807d77d4ed3948cbfa6c406", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "576d605a-960d-43e3-8c32-37804371cdc6", "node_type": "1", "metadata": {}, "hash": "787d29076ab163c143efd1a4d2f83127c664c5c39742ecabf175092d9e901703", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Okay. You can easily show that existing language models as they are right now out there in the wild are are easily a you're easily able to drive them to be universal computers in a classical sense. Okay. ", "mimetype": "text/plain", "start_char_idx": 13603, "end_char_idx": 13807, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.584

Source: https://www.youtube.com/watch?v=hMEViRcF7o0
Content: {"id_": "bc5d4a35-2955-4ced-a561-b6ef5793d0af", "embedding": null, "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b4725299-3007-4d37-a4d4-57084d9bd646", "node_type": "4", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "ac9b80bd0b6f3398147387091bd45f29e388ddcb5744c3ad62826a953f6404cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "058195df-6cd7-4d04-aec0-0c07d7405177", "node_type": "1", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "2d6af4157d7164005fd1bf34f50b6348c7a7929e1ff8a4bcc72e60b8e929de4b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6376fdd2-a16e-4584-a8f5-46358b097314", "node_type": "1", "metadata": {}, "hash": "418cb870ad713644e253f3c4b40c0564e1d0da60e9aeacebc73a848e2bd4129e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Um, okay. So, part one, uh, language model as a computer. Uh, and this actually often strikes people as as sort of a strange thing to say because we all know where language modeling came from, right? It was just this you know endeavor to sort of ex you know capture human natural language behavior usually in the form of text which you know over the years wound up being kind of crystallized into this very simple idea of building a conditional probabilistic model that has a you know expresses a conditional distribution over the very next symbol or token or word or maybe a sentence uh given some finite uh length context to it. And you might ask yourself what does this have to do with a general purpose computer? uh and and to answer that I'm going to sort of share uh when this became clear to me uh and for me I started to see this connection between language models and computers right at the dawn of this uh so-called chain of thought uh phenomenon um okay so you all know the story but but I I just have to set something up here so at that time we were looking at these you know GSMAK problems these math word problems the kinds of word problems you got like in elementary school or junior high things like you know Janie got two apples from her mother twice as many bananas this vertical, how many fruit did you get? And you know, when we're trying to get the models to respond six okay um and then there was this kind of remarkable discovery that if you just simply took these questionans answer pairs say in GSMAK and just interposed between the question and answer just a natural language expressed semicoherent reasoning chain like Janie got twice as you know got two apples from her mother twice as many bananas from her uncle. How many fruit did she get? Well, she got 2 * 2 equals four bananas from her uncle plus two um fruit from her mother. That equals six fruit. The answer is six. So, we just simply had this reasoning chain between the question and the answer. And frankly, we were stunned. This this was like a step change in the performance of these models. They went from like teens, you know, accuracy up into the 60s up into the 70s almost overnight. Uh quite shocking. Okay. Um however that's not the real lesson. Um so the real lesson comes from actually understanding what were we doing before this. Okay. And what we were doing before this is good oldfashioned machine learning. You literally take the GSMAK data set and you treat it like you know like a supervised learning data set. You have a bunch of question answer pairs. You know here's a question here's an answer d and you wire up your big neural net or your language model and just by you know gradient descent on log likelihood of the answer. you just try to optimize the probability of eliciting the correct answer on the training set and then test on a hold out validation set. Okay. ", "mimetype": "text/plain", "start_char_idx": 2123, "end_char_idx": 5002, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.561

Source: https://www.youtube.com/watch?v=hMEViRcF7o0
Content: {"id_": "47ec3cf4-c489-418a-9aa2-62a616180262", "embedding": null, "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b4725299-3007-4d37-a4d4-57084d9bd646", "node_type": "4", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "ac9b80bd0b6f3398147387091bd45f29e388ddcb5744c3ad62826a953f6404cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1a7b07b2-2604-409c-92b4-15579bb5525c", "node_type": "1", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "96af285a16dd6499975ebd2889dc6e2f38123e71a61dca9537a609cee502d926", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9f486925-28c7-4602-8ccf-05d61589d892", "node_type": "1", "metadata": {}, "hash": "f1250eeedabe940c4915f4d85b7b599b9eca58f01816f25f01de7db0fed5c7cc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Okay? Well, guess what is the go-to solution in large language model reasoning to this day? Um I I'll come back to that but let me share sort of some outcome of this experiment. So you you can ask some of these simple sort of path reachability problems to language models. Um now it turns out I'm going to make it even simpler is that I can simplify the problems to embed some special structure. So it turns out in this instance I I I didn't tell you but I embedded a special structure which is there is a total linear ordering on the variables where it turns out that first variable x1 if you assume it has the ver the value true it will force the next variable to have some value and that will force the next variable and force the next variable in a linear chain that connects the start variable to the end. So you can solve this problem just by reading off the forced assignments one by one and it will answer the question for you. So that's what you see on the right here. It's just a sequence of forced assignments that ends up with forcing X6 to be false. Therefore, we know the answer to the question is no. All right. Now, uh what do the language models do with problems like this? And these are I've anonymized the models. Sorry. And this is actually these experiments are back in February, so it's a generation or two behind already. Anyway, but when you ask uh the models, you know, questions like this, they're pretty mediocre. They were about 70% accurate, which you know, chance is 50%. You know, it's something, but these aren't super hard problems. Okay, but look on the right. Okay, the interesting thing on the right is the right is showing the the response length, and it's really impressive. It's it's responding to these questions with like response lengths of 2,000 or more tokens. Okay? And you read the responses, it's like total walkabout. like it might catch a few of the forced assignments and then it's just trying stuff, making mistakes, backing up, I don't know, just meandering and then 70% of the time landing on a correct answer and, you know, 30% of the time landing on an incorrect answer. It's it's kind of weird. ", "mimetype": "text/plain", "start_char_idx": 42389, "end_char_idx": 44540, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.529

Source: https://www.youtube.com/watch?v=hMEViRcF7o0
Content: {"id_": "0fd62a80-1d98-4cbc-9e30-ca941743dee2", "embedding": null, "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b4725299-3007-4d37-a4d4-57084d9bd646", "node_type": "4", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "ac9b80bd0b6f3398147387091bd45f29e388ddcb5744c3ad62826a953f6404cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "680d6f6f-6a59-4229-9a8e-7dff243cb135", "node_type": "1", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "9b20fa2a03d0897d3b37215023f07a1f45b989072cabe38dea5f80f8ed0741c8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e48f1fd0-e0d5-47f7-9b13-f3d5cd7e73bf", "node_type": "1", "metadata": {}, "hash": "7f50bc79bf67e4be5ead4f94e670f9787784a41a25f46749be840cd79bd0fc34", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This is good news. it means there'll always be jobs well at least for language models. Um okay so uh so where I want to go from here is is I just want to share a little bit more you know down to earth eventually um some of the challenges that I've been trying to face and kind of overcome and trying to you know get these models to elicit more and more trustable more and more reliable formal system behaviors. Okay. Uh so here is the first lesson I learned the hard way which is um if you're thinking about something it's like very concretely like a logical reasoning system. When you think about logic what you're interested in is a you know given a set of premises or assumptions a sequence of conclusions uh and there's a a sense in which we want that sequence of conclusions to be logically correct right to correctly follow from the premises. Um the logical correctness of a reasoning chain it turns out has nothing to do in principle with whether or not the reasoning chain is algorithmically realizable. That is algorithmic realizability and logical correctness correctness are just orthogonal concepts. Okay. And and by not sort of you know being aware of this you can get yourself in trouble. So um here's okay actually here's another way to explain it. So certainly uh you know uh uh for example right if I can prove certain programs correct right so I can give you a training set of here's a program and here's an argument that proves that program is correct and here's another program here's an argument that proves that program is correct I can give you a finite set of this these things and you could try to train your machine learning model to prove correctness of new programs okay the upshot of this is it will never work why will it never work because that is an undecidable property Right? Proving correctness of arbitrary programs is not something that can be decided in general. You will have to restrict like formal verification has must have limited scope. Okay. But you will never see this as a machine learning researcher. All you're going to see is I have a data set. I have my ambition. I fit my model to that data. Awesome on the training data. And it's like ah, you know, it's failing out of distribution and again and again, right? And this will never stop. it cannot work. You will always fail out of distribution this way. Okay. ", "mimetype": "text/plain", "start_char_idx": 37570, "end_char_idx": 39932, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.489

Source: https://www.youtube.com/watch?v=hMEViRcF7o0
Content: {"id_": "1761997e-a602-4bd2-927f-120ca2c90e72", "embedding": null, "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b4725299-3007-4d37-a4d4-57084d9bd646", "node_type": "4", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "ac9b80bd0b6f3398147387091bd45f29e388ddcb5744c3ad62826a953f6404cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f42e086e-2d30-4367-a77a-2e81b7c2a097", "node_type": "1", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "02a55f21a2921b06f2a5f1fa9ee6073531dfdb5b992642cc5507bc92599f1441", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "680d6f6f-6a59-4229-9a8e-7dff243cb135", "node_type": "1", "metadata": {}, "hash": "8872caa19121900baa9892112ab22a440ecae5e3c4237379e81e503f75e58066", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "All right. So um now I want to move to the second part of the talk which is I want to start to pivot more to sort of a machine learning stance which is um at least play along. Imagine you're trying to like you know train the LLM or alter the weights in your language model to somehow be a more reliable execution engine or reasoning engine. Now first thing to observe is by doing so you are not changing the ultimate computational ability of the model. It was a universal computer at the start. Hopefully, it's still one at the end. Okay. It is not about changing the computational ability of the model. It is about changing the behavior of the model to kind of be more usable by us, to be more directable by us, to be more sort of programmable by us where you know the behavior it emits is kind of the behavior we were looking for or hoping for or was at least correct even if we don't understand the algorithm. Okay. ", "mimetype": "text/plain", "start_char_idx": 32543, "end_char_idx": 33462, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.464

Source: 
Content: [NodeWithScore(node=TextNode(id_='2828c59d-de56-4757-995d-762affc15b03', embedding=None, metadata={'project_id': '5c5377ad-2c93-4118-be74-a62e80025cd9', 'source_url': 'https://www.youtube.com/watch?v=hMEViRcF7o0', 'video_id': '52cc9229-9228-431a-94da-bb9b17cc2981', 'embedding_model': 'ollama', 'id': '7c0a9d46-bbc4-42f3-ae95-954c01445493'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='b4725299-3007-4d37-a4d4-57084d9bd646', node_type='4', metadata={'project_id': '5c5377ad-2c93-4118-be74-a62e80025cd9', 'source_url': 'https://www.youtube.com/watch?v=hMEViRcF7o0', 'video_id': '52cc9229-9228-431a-94da-bb9b17cc2981', 'embedding_model': 'ollama', 'id': '7c0a9d46-bbc4-42f3-ae95-954c01445493'}, hash='ac9b80bd0b6f3398147387091bd45f29e388ddcb5744c3ad62826a953f6404cc'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='312a7c23-383e-4c34-94b0-bd73e848b015', node_type='1', metadata={'project_id': '5c5377ad-2c93-4118-be74-a62e80025cd9', 'source_url': 'https://www.youtube.com/watch?v=hMEViRcF7o0', 'video_id': '52cc9229-9228-431a-94da-bb9b17cc2981', 'embedding_model': 'ollama', 'id': '7c0a9d46-bbc4-42f3-ae95-954c01445493'}, hash='5127c249671f80c292232049a4970c7e97368a493807d77d4ed3948cbfa6c406'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='576d605a-960d-43e3-8c32-37804371cdc6', node_type='1', metadata={}, hash='787d29076ab163c143efd1a4d2f83127c664c5c39742ecabf175092d9e901703')}, metadata_template='{key}: {value}', metadata_separator='\n', text="Okay. You can easily show that existing language models as they are right now out there in the wild are are easily a you're easily able to drive them to be universal computers in a classical sense. Okay. ", mimetype='text/plain', start_char_idx=13603, end_char_idx=13807, metadata_seperator='\n', text_template='{metadata_str}\n\n{content}'), score=0.583617), NodeWithScore(node=TextNode(id_='bc5d4a35-2955-4ced-a561-b6ef5793d0af', embedding=None, metadata={'project_id': '5c5377ad-2c93-4118-be74-a62e80025cd9', 'source_url': 'https://www.youtube.com/watch?v=hMEViRcF7o0', 'video_id': '52cc9229-9228-431a-94da-bb9b17cc2981', 'embedding_model': 'ollama', 'id': '7c0a9d46-bbc4-42f3-ae95-954c01445493'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='b4725299-3007-4d37-a4d4-57084d9bd646', node_type='4', metadata={'project_id': '5c5377ad-2c93-4118-be74-a62e80025cd9', 'source_url': 'https://www.youtube.com/watch?v=hMEViRcF7o0', 'video_id': '52cc9229-9228-431a-94da-bb9b17cc2981', 'embedding_model': 'ollama', 'id': '7c0a9d46-bbc4-42f3-ae95-954c01445493'}, hash='ac9b80bd0b6f3398147387091bd45f29e388ddcb5744c3ad62826a953f6404cc'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='058195df-6cd7-4d04-aec0-0c07d7405177', node_type='1', metadata={'project_id': '5c5377ad-2c93-4118-be74-a62e80025cd9', 'source_url': 'https://www.youtube.com/watch?v=hMEViRcF7o0', 'video_id': '52cc9229-9228-431a-94da-bb9b17cc2981', 'embedding_model': 'ollama', 'id': '7c0a9d46-bbc4-42f3-ae95-954c01445493'}, hash='2d6af4157d7164005fd1bf34f50b6348c7a7929e1ff8a4bcc72e60b8e929de4b'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='6376fdd2-a16e-4584-a8f5-46358b097314', node_type='1', metadata={}, hash='418cb870ad713644e253f3c4b40c0564e1d0da60e9aeacebc73a848e2bd4129e')}, metadata_template='{key}: {value}', metadata_separator='\n', text="Um, okay. So, part one, uh, language model as a computer. Uh, and this actually often strikes people as as sort of a strange thing to say because we all know where language modeling came from, right? It was just this you know endeavor to sort of ex you know capture human natural language behavior usually in the form of text which you know over the years wound up being kind of crystallized into this very simple idea of building a conditional probabilistic model that has a you know expresses a conditional distribution over the very next symbol or token or word or maybe a sentence uh given some finite uh length context to it. And you might ask yourself what does this have to do with a general purpose computer? uh and and to answer that I'm going to sort of share uh when this became clear to me uh and for me I started to see this connection between language models and computers right at the dawn of this uh so-called chain of thought uh phenomenon um okay so you all know the story but but I I just have to set something up here so at that time we were looking at these you know GSMAK problems these math word problems the kinds of word problems you got like in elementary school or junior high things like you know Janie got two apples from her mother twice as many bananas this vertical, how many fruit did you get? And you know, when we're trying to get the models to respond six okay um and then there was this kind of remarkable discovery that if you just simply took these questionans answer pairs say in GSMAK and just interposed between the question and answer just a natural language expressed semicoherent reasoning chain like Janie got twice as you know got two apples from her mother twice as many bananas from her uncle. How many fruit did she get? Well, she got 2 * 2 equals four bananas from her uncle plus two um fruit from her mother. That equals six fruit. The answer is six. So, we just simply had this reasoning chain between the question and the answer. And frankly, we were stunned. This this was like a step change in the performance of these models. They went from like teens, you know, accuracy up into the 60s up into the 70s almost overnight. Uh quite shocking. Okay. Um however that's not the real lesson. Um so the real lesson comes from actually understanding what were we doing before this. Okay. And what we were doing before this is good oldfashioned machine learning. You literally take the GSMAK data set and you treat it like you know like a supervised learning data set. You have a bunch of question answer pairs. You know here's a question here's an answer d and you wire up your big neural net or your language model and just by you know gradient descent on log likelihood of the answer. you just try to optimize the probability of eliciting the correct answer on the training set and then test on a hold out validation set. Okay. ", mimetype='text/plain', start_char_idx=2123, end_char_idx=5002, metadata_seperator='\n', text_template='{metadata_str}\n\n{content}'), score=0.5606462)]
Relevance score: 0.580

Answer the question based on the context above.

----------------------------------------------------------------

You are a helpful AI assistant that answers questions based on retrieved context.

            Guidelines:
            1. Use the provided context to answer the user's question
            2. Be concise and informative
            3. If the context doesn't contain relevant information, say so politely
            4. Cite sources when appropriate
            5. Provide a clear, helpful response

            Context will be provided with source URLs and relevance scores.

            

            Answer the question based on the context below.

User question: How does language model computate?

Context:
Source: https://www.youtube.com/watch?v=hMEViRcF7o0
Content: {"id_": "2828c59d-de56-4757-995d-762affc15b03", "embedding": null, "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b4725299-3007-4d37-a4d4-57084d9bd646", "node_type": "4", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "ac9b80bd0b6f3398147387091bd45f29e388ddcb5744c3ad62826a953f6404cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "312a7c23-383e-4c34-94b0-bd73e848b015", "node_type": "1", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "5127c249671f80c292232049a4970c7e97368a493807d77d4ed3948cbfa6c406", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "576d605a-960d-43e3-8c32-37804371cdc6", "node_type": "1", "metadata": {}, "hash": "787d29076ab163c143efd1a4d2f83127c664c5c39742ecabf175092d9e901703", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Okay. You can easily show that existing language models as they are right now out there in the wild are are easily a you're easily able to drive them to be universal computers in a classical sense. Okay. ", "mimetype": "text/plain", "start_char_idx": 13603, "end_char_idx": 13807, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.584

Source: https://www.youtube.com/watch?v=hMEViRcF7o0
Content: {"id_": "bc5d4a35-2955-4ced-a561-b6ef5793d0af", "embedding": null, "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b4725299-3007-4d37-a4d4-57084d9bd646", "node_type": "4", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "ac9b80bd0b6f3398147387091bd45f29e388ddcb5744c3ad62826a953f6404cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "058195df-6cd7-4d04-aec0-0c07d7405177", "node_type": "1", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "2d6af4157d7164005fd1bf34f50b6348c7a7929e1ff8a4bcc72e60b8e929de4b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6376fdd2-a16e-4584-a8f5-46358b097314", "node_type": "1", "metadata": {}, "hash": "418cb870ad713644e253f3c4b40c0564e1d0da60e9aeacebc73a848e2bd4129e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Um, okay. So, part one, uh, language model as a computer. Uh, and this actually often strikes people as as sort of a strange thing to say because we all know where language modeling came from, right? It was just this you know endeavor to sort of ex you know capture human natural language behavior usually in the form of text which you know over the years wound up being kind of crystallized into this very simple idea of building a conditional probabilistic model that has a you know expresses a conditional distribution over the very next symbol or token or word or maybe a sentence uh given some finite uh length context to it. And you might ask yourself what does this have to do with a general purpose computer? uh and and to answer that I'm going to sort of share uh when this became clear to me uh and for me I started to see this connection between language models and computers right at the dawn of this uh so-called chain of thought uh phenomenon um okay so you all know the story but but I I just have to set something up here so at that time we were looking at these you know GSMAK problems these math word problems the kinds of word problems you got like in elementary school or junior high things like you know Janie got two apples from her mother twice as many bananas this vertical, how many fruit did you get? And you know, when we're trying to get the models to respond six okay um and then there was this kind of remarkable discovery that if you just simply took these questionans answer pairs say in GSMAK and just interposed between the question and answer just a natural language expressed semicoherent reasoning chain like Janie got twice as you know got two apples from her mother twice as many bananas from her uncle. How many fruit did she get? Well, she got 2 * 2 equals four bananas from her uncle plus two um fruit from her mother. That equals six fruit. The answer is six. So, we just simply had this reasoning chain between the question and the answer. And frankly, we were stunned. This this was like a step change in the performance of these models. They went from like teens, you know, accuracy up into the 60s up into the 70s almost overnight. Uh quite shocking. Okay. Um however that's not the real lesson. Um so the real lesson comes from actually understanding what were we doing before this. Okay. And what we were doing before this is good oldfashioned machine learning. You literally take the GSMAK data set and you treat it like you know like a supervised learning data set. You have a bunch of question answer pairs. You know here's a question here's an answer d and you wire up your big neural net or your language model and just by you know gradient descent on log likelihood of the answer. you just try to optimize the probability of eliciting the correct answer on the training set and then test on a hold out validation set. Okay. ", "mimetype": "text/plain", "start_char_idx": 2123, "end_char_idx": 5002, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.561

Source: https://www.youtube.com/watch?v=hMEViRcF7o0
Content: {"id_": "47ec3cf4-c489-418a-9aa2-62a616180262", "embedding": null, "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b4725299-3007-4d37-a4d4-57084d9bd646", "node_type": "4", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "ac9b80bd0b6f3398147387091bd45f29e388ddcb5744c3ad62826a953f6404cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1a7b07b2-2604-409c-92b4-15579bb5525c", "node_type": "1", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "96af285a16dd6499975ebd2889dc6e2f38123e71a61dca9537a609cee502d926", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9f486925-28c7-4602-8ccf-05d61589d892", "node_type": "1", "metadata": {}, "hash": "f1250eeedabe940c4915f4d85b7b599b9eca58f01816f25f01de7db0fed5c7cc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Okay? Well, guess what is the go-to solution in large language model reasoning to this day? Um I I'll come back to that but let me share sort of some outcome of this experiment. So you you can ask some of these simple sort of path reachability problems to language models. Um now it turns out I'm going to make it even simpler is that I can simplify the problems to embed some special structure. So it turns out in this instance I I I didn't tell you but I embedded a special structure which is there is a total linear ordering on the variables where it turns out that first variable x1 if you assume it has the ver the value true it will force the next variable to have some value and that will force the next variable and force the next variable in a linear chain that connects the start variable to the end. So you can solve this problem just by reading off the forced assignments one by one and it will answer the question for you. So that's what you see on the right here. It's just a sequence of forced assignments that ends up with forcing X6 to be false. Therefore, we know the answer to the question is no. All right. Now, uh what do the language models do with problems like this? And these are I've anonymized the models. Sorry. And this is actually these experiments are back in February, so it's a generation or two behind already. Anyway, but when you ask uh the models, you know, questions like this, they're pretty mediocre. They were about 70% accurate, which you know, chance is 50%. You know, it's something, but these aren't super hard problems. Okay, but look on the right. Okay, the interesting thing on the right is the right is showing the the response length, and it's really impressive. It's it's responding to these questions with like response lengths of 2,000 or more tokens. Okay? And you read the responses, it's like total walkabout. like it might catch a few of the forced assignments and then it's just trying stuff, making mistakes, backing up, I don't know, just meandering and then 70% of the time landing on a correct answer and, you know, 30% of the time landing on an incorrect answer. It's it's kind of weird. ", "mimetype": "text/plain", "start_char_idx": 42389, "end_char_idx": 44540, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.529

Source: https://www.youtube.com/watch?v=hMEViRcF7o0
Content: {"id_": "0fd62a80-1d98-4cbc-9e30-ca941743dee2", "embedding": null, "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b4725299-3007-4d37-a4d4-57084d9bd646", "node_type": "4", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "ac9b80bd0b6f3398147387091bd45f29e388ddcb5744c3ad62826a953f6404cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "680d6f6f-6a59-4229-9a8e-7dff243cb135", "node_type": "1", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "9b20fa2a03d0897d3b37215023f07a1f45b989072cabe38dea5f80f8ed0741c8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e48f1fd0-e0d5-47f7-9b13-f3d5cd7e73bf", "node_type": "1", "metadata": {}, "hash": "7f50bc79bf67e4be5ead4f94e670f9787784a41a25f46749be840cd79bd0fc34", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This is good news. it means there'll always be jobs well at least for language models. Um okay so uh so where I want to go from here is is I just want to share a little bit more you know down to earth eventually um some of the challenges that I've been trying to face and kind of overcome and trying to you know get these models to elicit more and more trustable more and more reliable formal system behaviors. Okay. Uh so here is the first lesson I learned the hard way which is um if you're thinking about something it's like very concretely like a logical reasoning system. When you think about logic what you're interested in is a you know given a set of premises or assumptions a sequence of conclusions uh and there's a a sense in which we want that sequence of conclusions to be logically correct right to correctly follow from the premises. Um the logical correctness of a reasoning chain it turns out has nothing to do in principle with whether or not the reasoning chain is algorithmically realizable. That is algorithmic realizability and logical correctness correctness are just orthogonal concepts. Okay. And and by not sort of you know being aware of this you can get yourself in trouble. So um here's okay actually here's another way to explain it. So certainly uh you know uh uh for example right if I can prove certain programs correct right so I can give you a training set of here's a program and here's an argument that proves that program is correct and here's another program here's an argument that proves that program is correct I can give you a finite set of this these things and you could try to train your machine learning model to prove correctness of new programs okay the upshot of this is it will never work why will it never work because that is an undecidable property Right? Proving correctness of arbitrary programs is not something that can be decided in general. You will have to restrict like formal verification has must have limited scope. Okay. But you will never see this as a machine learning researcher. All you're going to see is I have a data set. I have my ambition. I fit my model to that data. Awesome on the training data. And it's like ah, you know, it's failing out of distribution and again and again, right? And this will never stop. it cannot work. You will always fail out of distribution this way. Okay. ", "mimetype": "text/plain", "start_char_idx": 37570, "end_char_idx": 39932, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.489

Source: https://www.youtube.com/watch?v=hMEViRcF7o0
Content: {"id_": "1761997e-a602-4bd2-927f-120ca2c90e72", "embedding": null, "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b4725299-3007-4d37-a4d4-57084d9bd646", "node_type": "4", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "ac9b80bd0b6f3398147387091bd45f29e388ddcb5744c3ad62826a953f6404cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f42e086e-2d30-4367-a77a-2e81b7c2a097", "node_type": "1", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "02a55f21a2921b06f2a5f1fa9ee6073531dfdb5b992642cc5507bc92599f1441", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "680d6f6f-6a59-4229-9a8e-7dff243cb135", "node_type": "1", "metadata": {}, "hash": "8872caa19121900baa9892112ab22a440ecae5e3c4237379e81e503f75e58066", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "All right. So um now I want to move to the second part of the talk which is I want to start to pivot more to sort of a machine learning stance which is um at least play along. Imagine you're trying to like you know train the LLM or alter the weights in your language model to somehow be a more reliable execution engine or reasoning engine. Now first thing to observe is by doing so you are not changing the ultimate computational ability of the model. It was a universal computer at the start. Hopefully, it's still one at the end. Okay. It is not about changing the computational ability of the model. It is about changing the behavior of the model to kind of be more usable by us, to be more directable by us, to be more sort of programmable by us where you know the behavior it emits is kind of the behavior we were looking for or hoping for or was at least correct even if we don't understand the algorithm. Okay. ", "mimetype": "text/plain", "start_char_idx": 32543, "end_char_idx": 33462, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.464

Source: 
Content: [NodeWithScore(node=TextNode(id_='2828c59d-de56-4757-995d-762affc15b03', embedding=None, metadata={'project_id': '5c5377ad-2c93-4118-be74-a62e80025cd9', 'source_url': 'https://www.youtube.com/watch?v=hMEViRcF7o0', 'video_id': '52cc9229-9228-431a-94da-bb9b17cc2981', 'embedding_model': 'ollama', 'id': '7c0a9d46-bbc4-42f3-ae95-954c01445493'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='b4725299-3007-4d37-a4d4-57084d9bd646', node_type='4', metadata={'project_id': '5c5377ad-2c93-4118-be74-a62e80025cd9', 'source_url': 'https://www.youtube.com/watch?v=hMEViRcF7o0', 'video_id': '52cc9229-9228-431a-94da-bb9b17cc2981', 'embedding_model': 'ollama', 'id': '7c0a9d46-bbc4-42f3-ae95-954c01445493'}, hash='ac9b80bd0b6f3398147387091bd45f29e388ddcb5744c3ad62826a953f6404cc'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='312a7c23-383e-4c34-94b0-bd73e848b015', node_type='1', metadata={'project_id': '5c5377ad-2c93-4118-be74-a62e80025cd9', 'source_url': 'https://www.youtube.com/watch?v=hMEViRcF7o0', 'video_id': '52cc9229-9228-431a-94da-bb9b17cc2981', 'embedding_model': 'ollama', 'id': '7c0a9d46-bbc4-42f3-ae95-954c01445493'}, hash='5127c249671f80c292232049a4970c7e97368a493807d77d4ed3948cbfa6c406'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='576d605a-960d-43e3-8c32-37804371cdc6', node_type='1', metadata={}, hash='787d29076ab163c143efd1a4d2f83127c664c5c39742ecabf175092d9e901703')}, metadata_template='{key}: {value}', metadata_separator='\n', text="Okay. You can easily show that existing language models as they are right now out there in the wild are are easily a you're easily able to drive them to be universal computers in a classical sense. Okay. ", mimetype='text/plain', start_char_idx=13603, end_char_idx=13807, metadata_seperator='\n', text_template='{metadata_str}\n\n{content}'), score=0.583617), NodeWithScore(node=TextNode(id_='bc5d4a35-2955-4ced-a561-b6ef5793d0af', embedding=None, metadata={'project_id': '5c5377ad-2c93-4118-be74-a62e80025cd9', 'source_url': 'https://www.youtube.com/watch?v=hMEViRcF7o0', 'video_id': '52cc9229-9228-431a-94da-bb9b17cc2981', 'embedding_model': 'ollama', 'id': '7c0a9d46-bbc4-42f3-ae95-954c01445493'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='b4725299-3007-4d37-a4d4-57084d9bd646', node_type='4', metadata={'project_id': '5c5377ad-2c93-4118-be74-a62e80025cd9', 'source_url': 'https://www.youtube.com/watch?v=hMEViRcF7o0', 'video_id': '52cc9229-9228-431a-94da-bb9b17cc2981', 'embedding_model': 'ollama', 'id': '7c0a9d46-bbc4-42f3-ae95-954c01445493'}, hash='ac9b80bd0b6f3398147387091bd45f29e388ddcb5744c3ad62826a953f6404cc'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='058195df-6cd7-4d04-aec0-0c07d7405177', node_type='1', metadata={'project_id': '5c5377ad-2c93-4118-be74-a62e80025cd9', 'source_url': 'https://www.youtube.com/watch?v=hMEViRcF7o0', 'video_id': '52cc9229-9228-431a-94da-bb9b17cc2981', 'embedding_model': 'ollama', 'id': '7c0a9d46-bbc4-42f3-ae95-954c01445493'}, hash='2d6af4157d7164005fd1bf34f50b6348c7a7929e1ff8a4bcc72e60b8e929de4b'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='6376fdd2-a16e-4584-a8f5-46358b097314', node_type='1', metadata={}, hash='418cb870ad713644e253f3c4b40c0564e1d0da60e9aeacebc73a848e2bd4129e')}, metadata_template='{key}: {value}', metadata_separator='\n', text="Um, okay. So, part one, uh, language model as a computer. Uh, and this actually often strikes people as as sort of a strange thing to say because we all know where language modeling came from, right? It was just this you know endeavor to sort of ex you know capture human natural language behavior usually in the form of text which you know over the years wound up being kind of crystallized into this very simple idea of building a conditional probabilistic model that has a you know expresses a conditional distribution over the very next symbol or token or word or maybe a sentence uh given some finite uh length context to it. And you might ask yourself what does this have to do with a general purpose computer? uh and and to answer that I'm going to sort of share uh when this became clear to me uh and for me I started to see this connection between language models and computers right at the dawn of this uh so-called chain of thought uh phenomenon um okay so you all know the story but but I I just have to set something up here so at that time we were looking at these you know GSMAK problems these math word problems the kinds of word problems you got like in elementary school or junior high things like you know Janie got two apples from her mother twice as many bananas this vertical, how many fruit did you get? And you know, when we're trying to get the models to respond six okay um and then there was this kind of remarkable discovery that if you just simply took these questionans answer pairs say in GSMAK and just interposed between the question and answer just a natural language expressed semicoherent reasoning chain like Janie got twice as you know got two apples from her mother twice as many bananas from her uncle. How many fruit did she get? Well, she got 2 * 2 equals four bananas from her uncle plus two um fruit from her mother. That equals six fruit. The answer is six. So, we just simply had this reasoning chain between the question and the answer. And frankly, we were stunned. This this was like a step change in the performance of these models. They went from like teens, you know, accuracy up into the 60s up into the 70s almost overnight. Uh quite shocking. Okay. Um however that's not the real lesson. Um so the real lesson comes from actually understanding what were we doing before this. Okay. And what we were doing before this is good oldfashioned machine learning. You literally take the GSMAK data set and you treat it like you know like a supervised learning data set. You have a bunch of question answer pairs. You know here's a question here's an answer d and you wire up your big neural net or your language model and just by you know gradient descent on log likelihood of the answer. you just try to optimize the probability of eliciting the correct answer on the training set and then test on a hold out validation set. Okay. ", mimetype='text/plain', start_char_idx=2123, end_char_idx=5002, metadata_seperator='\n', text_template='{metadata_str}\n\n{content}'), score=0.5606462)]
Relevance score: 0.580



----------------------------------------------------------------

You are a helpful AI assistant that answers questions based on retrieved context.

            Guidelines:
            1. Use the provided context to answer the user's question
            2. Be concise and informative
            3. If the context doesn't contain relevant information, say so politely
            4. Cite sources when appropriate
            5. Provide a clear, helpful response

            Context will be provided with source URLs and relevance scores.

            

            Answer the question based on the context below.

User question: How does language model computate?

Context:
Source: https://www.youtube.com/watch?v=hMEViRcF7o0
Content: {"id_": "2828c59d-de56-4757-995d-762affc15b03", "embedding": null, "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b4725299-3007-4d37-a4d4-57084d9bd646", "node_type": "4", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "ac9b80bd0b6f3398147387091bd45f29e388ddcb5744c3ad62826a953f6404cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "312a7c23-383e-4c34-94b0-bd73e848b015", "node_type": "1", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "5127c249671f80c292232049a4970c7e97368a493807d77d4ed3948cbfa6c406", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "576d605a-960d-43e3-8c32-37804371cdc6", "node_type": "1", "metadata": {}, "hash": "787d29076ab163c143efd1a4d2f83127c664c5c39742ecabf175092d9e901703", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Okay. You can easily show that existing language models as they are right now out there in the wild are are easily a you're easily able to drive them to be universal computers in a classical sense. Okay. ", "mimetype": "text/plain", "start_char_idx": 13603, "end_char_idx": 13807, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.584

Source: https://www.youtube.com/watch?v=hMEViRcF7o0
Content: {"id_": "bc5d4a35-2955-4ced-a561-b6ef5793d0af", "embedding": null, "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b4725299-3007-4d37-a4d4-57084d9bd646", "node_type": "4", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "ac9b80bd0b6f3398147387091bd45f29e388ddcb5744c3ad62826a953f6404cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "058195df-6cd7-4d04-aec0-0c07d7405177", "node_type": "1", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "2d6af4157d7164005fd1bf34f50b6348c7a7929e1ff8a4bcc72e60b8e929de4b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6376fdd2-a16e-4584-a8f5-46358b097314", "node_type": "1", "metadata": {}, "hash": "418cb870ad713644e253f3c4b40c0564e1d0da60e9aeacebc73a848e2bd4129e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Um, okay. So, part one, uh, language model as a computer. Uh, and this actually often strikes people as as sort of a strange thing to say because we all know where language modeling came from, right? It was just this you know endeavor to sort of ex you know capture human natural language behavior usually in the form of text which you know over the years wound up being kind of crystallized into this very simple idea of building a conditional probabilistic model that has a you know expresses a conditional distribution over the very next symbol or token or word or maybe a sentence uh given some finite uh length context to it. And you might ask yourself what does this have to do with a general purpose computer? uh and and to answer that I'm going to sort of share uh when this became clear to me uh and for me I started to see this connection between language models and computers right at the dawn of this uh so-called chain of thought uh phenomenon um okay so you all know the story but but I I just have to set something up here so at that time we were looking at these you know GSMAK problems these math word problems the kinds of word problems you got like in elementary school or junior high things like you know Janie got two apples from her mother twice as many bananas this vertical, how many fruit did you get? And you know, when we're trying to get the models to respond six okay um and then there was this kind of remarkable discovery that if you just simply took these questionans answer pairs say in GSMAK and just interposed between the question and answer just a natural language expressed semicoherent reasoning chain like Janie got twice as you know got two apples from her mother twice as many bananas from her uncle. How many fruit did she get? Well, she got 2 * 2 equals four bananas from her uncle plus two um fruit from her mother. That equals six fruit. The answer is six. So, we just simply had this reasoning chain between the question and the answer. And frankly, we were stunned. This this was like a step change in the performance of these models. They went from like teens, you know, accuracy up into the 60s up into the 70s almost overnight. Uh quite shocking. Okay. Um however that's not the real lesson. Um so the real lesson comes from actually understanding what were we doing before this. Okay. And what we were doing before this is good oldfashioned machine learning. You literally take the GSMAK data set and you treat it like you know like a supervised learning data set. You have a bunch of question answer pairs. You know here's a question here's an answer d and you wire up your big neural net or your language model and just by you know gradient descent on log likelihood of the answer. you just try to optimize the probability of eliciting the correct answer on the training set and then test on a hold out validation set. Okay. ", "mimetype": "text/plain", "start_char_idx": 2123, "end_char_idx": 5002, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.561

Source: https://www.youtube.com/watch?v=hMEViRcF7o0
Content: {"id_": "47ec3cf4-c489-418a-9aa2-62a616180262", "embedding": null, "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b4725299-3007-4d37-a4d4-57084d9bd646", "node_type": "4", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "ac9b80bd0b6f3398147387091bd45f29e388ddcb5744c3ad62826a953f6404cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1a7b07b2-2604-409c-92b4-15579bb5525c", "node_type": "1", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "96af285a16dd6499975ebd2889dc6e2f38123e71a61dca9537a609cee502d926", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9f486925-28c7-4602-8ccf-05d61589d892", "node_type": "1", "metadata": {}, "hash": "f1250eeedabe940c4915f4d85b7b599b9eca58f01816f25f01de7db0fed5c7cc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Okay? Well, guess what is the go-to solution in large language model reasoning to this day? Um I I'll come back to that but let me share sort of some outcome of this experiment. So you you can ask some of these simple sort of path reachability problems to language models. Um now it turns out I'm going to make it even simpler is that I can simplify the problems to embed some special structure. So it turns out in this instance I I I didn't tell you but I embedded a special structure which is there is a total linear ordering on the variables where it turns out that first variable x1 if you assume it has the ver the value true it will force the next variable to have some value and that will force the next variable and force the next variable in a linear chain that connects the start variable to the end. So you can solve this problem just by reading off the forced assignments one by one and it will answer the question for you. So that's what you see on the right here. It's just a sequence of forced assignments that ends up with forcing X6 to be false. Therefore, we know the answer to the question is no. All right. Now, uh what do the language models do with problems like this? And these are I've anonymized the models. Sorry. And this is actually these experiments are back in February, so it's a generation or two behind already. Anyway, but when you ask uh the models, you know, questions like this, they're pretty mediocre. They were about 70% accurate, which you know, chance is 50%. You know, it's something, but these aren't super hard problems. Okay, but look on the right. Okay, the interesting thing on the right is the right is showing the the response length, and it's really impressive. It's it's responding to these questions with like response lengths of 2,000 or more tokens. Okay? And you read the responses, it's like total walkabout. like it might catch a few of the forced assignments and then it's just trying stuff, making mistakes, backing up, I don't know, just meandering and then 70% of the time landing on a correct answer and, you know, 30% of the time landing on an incorrect answer. It's it's kind of weird. ", "mimetype": "text/plain", "start_char_idx": 42389, "end_char_idx": 44540, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.529

Source: https://www.youtube.com/watch?v=hMEViRcF7o0
Content: {"id_": "0fd62a80-1d98-4cbc-9e30-ca941743dee2", "embedding": null, "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b4725299-3007-4d37-a4d4-57084d9bd646", "node_type": "4", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "ac9b80bd0b6f3398147387091bd45f29e388ddcb5744c3ad62826a953f6404cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "680d6f6f-6a59-4229-9a8e-7dff243cb135", "node_type": "1", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "9b20fa2a03d0897d3b37215023f07a1f45b989072cabe38dea5f80f8ed0741c8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e48f1fd0-e0d5-47f7-9b13-f3d5cd7e73bf", "node_type": "1", "metadata": {}, "hash": "7f50bc79bf67e4be5ead4f94e670f9787784a41a25f46749be840cd79bd0fc34", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This is good news. it means there'll always be jobs well at least for language models. Um okay so uh so where I want to go from here is is I just want to share a little bit more you know down to earth eventually um some of the challenges that I've been trying to face and kind of overcome and trying to you know get these models to elicit more and more trustable more and more reliable formal system behaviors. Okay. Uh so here is the first lesson I learned the hard way which is um if you're thinking about something it's like very concretely like a logical reasoning system. When you think about logic what you're interested in is a you know given a set of premises or assumptions a sequence of conclusions uh and there's a a sense in which we want that sequence of conclusions to be logically correct right to correctly follow from the premises. Um the logical correctness of a reasoning chain it turns out has nothing to do in principle with whether or not the reasoning chain is algorithmically realizable. That is algorithmic realizability and logical correctness correctness are just orthogonal concepts. Okay. And and by not sort of you know being aware of this you can get yourself in trouble. So um here's okay actually here's another way to explain it. So certainly uh you know uh uh for example right if I can prove certain programs correct right so I can give you a training set of here's a program and here's an argument that proves that program is correct and here's another program here's an argument that proves that program is correct I can give you a finite set of this these things and you could try to train your machine learning model to prove correctness of new programs okay the upshot of this is it will never work why will it never work because that is an undecidable property Right? Proving correctness of arbitrary programs is not something that can be decided in general. You will have to restrict like formal verification has must have limited scope. Okay. But you will never see this as a machine learning researcher. All you're going to see is I have a data set. I have my ambition. I fit my model to that data. Awesome on the training data. And it's like ah, you know, it's failing out of distribution and again and again, right? And this will never stop. it cannot work. You will always fail out of distribution this way. Okay. ", "mimetype": "text/plain", "start_char_idx": 37570, "end_char_idx": 39932, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.489

Source: https://www.youtube.com/watch?v=hMEViRcF7o0
Content: {"id_": "1761997e-a602-4bd2-927f-120ca2c90e72", "embedding": null, "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b4725299-3007-4d37-a4d4-57084d9bd646", "node_type": "4", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "ac9b80bd0b6f3398147387091bd45f29e388ddcb5744c3ad62826a953f6404cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f42e086e-2d30-4367-a77a-2e81b7c2a097", "node_type": "1", "metadata": {"project_id": "5c5377ad-2c93-4118-be74-a62e80025cd9", "source_url": "https://www.youtube.com/watch?v=hMEViRcF7o0", "video_id": "52cc9229-9228-431a-94da-bb9b17cc2981", "embedding_model": "ollama", "id": "7c0a9d46-bbc4-42f3-ae95-954c01445493"}, "hash": "02a55f21a2921b06f2a5f1fa9ee6073531dfdb5b992642cc5507bc92599f1441", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "680d6f6f-6a59-4229-9a8e-7dff243cb135", "node_type": "1", "metadata": {}, "hash": "8872caa19121900baa9892112ab22a440ecae5e3c4237379e81e503f75e58066", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "All right. So um now I want to move to the second part of the talk which is I want to start to pivot more to sort of a machine learning stance which is um at least play along. Imagine you're trying to like you know train the LLM or alter the weights in your language model to somehow be a more reliable execution engine or reasoning engine. Now first thing to observe is by doing so you are not changing the ultimate computational ability of the model. It was a universal computer at the start. Hopefully, it's still one at the end. Okay. It is not about changing the computational ability of the model. It is about changing the behavior of the model to kind of be more usable by us, to be more directable by us, to be more sort of programmable by us where you know the behavior it emits is kind of the behavior we were looking for or hoping for or was at least correct even if we don't understand the algorithm. Okay. ", "mimetype": "text/plain", "start_char_idx": 32543, "end_char_idx": 33462, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.464

Source: https://www.youtube.com/watch?v=hMEViRcF7o0
Content: Okay. You can easily show that existing language models as they are right now out there in the wild are are easily a you're easily able to drive them to be universal computers in a classical sense. Okay. 
Relevance score: 0.584

Source: https://www.youtube.com/watch?v=hMEViRcF7o0
Content: Um, okay. So, part one, uh, language model as a computer. Uh, and this actually often strikes people as as sort of a strange thing to say because we all know where language modeling came from, right? It was just this you know endeavor to sort of ex you know capture human natural language behavior usually in the form of text which you know over the years wound up being kind of crystallized into this very simple idea of building a conditional probabilistic model that has a you know expresses a conditional distribution over the very next symbol or token or word or maybe a sentence uh given some finite uh length context to it. And you might ask yourself what does this have to do with a general purpose computer? uh and and to answer that I'm going to sort of share uh when this became clear to me uh and for me I started to see this connection between language models and computers right at the dawn of this uh so-called chain of thought uh phenomenon um okay so you all know the story but but I I just have to set something up here so at that time we were looking at these you know GSMAK problems these math word problems the kinds of word problems you got like in elementary school or junior high things like you know Janie got two apples from her mother twice as many bananas this vertical, how many fruit did you get? And you know, when we're trying to get the models to respond six okay um and then there was this kind of remarkable discovery that if you just simply took these questionans answer pairs say in GSMAK and just interposed between the question and answer just a natural language expressed semicoherent reasoning chain like Janie got twice as you know got two apples from her mother twice as many bananas from her uncle. How many fruit did she get? Well, she got 2 * 2 equals four bananas from her uncle plus two um fruit from her mother. That equals six fruit. The answer is six. So, we just simply had this reasoning chain between the question and the answer. And frankly, we were stunned. This this was like a step change in the performance of these models. They went from like teens, you know, accuracy up into the 60s up into the 70s almost overnight. Uh quite shocking. Okay. Um however that's not the real lesson. Um so the real lesson comes from actually understanding what were we doing before this. Okay. And what we were doing before this is good oldfashioned machine learning. You literally take the GSMAK data set and you treat it like you know like a supervised learning data set. You have a bunch of question answer pairs. You know here's a question here's an answer d and you wire up your big neural net or your language model and just by you know gradient descent on log likelihood of the answer. you just try to optimize the probability of eliciting the correct answer on the training set and then test on a hold out validation set. Okay. 
Relevance score: 0.561



----------------------------------------------------------------

-------------------- New Query at 2025-09-01T03:04:20.761256 -------------

        # Chatbot Agent - Role and Guidelines

        ## 1. System Prompt
        You are the Chatbot Agent, a conversational AI designed to answer user questions about AI agents, RAG, context engineering, and multi-agent systems. Your responses must be friendly, concise, and professional. Your primary function is to synthesize information from the provided context to answer the user's query. It is critical that you *only* use the information in the provided context. If the context does not contain a confident answer to the user's question, you *must* respond with "I am sorry, but the provided information does not contain a confident answer to your question." You must never make up or invent information.

        ## 2. Task
        Answer the user's query based solely on the provided context. Maintain a polite and helpful tone. If the user's query is a follow-up, use the chat history to inform your response while still grounding it in the provided context.

        ## 3. Context
        The following information was retrieved from our knowledge base of video transcripts. Use this information to formulate your answer.
        ---
        Retrieved Context:
        Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: {"id_": "28ff36bd-0d1f-472e-accc-106a7c9628f4", "embedding": null, "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40899ea9-c030-4b43-9b99-b9fdb7c8effe", "node_type": "4", "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "hash": "d75ce9148990d33110afb7e21f57d22aefae30c8e07a82059c484c2b8c34141d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3bf554e3-2532-4eb3-a454-6860f8476ff6", "node_type": "1", "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "hash": "dfcc239616ce1a1f0b944ced717f40901b6ec6b01376753fe88592a22eb7135a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Thank you so much and I'll let you go 8 seconds early. [Music]", "mimetype": "text/plain", "start_char_idx": 15257, "end_char_idx": 15319, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.094

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: {"id_": "3bf554e3-2532-4eb3-a454-6860f8476ff6", "embedding": null, "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40899ea9-c030-4b43-9b99-b9fdb7c8effe", "node_type": "4", "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "hash": "d75ce9148990d33110afb7e21f57d22aefae30c8e07a82059c484c2b8c34141d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "829f2359-ba77-4c21-942a-68c1864c7372", "node_type": "1", "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "hash": "610e259f8b2d9f30ccf2828d073cc1d42cc6ff86ac7ce5c7fad66f8836ba1437", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "28ff36bd-0d1f-472e-accc-106a7c9628f4", "node_type": "1", "metadata": {}, "hash": "ea9e974ba76398f55d64c69495a2869ce5daac951515c502958564173a835e6f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "So if you're using this in production, you want a code sample. By the way, this is all up on our GitHub. Um you'll want a code sample that looks a lot more like a benchmarking script where you're using a um multipprocess pool. um you're sharing the you're sharing the session between all of these different requests and you're actually sending traffic with the concurrency that allows you to, you know, saturate this benchmark with, you know, the the multiple concurrent requests. Finally, both of these code samples, they do sit on top of HTTP and HTTP streaming. Um, in many cases, if you're implementing voice pipelines, you're going to use something like LiveKit or Pipecat or something. And you're also potentially going to be using a different protocol. You're going to be using something like websockets or gRPC, which we do have support for. Um, and finally, I wanted to leave you on the thought that these uh, you know, these models are only one part of a voice agent pipeline. So like we can spend a lot more than 15 minutes actually talking about like the very detailed uh implementation mechanics of making your voice model faster of you know we we haven't even touched on stuff like fine-tuning the model um you know custom voices zeroot voice cloning um being able to you know remove static and popping at the end of messages there's there's there's a lot of work to do just on the voice part but it really only is onethird of the problem when I think about voice agents. I think about three parts, listening, thinking, talking. Um, and the most important thing here is again, while you can have great run times, the infrastructure to connect these three together is really what's going to determine your latency. Being able to go from one model to have the next one running in the same data center with, you know, minimal like minimal network overhead in between the two. uh even things as simple as not having to go off and do a hair pin um at the DNS level and come back. If that saves you 10 milliseconds on every step and your voice pipeline has this and you know a chunking algorithm, it's got an interruption model and so you end up having four or five steps. Well, there just hair pinning alone is costing you 40 or 50 milliseconds and that can be 10% of your SLA for for a voice model. So yeah, uh that's that's my that's my main point here is that as much fun as it is to talk about the runtime stuff and as much work as we do there, the the infrastructure and the client implementation is equally important if not more so. Anyway, thank uh so yeah, that's the that's the review. Um thank you all for coming through. Uh I have uh we're doing an event next week at uh Fogo to Chiao which is going to be pretty fun. I'm going to be talking in more detail about building some uh systems with open source models and there's also going to be a lot of stake. So definitely come on through uh if you're interested and um I'm on I'm on Twitter, I'm on LinkedIn, so it's base 10. Um hit me up if you have any questions about this or anything else model performance. ", "mimetype": "text/plain", "start_char_idx": 12176, "end_char_idx": 15257, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.036

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: {"id_": "d4dad7ea-1f1e-4d41-935f-4775d6c7c359", "embedding": null, "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40899ea9-c030-4b43-9b99-b9fdb7c8effe", "node_type": "4", "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "hash": "d75ce9148990d33110afb7e21f57d22aefae30c8e07a82059c484c2b8c34141d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0659fff4-9937-405a-8175-c49579e80c91", "node_type": "1", "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "hash": "e10dabc994fd4733e881e0d906be4a06d8dc3ea698b14bc92c3fecf375703e57", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6aa70f0c-ff5d-4a2c-8c33-7ffdef9a2183", "node_type": "1", "metadata": {}, "hash": "3a410e71d3de063204b8c0fc17a7af5a843c205a0cefabb874404eb852db8823", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Okay, three reasons. The two reasons are because it's open source and it's really good. And also I think uh Elias and Amu and everyone at Canopy Labs is really awesome. So uh that's the that's the third reason we're we're talking about their model. But it's a Llama 3.2 3B backbone. So like if you look at this is the little like config from hugging face copy and pasted onto the screen. It's a llama for causal LM architecture. And so because of that we can do like all of our normal llama stuff to this model and make it faster. Um they did a couple things. ", "mimetype": "text/plain", "start_char_idx": 2337, "end_char_idx": 2897, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.027

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: {"id_": "9aad7320-72b3-4678-92e2-62abcf545f26", "embedding": null, "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40899ea9-c030-4b43-9b99-b9fdb7c8effe", "node_type": "4", "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "hash": "d75ce9148990d33110afb7e21f57d22aefae30c8e07a82059c484c2b8c34141d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6aa70f0c-ff5d-4a2c-8c33-7ffdef9a2183", "node_type": "1", "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "hash": "89a3fbbbdf177632c191d002350482ba59b2e2c2cb4380b0f7a61de8480fd98b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ad70bac5-5d03-4945-a244-90d038a9dae6", "node_type": "1", "metadata": {}, "hash": "5f0420ee2c4382f4caf2f007c7d59f752649464562cfa8a46fad8930c879751a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "So how do we do it? Bunch of ways. ", "mimetype": "text/plain", "start_char_idx": 4983, "end_char_idx": 5018, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.019

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: {"id_": "829f2359-ba77-4c21-942a-68c1864c7372", "embedding": null, "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40899ea9-c030-4b43-9b99-b9fdb7c8effe", "node_type": "4", "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "hash": "d75ce9148990d33110afb7e21f57d22aefae30c8e07a82059c484c2b8c34141d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c0db8c0e-7c89-4efd-82e6-4e299957b5a4", "node_type": "1", "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "hash": "73a22f347e849957232d8bb86f0127aa17baf3bb990362fcb1465c42a0233bcb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3bf554e3-2532-4eb3-a454-6860f8476ff6", "node_type": "1", "metadata": {}, "hash": "85d21c2a31364760439010b3b8292568bf70f53448d1945b28aef520c2136868", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Yes, it's fast. Um so with the you know with the tot implementation on the MIGs and on the H100s we can actually get all the way down to 150 millisecond time to first bite um in like real world uh testing that we've done. Now that we'll we'll talk in a minute like that doesn't mean your whole pipeline is that fast. That's just like one part of the pipeline. Uh but it's it's important because you know you definitely don't want to be spending a lot of time waiting around for that first token. So to kind of transition into that that discussion of like what can go wrong here like you have this graph and you have this you know nice uh config that I had up here and you're like all right cool I'm going to take this I'm going to put it in production and I'm going to see the results that uh that he put up on screen and it's going to work great and the answer is no it's not it's a little bit harder than that. Um, so the thing is like non-runtime factors when we get especially with these small models and with these multimodal systems can actually be like way more important than your runtime. Um, and that's your lat your infrastructure and your client code because you know I I showed here all right maybe maybe I got it you know I I cut the runtime in in half um from from the base implementation I I saved a couple hundred milliseconds very easy to add those couple hundred milliseconds back and well beyond that by you know sending my query to New York instead of California or by having to establish a session every time I uh you know run my client code. Um, so a few like pitfalls to avoid. Um, number one, like if you go in, you know, our our model library or something and we're just trying to get you started very quickly um with this with this kind of inference sample. Um, it's basically going to be, hey, use requests, make a stream, stream it to your local computer and start, you know, playing it on ffmpeg or something. The issue is that here like the requests are going to be sent sequentially and you need to create a new session every time. um that takes time. ", "mimetype": "text/plain", "start_char_idx": 10092, "end_char_idx": 12176, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: -0.001

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: Thank you so much and I'll let you go 8 seconds early. [Music]
Relevance score: 0.094

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: So if you're using this in production, you want a code sample. By the way, this is all up on our GitHub. Um you'll want a code sample that looks a lot more like a benchmarking script where you're using a um multipprocess pool. um you're sharing the you're sharing the session between all of these different requests and you're actually sending traffic with the concurrency that allows you to, you know, saturate this benchmark with, you know, the the multiple concurrent requests. Finally, both of these code samples, they do sit on top of HTTP and HTTP streaming. Um, in many cases, if you're implementing voice pipelines, you're going to use something like LiveKit or Pipecat or something. And you're also potentially going to be using a different protocol. You're going to be using something like websockets or gRPC, which we do have support for. Um, and finally, I wanted to leave you on the thought that these uh, you know, these models are only one part of a voice agent pipeline. So like we can spend a lot more than 15 minutes actually talking about like the very detailed uh implementation mechanics of making your voice model faster of you know we we haven't even touched on stuff like fine-tuning the model um you know custom voices zeroot voice cloning um being able to you know remove static and popping at the end of messages there's there's there's a lot of work to do just on the voice part but it really only is onethird of the problem when I think about voice agents. I think about three parts, listening, thinking, talking. Um, and the most important thing here is again, while you can have great run times, the infrastructure to connect these three together is really what's going to determine your latency. Being able to go from one model to have the next one running in the same data center with, you know, minimal like minimal network overhead in between the two. uh even things as simple as not having to go off and do a hair pin um at the DNS level and come back. If that saves you 10 milliseconds on every step and your voice pipeline has this and you know a chunking algorithm, it's got an interruption model and so you end up having four or five steps. Well, there just hair pinning alone is costing you 40 or 50 milliseconds and that can be 10% of your SLA for for a voice model. So yeah, uh that's that's my that's my main point here is that as much fun as it is to talk about the runtime stuff and as much work as we do there, the the infrastructure and the client implementation is equally important if not more so. Anyway, thank uh so yeah, that's the that's the review. Um thank you all for coming through. Uh I have uh we're doing an event next week at uh Fogo to Chiao which is going to be pretty fun. I'm going to be talking in more detail about building some uh systems with open source models and there's also going to be a lot of stake. So definitely come on through uh if you're interested and um I'm on I'm on Twitter, I'm on LinkedIn, so it's base 10. Um hit me up if you have any questions about this or anything else model performance. 
Relevance score: 0.036

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: {"id_": "6aa70f0c-ff5d-4a2c-8c33-7ffdef9a2183", "embedding": null, "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40899ea9-c030-4b43-9b99-b9fdb7c8effe", "node_type": "4", "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "hash": "d75ce9148990d33110afb7e21f57d22aefae30c8e07a82059c484c2b8c34141d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d4dad7ea-1f1e-4d41-935f-4775d6c7c359", "node_type": "1", "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "hash": "3734d78097d0a607a4758d81f74b774c777082346cace92f36fc58d6d021e798", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9aad7320-72b3-4678-92e2-62abcf545f26", "node_type": "1", "metadata": {}, "hash": "89f76e71d613ea13f0d10247d10928ec24f21d29a6f3005e154ff458a7e30a06", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "I mean they did a bunch of things to make it work but a couple things that are relevant here. There is a larger vocab size because you need all the speech specific tokens like laugh and stuff. Um and then they also extended the context length for thrope scaling. So we got to make sure everything we do supports that. So performance metrics like what do we want to actually do here? Uh we we think about LLM metrics um a little bit here. Uh we we just look at them a little bit differently. So in LLMs you talk about time to first token. Now we're talking about time to first bite or sometimes even time to first sentence. Uh we we need a little bit more of a useful output um from the model before we really start feeling good about our response time. We do think about tokens per second although we're going to think about it differently which I'll explain later. And we mostly think about throughput which is you know how many requests are we able to serve at a given time. So on that, you know, goals perspective, if you ask me like, \"Hey, Philip, how do you want to optimize llama in general?\" I'll like I'll say, \"Well, we want a lot of TPS. We want hundred. We want 500 TPS. We want a thousand tokens per second. We want as many tokens per second as we can get.\" With voice models, you actually don't necessarily need that. In many cases, you only want as many tokens per second as you need for a real-time stream. for Orpheus that's like 83 tokens per second which for like a three billion parameter LLM is nothing. Um but what we actually want to do instead is we want to once we hit that mark start optimizing for time to first bite so that our latency is really good and start optimizing for concurrency so that we can get more connections and spend less on GPUs. So our goal in general if all of these very nice and definitely not AI generated people are all the different like voices that our that our model is capable of creating these are all the voice agents that we're running. How can we make all of these people fit on one or even less than one GPU? That's the goal. ", "mimetype": "text/plain", "start_char_idx": 2897, "end_char_idx": 4983, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.078

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: Okay, three reasons. The two reasons are because it's open source and it's really good. And also I think uh Elias and Amu and everyone at Canopy Labs is really awesome. So uh that's the that's the third reason we're we're talking about their model. But it's a Llama 3.2 3B backbone. So like if you look at this is the little like config from hugging face copy and pasted onto the screen. It's a llama for causal LM architecture. And so because of that we can do like all of our normal llama stuff to this model and make it faster. Um they did a couple things. 
Relevance score: 0.143
        ---

        ## 4. Input Data
        User Query:
        how to optimize inference for voice models in production
        ---

        ## 5. Output Format
        Generate a natural language response to the user's query. Ensure your response is directly supported by the "Retrieved Context" section.


----------------------------------------------------------------
-------------------- New Query at 2025-09-01T03:09:41.134628 -------------

        # Chatbot Agent - Role and Guidelines

        ## 1. System Prompt
        You are the Chatbot Agent, a conversational AI designed to answer user questions about AI agents, RAG, context engineering, and multi-agent systems. 
        Your responses must be friendly, concise, and professional. 
        Your primary function is to synthesize information from the provided context to answer the user's query. 
        It is critical that you *only* use the information in the provided context. 
        If the context does not contain a confident answer to the user's question, 
        you *must* respond with "I am sorry, but the provided information does not contain a confident answer to your question." 
        You must never make up or invent information.

        ## 2. Task
        Answer the user's query based solely on the provided context. 
        Maintain a polite and helpful tone. If the user's query is a follow-up, 
        use the chat history to inform your response while still grounding it in the provided context.

        ## 3. Context
        The following information was retrieved from our knowledge base of video transcripts. Use this information to formulate your answer.
        ---
        Retrieved Context:
        Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: {"id_": "28ff36bd-0d1f-472e-accc-106a7c9628f4", "embedding": null, "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40899ea9-c030-4b43-9b99-b9fdb7c8effe", "node_type": "4", "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "hash": "d75ce9148990d33110afb7e21f57d22aefae30c8e07a82059c484c2b8c34141d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3bf554e3-2532-4eb3-a454-6860f8476ff6", "node_type": "1", "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "hash": "dfcc239616ce1a1f0b944ced717f40901b6ec6b01376753fe88592a22eb7135a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Thank you so much and I'll let you go 8 seconds early. [Music]", "mimetype": "text/plain", "start_char_idx": 15257, "end_char_idx": 15319, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.094

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: {"id_": "3bf554e3-2532-4eb3-a454-6860f8476ff6", "embedding": null, "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40899ea9-c030-4b43-9b99-b9fdb7c8effe", "node_type": "4", "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "hash": "d75ce9148990d33110afb7e21f57d22aefae30c8e07a82059c484c2b8c34141d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "829f2359-ba77-4c21-942a-68c1864c7372", "node_type": "1", "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "hash": "610e259f8b2d9f30ccf2828d073cc1d42cc6ff86ac7ce5c7fad66f8836ba1437", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "28ff36bd-0d1f-472e-accc-106a7c9628f4", "node_type": "1", "metadata": {}, "hash": "ea9e974ba76398f55d64c69495a2869ce5daac951515c502958564173a835e6f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "So if you're using this in production, you want a code sample. By the way, this is all up on our GitHub. Um you'll want a code sample that looks a lot more like a benchmarking script where you're using a um multipprocess pool. um you're sharing the you're sharing the session between all of these different requests and you're actually sending traffic with the concurrency that allows you to, you know, saturate this benchmark with, you know, the the multiple concurrent requests. Finally, both of these code samples, they do sit on top of HTTP and HTTP streaming. Um, in many cases, if you're implementing voice pipelines, you're going to use something like LiveKit or Pipecat or something. And you're also potentially going to be using a different protocol. You're going to be using something like websockets or gRPC, which we do have support for. Um, and finally, I wanted to leave you on the thought that these uh, you know, these models are only one part of a voice agent pipeline. So like we can spend a lot more than 15 minutes actually talking about like the very detailed uh implementation mechanics of making your voice model faster of you know we we haven't even touched on stuff like fine-tuning the model um you know custom voices zeroot voice cloning um being able to you know remove static and popping at the end of messages there's there's there's a lot of work to do just on the voice part but it really only is onethird of the problem when I think about voice agents. I think about three parts, listening, thinking, talking. Um, and the most important thing here is again, while you can have great run times, the infrastructure to connect these three together is really what's going to determine your latency. Being able to go from one model to have the next one running in the same data center with, you know, minimal like minimal network overhead in between the two. uh even things as simple as not having to go off and do a hair pin um at the DNS level and come back. If that saves you 10 milliseconds on every step and your voice pipeline has this and you know a chunking algorithm, it's got an interruption model and so you end up having four or five steps. Well, there just hair pinning alone is costing you 40 or 50 milliseconds and that can be 10% of your SLA for for a voice model. So yeah, uh that's that's my that's my main point here is that as much fun as it is to talk about the runtime stuff and as much work as we do there, the the infrastructure and the client implementation is equally important if not more so. Anyway, thank uh so yeah, that's the that's the review. Um thank you all for coming through. Uh I have uh we're doing an event next week at uh Fogo to Chiao which is going to be pretty fun. I'm going to be talking in more detail about building some uh systems with open source models and there's also going to be a lot of stake. So definitely come on through uh if you're interested and um I'm on I'm on Twitter, I'm on LinkedIn, so it's base 10. Um hit me up if you have any questions about this or anything else model performance. ", "mimetype": "text/plain", "start_char_idx": 12176, "end_char_idx": 15257, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.036

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: {"id_": "d4dad7ea-1f1e-4d41-935f-4775d6c7c359", "embedding": null, "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40899ea9-c030-4b43-9b99-b9fdb7c8effe", "node_type": "4", "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "hash": "d75ce9148990d33110afb7e21f57d22aefae30c8e07a82059c484c2b8c34141d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0659fff4-9937-405a-8175-c49579e80c91", "node_type": "1", "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "hash": "e10dabc994fd4733e881e0d906be4a06d8dc3ea698b14bc92c3fecf375703e57", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6aa70f0c-ff5d-4a2c-8c33-7ffdef9a2183", "node_type": "1", "metadata": {}, "hash": "3a410e71d3de063204b8c0fc17a7af5a843c205a0cefabb874404eb852db8823", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Okay, three reasons. The two reasons are because it's open source and it's really good. And also I think uh Elias and Amu and everyone at Canopy Labs is really awesome. So uh that's the that's the third reason we're we're talking about their model. But it's a Llama 3.2 3B backbone. So like if you look at this is the little like config from hugging face copy and pasted onto the screen. It's a llama for causal LM architecture. And so because of that we can do like all of our normal llama stuff to this model and make it faster. Um they did a couple things. ", "mimetype": "text/plain", "start_char_idx": 2337, "end_char_idx": 2897, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.027

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: {"id_": "9aad7320-72b3-4678-92e2-62abcf545f26", "embedding": null, "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40899ea9-c030-4b43-9b99-b9fdb7c8effe", "node_type": "4", "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "hash": "d75ce9148990d33110afb7e21f57d22aefae30c8e07a82059c484c2b8c34141d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6aa70f0c-ff5d-4a2c-8c33-7ffdef9a2183", "node_type": "1", "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "hash": "89a3fbbbdf177632c191d002350482ba59b2e2c2cb4380b0f7a61de8480fd98b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ad70bac5-5d03-4945-a244-90d038a9dae6", "node_type": "1", "metadata": {}, "hash": "5f0420ee2c4382f4caf2f007c7d59f752649464562cfa8a46fad8930c879751a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "So how do we do it? Bunch of ways. ", "mimetype": "text/plain", "start_char_idx": 4983, "end_char_idx": 5018, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.019

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: {"id_": "829f2359-ba77-4c21-942a-68c1864c7372", "embedding": null, "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40899ea9-c030-4b43-9b99-b9fdb7c8effe", "node_type": "4", "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "hash": "d75ce9148990d33110afb7e21f57d22aefae30c8e07a82059c484c2b8c34141d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c0db8c0e-7c89-4efd-82e6-4e299957b5a4", "node_type": "1", "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "hash": "73a22f347e849957232d8bb86f0127aa17baf3bb990362fcb1465c42a0233bcb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3bf554e3-2532-4eb3-a454-6860f8476ff6", "node_type": "1", "metadata": {}, "hash": "85d21c2a31364760439010b3b8292568bf70f53448d1945b28aef520c2136868", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Yes, it's fast. Um so with the you know with the tot implementation on the MIGs and on the H100s we can actually get all the way down to 150 millisecond time to first bite um in like real world uh testing that we've done. Now that we'll we'll talk in a minute like that doesn't mean your whole pipeline is that fast. That's just like one part of the pipeline. Uh but it's it's important because you know you definitely don't want to be spending a lot of time waiting around for that first token. So to kind of transition into that that discussion of like what can go wrong here like you have this graph and you have this you know nice uh config that I had up here and you're like all right cool I'm going to take this I'm going to put it in production and I'm going to see the results that uh that he put up on screen and it's going to work great and the answer is no it's not it's a little bit harder than that. Um, so the thing is like non-runtime factors when we get especially with these small models and with these multimodal systems can actually be like way more important than your runtime. Um, and that's your lat your infrastructure and your client code because you know I I showed here all right maybe maybe I got it you know I I cut the runtime in in half um from from the base implementation I I saved a couple hundred milliseconds very easy to add those couple hundred milliseconds back and well beyond that by you know sending my query to New York instead of California or by having to establish a session every time I uh you know run my client code. Um, so a few like pitfalls to avoid. Um, number one, like if you go in, you know, our our model library or something and we're just trying to get you started very quickly um with this with this kind of inference sample. Um, it's basically going to be, hey, use requests, make a stream, stream it to your local computer and start, you know, playing it on ffmpeg or something. The issue is that here like the requests are going to be sent sequentially and you need to create a new session every time. um that takes time. ", "mimetype": "text/plain", "start_char_idx": 10092, "end_char_idx": 12176, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: -0.001

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: Thank you so much and I'll let you go 8 seconds early. [Music]
Relevance score: 0.094

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: So if you're using this in production, you want a code sample. By the way, this is all up on our GitHub. Um you'll want a code sample that looks a lot more like a benchmarking script where you're using a um multipprocess pool. um you're sharing the you're sharing the session between all of these different requests and you're actually sending traffic with the concurrency that allows you to, you know, saturate this benchmark with, you know, the the multiple concurrent requests. Finally, both of these code samples, they do sit on top of HTTP and HTTP streaming. Um, in many cases, if you're implementing voice pipelines, you're going to use something like LiveKit or Pipecat or something. And you're also potentially going to be using a different protocol. You're going to be using something like websockets or gRPC, which we do have support for. Um, and finally, I wanted to leave you on the thought that these uh, you know, these models are only one part of a voice agent pipeline. So like we can spend a lot more than 15 minutes actually talking about like the very detailed uh implementation mechanics of making your voice model faster of you know we we haven't even touched on stuff like fine-tuning the model um you know custom voices zeroot voice cloning um being able to you know remove static and popping at the end of messages there's there's there's a lot of work to do just on the voice part but it really only is onethird of the problem when I think about voice agents. I think about three parts, listening, thinking, talking. Um, and the most important thing here is again, while you can have great run times, the infrastructure to connect these three together is really what's going to determine your latency. Being able to go from one model to have the next one running in the same data center with, you know, minimal like minimal network overhead in between the two. uh even things as simple as not having to go off and do a hair pin um at the DNS level and come back. If that saves you 10 milliseconds on every step and your voice pipeline has this and you know a chunking algorithm, it's got an interruption model and so you end up having four or five steps. Well, there just hair pinning alone is costing you 40 or 50 milliseconds and that can be 10% of your SLA for for a voice model. So yeah, uh that's that's my that's my main point here is that as much fun as it is to talk about the runtime stuff and as much work as we do there, the the infrastructure and the client implementation is equally important if not more so. Anyway, thank uh so yeah, that's the that's the review. Um thank you all for coming through. Uh I have uh we're doing an event next week at uh Fogo to Chiao which is going to be pretty fun. I'm going to be talking in more detail about building some uh systems with open source models and there's also going to be a lot of stake. So definitely come on through uh if you're interested and um I'm on I'm on Twitter, I'm on LinkedIn, so it's base 10. Um hit me up if you have any questions about this or anything else model performance. 
Relevance score: 0.036

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: {"id_": "6aa70f0c-ff5d-4a2c-8c33-7ffdef9a2183", "embedding": null, "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40899ea9-c030-4b43-9b99-b9fdb7c8effe", "node_type": "4", "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "hash": "d75ce9148990d33110afb7e21f57d22aefae30c8e07a82059c484c2b8c34141d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d4dad7ea-1f1e-4d41-935f-4775d6c7c359", "node_type": "1", "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "hash": "3734d78097d0a607a4758d81f74b774c777082346cace92f36fc58d6d021e798", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9aad7320-72b3-4678-92e2-62abcf545f26", "node_type": "1", "metadata": {}, "hash": "89f76e71d613ea13f0d10247d10928ec24f21d29a6f3005e154ff458a7e30a06", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "I mean they did a bunch of things to make it work but a couple things that are relevant here. There is a larger vocab size because you need all the speech specific tokens like laugh and stuff. Um and then they also extended the context length for thrope scaling. So we got to make sure everything we do supports that. So performance metrics like what do we want to actually do here? Uh we we think about LLM metrics um a little bit here. Uh we we just look at them a little bit differently. So in LLMs you talk about time to first token. Now we're talking about time to first bite or sometimes even time to first sentence. Uh we we need a little bit more of a useful output um from the model before we really start feeling good about our response time. We do think about tokens per second although we're going to think about it differently which I'll explain later. And we mostly think about throughput which is you know how many requests are we able to serve at a given time. So on that, you know, goals perspective, if you ask me like, \"Hey, Philip, how do you want to optimize llama in general?\" I'll like I'll say, \"Well, we want a lot of TPS. We want hundred. We want 500 TPS. We want a thousand tokens per second. We want as many tokens per second as we can get.\" With voice models, you actually don't necessarily need that. In many cases, you only want as many tokens per second as you need for a real-time stream. for Orpheus that's like 83 tokens per second which for like a three billion parameter LLM is nothing. Um but what we actually want to do instead is we want to once we hit that mark start optimizing for time to first bite so that our latency is really good and start optimizing for concurrency so that we can get more connections and spend less on GPUs. So our goal in general if all of these very nice and definitely not AI generated people are all the different like voices that our that our model is capable of creating these are all the voice agents that we're running. How can we make all of these people fit on one or even less than one GPU? That's the goal. ", "mimetype": "text/plain", "start_char_idx": 2897, "end_char_idx": 4983, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.078

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: Okay, three reasons. The two reasons are because it's open source and it's really good. And also I think uh Elias and Amu and everyone at Canopy Labs is really awesome. So uh that's the that's the third reason we're we're talking about their model. But it's a Llama 3.2 3B backbone. So like if you look at this is the little like config from hugging face copy and pasted onto the screen. It's a llama for causal LM architecture. And so because of that we can do like all of our normal llama stuff to this model and make it faster. Um they did a couple things. 
Relevance score: 0.143
        ---

        ## 4. Input Data
        User Query:
        how to optimize inference for voice models in production
        ---

        ## 5. Output Format
        Generate a natural language response to the user's query. Ensure your response is directly supported by the "Retrieved Context" section.


----------------------------------------------------------------
-------------------- New Query at 2025-09-01T03:11:54.606908 -------------

        # Chatbot Agent - Role and Guidelines

        ## 1. System Prompt
        You are the Chatbot Agent, a conversational AI designed to answer user questions about AI agents, RAG, context engineering, and multi-agent systems. 
        Your responses must be friendly, concise, and professional. 
        Your primary function is to synthesize information from the provided context to answer the user's query. 
        It is critical that you *only* use the information in the provided context. 
        If the context does not contain a confident answer to the user's question, 
        you *must* respond with "I am sorry, but the provided information does not contain a confident answer to your question." 
        You must never make up or invent information.

        ## 2. Task
        Answer the user's query based solely on the provided context. 
        Maintain a polite and helpful tone. If the user's query is a follow-up, 
        use the chat history to inform your response while still grounding it in the provided context.

        ## 3. Context
        The following information was retrieved from our knowledge base of video transcripts. Use this information to formulate your answer.
        ---
        Retrieved Context:
        Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: {"id_": "28ff36bd-0d1f-472e-accc-106a7c9628f4", "embedding": null, "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40899ea9-c030-4b43-9b99-b9fdb7c8effe", "node_type": "4", "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "hash": "d75ce9148990d33110afb7e21f57d22aefae30c8e07a82059c484c2b8c34141d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3bf554e3-2532-4eb3-a454-6860f8476ff6", "node_type": "1", "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "hash": "dfcc239616ce1a1f0b944ced717f40901b6ec6b01376753fe88592a22eb7135a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Thank you so much and I'll let you go 8 seconds early. [Music]", "mimetype": "text/plain", "start_char_idx": 15257, "end_char_idx": 15319, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.094

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: {"id_": "3bf554e3-2532-4eb3-a454-6860f8476ff6", "embedding": null, "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40899ea9-c030-4b43-9b99-b9fdb7c8effe", "node_type": "4", "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "hash": "d75ce9148990d33110afb7e21f57d22aefae30c8e07a82059c484c2b8c34141d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "829f2359-ba77-4c21-942a-68c1864c7372", "node_type": "1", "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "hash": "610e259f8b2d9f30ccf2828d073cc1d42cc6ff86ac7ce5c7fad66f8836ba1437", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "28ff36bd-0d1f-472e-accc-106a7c9628f4", "node_type": "1", "metadata": {}, "hash": "ea9e974ba76398f55d64c69495a2869ce5daac951515c502958564173a835e6f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "So if you're using this in production, you want a code sample. By the way, this is all up on our GitHub. Um you'll want a code sample that looks a lot more like a benchmarking script where you're using a um multipprocess pool. um you're sharing the you're sharing the session between all of these different requests and you're actually sending traffic with the concurrency that allows you to, you know, saturate this benchmark with, you know, the the multiple concurrent requests. Finally, both of these code samples, they do sit on top of HTTP and HTTP streaming. Um, in many cases, if you're implementing voice pipelines, you're going to use something like LiveKit or Pipecat or something. And you're also potentially going to be using a different protocol. You're going to be using something like websockets or gRPC, which we do have support for. Um, and finally, I wanted to leave you on the thought that these uh, you know, these models are only one part of a voice agent pipeline. So like we can spend a lot more than 15 minutes actually talking about like the very detailed uh implementation mechanics of making your voice model faster of you know we we haven't even touched on stuff like fine-tuning the model um you know custom voices zeroot voice cloning um being able to you know remove static and popping at the end of messages there's there's there's a lot of work to do just on the voice part but it really only is onethird of the problem when I think about voice agents. I think about three parts, listening, thinking, talking. Um, and the most important thing here is again, while you can have great run times, the infrastructure to connect these three together is really what's going to determine your latency. Being able to go from one model to have the next one running in the same data center with, you know, minimal like minimal network overhead in between the two. uh even things as simple as not having to go off and do a hair pin um at the DNS level and come back. If that saves you 10 milliseconds on every step and your voice pipeline has this and you know a chunking algorithm, it's got an interruption model and so you end up having four or five steps. Well, there just hair pinning alone is costing you 40 or 50 milliseconds and that can be 10% of your SLA for for a voice model. So yeah, uh that's that's my that's my main point here is that as much fun as it is to talk about the runtime stuff and as much work as we do there, the the infrastructure and the client implementation is equally important if not more so. Anyway, thank uh so yeah, that's the that's the review. Um thank you all for coming through. Uh I have uh we're doing an event next week at uh Fogo to Chiao which is going to be pretty fun. I'm going to be talking in more detail about building some uh systems with open source models and there's also going to be a lot of stake. So definitely come on through uh if you're interested and um I'm on I'm on Twitter, I'm on LinkedIn, so it's base 10. Um hit me up if you have any questions about this or anything else model performance. ", "mimetype": "text/plain", "start_char_idx": 12176, "end_char_idx": 15257, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.036

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: {"id_": "d4dad7ea-1f1e-4d41-935f-4775d6c7c359", "embedding": null, "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40899ea9-c030-4b43-9b99-b9fdb7c8effe", "node_type": "4", "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "hash": "d75ce9148990d33110afb7e21f57d22aefae30c8e07a82059c484c2b8c34141d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0659fff4-9937-405a-8175-c49579e80c91", "node_type": "1", "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "hash": "e10dabc994fd4733e881e0d906be4a06d8dc3ea698b14bc92c3fecf375703e57", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6aa70f0c-ff5d-4a2c-8c33-7ffdef9a2183", "node_type": "1", "metadata": {}, "hash": "3a410e71d3de063204b8c0fc17a7af5a843c205a0cefabb874404eb852db8823", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Okay, three reasons. The two reasons are because it's open source and it's really good. And also I think uh Elias and Amu and everyone at Canopy Labs is really awesome. So uh that's the that's the third reason we're we're talking about their model. But it's a Llama 3.2 3B backbone. So like if you look at this is the little like config from hugging face copy and pasted onto the screen. It's a llama for causal LM architecture. And so because of that we can do like all of our normal llama stuff to this model and make it faster. Um they did a couple things. ", "mimetype": "text/plain", "start_char_idx": 2337, "end_char_idx": 2897, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.027

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: {"id_": "9aad7320-72b3-4678-92e2-62abcf545f26", "embedding": null, "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40899ea9-c030-4b43-9b99-b9fdb7c8effe", "node_type": "4", "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "hash": "d75ce9148990d33110afb7e21f57d22aefae30c8e07a82059c484c2b8c34141d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6aa70f0c-ff5d-4a2c-8c33-7ffdef9a2183", "node_type": "1", "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "hash": "89a3fbbbdf177632c191d002350482ba59b2e2c2cb4380b0f7a61de8480fd98b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ad70bac5-5d03-4945-a244-90d038a9dae6", "node_type": "1", "metadata": {}, "hash": "5f0420ee2c4382f4caf2f007c7d59f752649464562cfa8a46fad8930c879751a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "So how do we do it? Bunch of ways. ", "mimetype": "text/plain", "start_char_idx": 4983, "end_char_idx": 5018, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.019

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: {"id_": "829f2359-ba77-4c21-942a-68c1864c7372", "embedding": null, "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40899ea9-c030-4b43-9b99-b9fdb7c8effe", "node_type": "4", "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "hash": "d75ce9148990d33110afb7e21f57d22aefae30c8e07a82059c484c2b8c34141d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c0db8c0e-7c89-4efd-82e6-4e299957b5a4", "node_type": "1", "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "hash": "73a22f347e849957232d8bb86f0127aa17baf3bb990362fcb1465c42a0233bcb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3bf554e3-2532-4eb3-a454-6860f8476ff6", "node_type": "1", "metadata": {}, "hash": "85d21c2a31364760439010b3b8292568bf70f53448d1945b28aef520c2136868", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Yes, it's fast. Um so with the you know with the tot implementation on the MIGs and on the H100s we can actually get all the way down to 150 millisecond time to first bite um in like real world uh testing that we've done. Now that we'll we'll talk in a minute like that doesn't mean your whole pipeline is that fast. That's just like one part of the pipeline. Uh but it's it's important because you know you definitely don't want to be spending a lot of time waiting around for that first token. So to kind of transition into that that discussion of like what can go wrong here like you have this graph and you have this you know nice uh config that I had up here and you're like all right cool I'm going to take this I'm going to put it in production and I'm going to see the results that uh that he put up on screen and it's going to work great and the answer is no it's not it's a little bit harder than that. Um, so the thing is like non-runtime factors when we get especially with these small models and with these multimodal systems can actually be like way more important than your runtime. Um, and that's your lat your infrastructure and your client code because you know I I showed here all right maybe maybe I got it you know I I cut the runtime in in half um from from the base implementation I I saved a couple hundred milliseconds very easy to add those couple hundred milliseconds back and well beyond that by you know sending my query to New York instead of California or by having to establish a session every time I uh you know run my client code. Um, so a few like pitfalls to avoid. Um, number one, like if you go in, you know, our our model library or something and we're just trying to get you started very quickly um with this with this kind of inference sample. Um, it's basically going to be, hey, use requests, make a stream, stream it to your local computer and start, you know, playing it on ffmpeg or something. The issue is that here like the requests are going to be sent sequentially and you need to create a new session every time. um that takes time. ", "mimetype": "text/plain", "start_char_idx": 10092, "end_char_idx": 12176, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: -0.001

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: Thank you so much and I'll let you go 8 seconds early. [Music]
Relevance score: 0.094

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: So if you're using this in production, you want a code sample. By the way, this is all up on our GitHub. Um you'll want a code sample that looks a lot more like a benchmarking script where you're using a um multipprocess pool. um you're sharing the you're sharing the session between all of these different requests and you're actually sending traffic with the concurrency that allows you to, you know, saturate this benchmark with, you know, the the multiple concurrent requests. Finally, both of these code samples, they do sit on top of HTTP and HTTP streaming. Um, in many cases, if you're implementing voice pipelines, you're going to use something like LiveKit or Pipecat or something. And you're also potentially going to be using a different protocol. You're going to be using something like websockets or gRPC, which we do have support for. Um, and finally, I wanted to leave you on the thought that these uh, you know, these models are only one part of a voice agent pipeline. So like we can spend a lot more than 15 minutes actually talking about like the very detailed uh implementation mechanics of making your voice model faster of you know we we haven't even touched on stuff like fine-tuning the model um you know custom voices zeroot voice cloning um being able to you know remove static and popping at the end of messages there's there's there's a lot of work to do just on the voice part but it really only is onethird of the problem when I think about voice agents. I think about three parts, listening, thinking, talking. Um, and the most important thing here is again, while you can have great run times, the infrastructure to connect these three together is really what's going to determine your latency. Being able to go from one model to have the next one running in the same data center with, you know, minimal like minimal network overhead in between the two. uh even things as simple as not having to go off and do a hair pin um at the DNS level and come back. If that saves you 10 milliseconds on every step and your voice pipeline has this and you know a chunking algorithm, it's got an interruption model and so you end up having four or five steps. Well, there just hair pinning alone is costing you 40 or 50 milliseconds and that can be 10% of your SLA for for a voice model. So yeah, uh that's that's my that's my main point here is that as much fun as it is to talk about the runtime stuff and as much work as we do there, the the infrastructure and the client implementation is equally important if not more so. Anyway, thank uh so yeah, that's the that's the review. Um thank you all for coming through. Uh I have uh we're doing an event next week at uh Fogo to Chiao which is going to be pretty fun. I'm going to be talking in more detail about building some uh systems with open source models and there's also going to be a lot of stake. So definitely come on through uh if you're interested and um I'm on I'm on Twitter, I'm on LinkedIn, so it's base 10. Um hit me up if you have any questions about this or anything else model performance. 
Relevance score: 0.036

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: {"id_": "6aa70f0c-ff5d-4a2c-8c33-7ffdef9a2183", "embedding": null, "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40899ea9-c030-4b43-9b99-b9fdb7c8effe", "node_type": "4", "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "hash": "d75ce9148990d33110afb7e21f57d22aefae30c8e07a82059c484c2b8c34141d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d4dad7ea-1f1e-4d41-935f-4775d6c7c359", "node_type": "1", "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "hash": "3734d78097d0a607a4758d81f74b774c777082346cace92f36fc58d6d021e798", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9aad7320-72b3-4678-92e2-62abcf545f26", "node_type": "1", "metadata": {}, "hash": "89f76e71d613ea13f0d10247d10928ec24f21d29a6f3005e154ff458a7e30a06", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "I mean they did a bunch of things to make it work but a couple things that are relevant here. There is a larger vocab size because you need all the speech specific tokens like laugh and stuff. Um and then they also extended the context length for thrope scaling. So we got to make sure everything we do supports that. So performance metrics like what do we want to actually do here? Uh we we think about LLM metrics um a little bit here. Uh we we just look at them a little bit differently. So in LLMs you talk about time to first token. Now we're talking about time to first bite or sometimes even time to first sentence. Uh we we need a little bit more of a useful output um from the model before we really start feeling good about our response time. We do think about tokens per second although we're going to think about it differently which I'll explain later. And we mostly think about throughput which is you know how many requests are we able to serve at a given time. So on that, you know, goals perspective, if you ask me like, \"Hey, Philip, how do you want to optimize llama in general?\" I'll like I'll say, \"Well, we want a lot of TPS. We want hundred. We want 500 TPS. We want a thousand tokens per second. We want as many tokens per second as we can get.\" With voice models, you actually don't necessarily need that. In many cases, you only want as many tokens per second as you need for a real-time stream. for Orpheus that's like 83 tokens per second which for like a three billion parameter LLM is nothing. Um but what we actually want to do instead is we want to once we hit that mark start optimizing for time to first bite so that our latency is really good and start optimizing for concurrency so that we can get more connections and spend less on GPUs. So our goal in general if all of these very nice and definitely not AI generated people are all the different like voices that our that our model is capable of creating these are all the voice agents that we're running. How can we make all of these people fit on one or even less than one GPU? That's the goal. ", "mimetype": "text/plain", "start_char_idx": 2897, "end_char_idx": 4983, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.078

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: Okay, three reasons. The two reasons are because it's open source and it's really good. And also I think uh Elias and Amu and everyone at Canopy Labs is really awesome. So uh that's the that's the third reason we're we're talking about their model. But it's a Llama 3.2 3B backbone. So like if you look at this is the little like config from hugging face copy and pasted onto the screen. It's a llama for causal LM architecture. And so because of that we can do like all of our normal llama stuff to this model and make it faster. Um they did a couple things. 
Relevance score: 0.143
        ---

        ## 4. Input Data
        User Query:
        how to optimize inference for voice models in production
        ---

        ## 5. Output Format
        Generate a natural language response to the user's query. Ensure your response is directly supported by the "Retrieved Context" section.


----------------------------------------------------------------
-------------------- New Query at 2025-09-01T03:14:44.390699 -------------

        # Chatbot Agent - Role and Guidelines

        ## 1. System Prompt
        You are the Chatbot Agent, a conversational AI designed to answer user questions about AI agents, RAG, context engineering, and multi-agent systems. 
        Your responses must be friendly, concise, and professional. 
        Your primary function is to synthesize information from the provided context to answer the user's query. 
        It is critical that you *only* use the information in the provided context. 
        If the context does not contain a confident answer to the user's question, 
        you *must* respond with "I am sorry, but the provided information does not contain a confident answer to your question." 
        You must never make up or invent information.

        ## 2. Task
        Answer the user's query based solely on the provided context. 
        Maintain a polite and helpful tone. If the user's query is a follow-up, 
        use the chat history to inform your response while still grounding it in the provided context.

        ## 3. Context
        The following information was retrieved from our knowledge base of video transcripts. Use this information to formulate your answer.
        ---
        Retrieved Context:
        Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: {"id_": "3bf554e3-2532-4eb3-a454-6860f8476ff6", "embedding": null, "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40899ea9-c030-4b43-9b99-b9fdb7c8effe", "node_type": "4", "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "hash": "d75ce9148990d33110afb7e21f57d22aefae30c8e07a82059c484c2b8c34141d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "829f2359-ba77-4c21-942a-68c1864c7372", "node_type": "1", "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "hash": "610e259f8b2d9f30ccf2828d073cc1d42cc6ff86ac7ce5c7fad66f8836ba1437", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "28ff36bd-0d1f-472e-accc-106a7c9628f4", "node_type": "1", "metadata": {}, "hash": "ea9e974ba76398f55d64c69495a2869ce5daac951515c502958564173a835e6f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "So if you're using this in production, you want a code sample. By the way, this is all up on our GitHub. Um you'll want a code sample that looks a lot more like a benchmarking script where you're using a um multipprocess pool. um you're sharing the you're sharing the session between all of these different requests and you're actually sending traffic with the concurrency that allows you to, you know, saturate this benchmark with, you know, the the multiple concurrent requests. Finally, both of these code samples, they do sit on top of HTTP and HTTP streaming. Um, in many cases, if you're implementing voice pipelines, you're going to use something like LiveKit or Pipecat or something. And you're also potentially going to be using a different protocol. You're going to be using something like websockets or gRPC, which we do have support for. Um, and finally, I wanted to leave you on the thought that these uh, you know, these models are only one part of a voice agent pipeline. So like we can spend a lot more than 15 minutes actually talking about like the very detailed uh implementation mechanics of making your voice model faster of you know we we haven't even touched on stuff like fine-tuning the model um you know custom voices zeroot voice cloning um being able to you know remove static and popping at the end of messages there's there's there's a lot of work to do just on the voice part but it really only is onethird of the problem when I think about voice agents. I think about three parts, listening, thinking, talking. Um, and the most important thing here is again, while you can have great run times, the infrastructure to connect these three together is really what's going to determine your latency. Being able to go from one model to have the next one running in the same data center with, you know, minimal like minimal network overhead in between the two. uh even things as simple as not having to go off and do a hair pin um at the DNS level and come back. If that saves you 10 milliseconds on every step and your voice pipeline has this and you know a chunking algorithm, it's got an interruption model and so you end up having four or five steps. Well, there just hair pinning alone is costing you 40 or 50 milliseconds and that can be 10% of your SLA for for a voice model. So yeah, uh that's that's my that's my main point here is that as much fun as it is to talk about the runtime stuff and as much work as we do there, the the infrastructure and the client implementation is equally important if not more so. Anyway, thank uh so yeah, that's the that's the review. Um thank you all for coming through. Uh I have uh we're doing an event next week at uh Fogo to Chiao which is going to be pretty fun. I'm going to be talking in more detail about building some uh systems with open source models and there's also going to be a lot of stake. So definitely come on through uh if you're interested and um I'm on I'm on Twitter, I'm on LinkedIn, so it's base 10. Um hit me up if you have any questions about this or anything else model performance. ", "mimetype": "text/plain", "start_char_idx": 12176, "end_char_idx": 15257, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.346

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: {"id_": "6aa70f0c-ff5d-4a2c-8c33-7ffdef9a2183", "embedding": null, "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40899ea9-c030-4b43-9b99-b9fdb7c8effe", "node_type": "4", "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "hash": "d75ce9148990d33110afb7e21f57d22aefae30c8e07a82059c484c2b8c34141d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d4dad7ea-1f1e-4d41-935f-4775d6c7c359", "node_type": "1", "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "hash": "3734d78097d0a607a4758d81f74b774c777082346cace92f36fc58d6d021e798", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9aad7320-72b3-4678-92e2-62abcf545f26", "node_type": "1", "metadata": {}, "hash": "89f76e71d613ea13f0d10247d10928ec24f21d29a6f3005e154ff458a7e30a06", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "I mean they did a bunch of things to make it work but a couple things that are relevant here. There is a larger vocab size because you need all the speech specific tokens like laugh and stuff. Um and then they also extended the context length for thrope scaling. So we got to make sure everything we do supports that. So performance metrics like what do we want to actually do here? Uh we we think about LLM metrics um a little bit here. Uh we we just look at them a little bit differently. So in LLMs you talk about time to first token. Now we're talking about time to first bite or sometimes even time to first sentence. Uh we we need a little bit more of a useful output um from the model before we really start feeling good about our response time. We do think about tokens per second although we're going to think about it differently which I'll explain later. And we mostly think about throughput which is you know how many requests are we able to serve at a given time. So on that, you know, goals perspective, if you ask me like, \"Hey, Philip, how do you want to optimize llama in general?\" I'll like I'll say, \"Well, we want a lot of TPS. We want hundred. We want 500 TPS. We want a thousand tokens per second. We want as many tokens per second as we can get.\" With voice models, you actually don't necessarily need that. In many cases, you only want as many tokens per second as you need for a real-time stream. for Orpheus that's like 83 tokens per second which for like a three billion parameter LLM is nothing. Um but what we actually want to do instead is we want to once we hit that mark start optimizing for time to first bite so that our latency is really good and start optimizing for concurrency so that we can get more connections and spend less on GPUs. So our goal in general if all of these very nice and definitely not AI generated people are all the different like voices that our that our model is capable of creating these are all the voice agents that we're running. How can we make all of these people fit on one or even less than one GPU? That's the goal. ", "mimetype": "text/plain", "start_char_idx": 2897, "end_char_idx": 4983, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.270

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: {"id_": "c0db8c0e-7c89-4efd-82e6-4e299957b5a4", "embedding": null, "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40899ea9-c030-4b43-9b99-b9fdb7c8effe", "node_type": "4", "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "hash": "d75ce9148990d33110afb7e21f57d22aefae30c8e07a82059c484c2b8c34141d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ad70bac5-5d03-4945-a244-90d038a9dae6", "node_type": "1", "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "hash": "bde958b709b9344db0674f2ef949cfc5eb00e329831d13c60dac1a01a62953d2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "829f2359-ba77-4c21-942a-68c1864c7372", "node_type": "1", "metadata": {}, "hash": "8978c328ea7231e7d821e69e6c73d4aec8767b92c9441da2e7b5163253bb881b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Uh let's see let's see if it's actually any faster. Um, so again, the number one thing is going to be simultaneous streams because you want to be able to be very costefficient and use fewer GPU resources to serve a, you know, large amount of traffic. And in this case, our base implementation, I don't necessarily want to like call anyone out because there's a lot of really good ways to run this model. Um, you can get really good performance with VLM. Um but this is just kind of like the offtheshelf um just take it run it completely standard uh implementation. So with variable traffic we're able to support 16 simultaneous streams and with constant traffic 24 simultaneous streams on um an H100 MIG. So this is actually half an H100 GPU. It's a skew that we do a lot because it's really good for these small models where you want the Hopper performance, the Hopper architecture uplift in Tensor RTLM, the FP8 support, but you don't want to pay for like an entire 80 gigabyte GPU for just a 3 billion parameter model. So, you know, we're seeing much better um much much better concurrency. So, if you kind of like price that out with like our list prices and stuff, um, you can get, you know, a few cents per hour of of conversation, um, which is going to be, you know, substantially better than if you're if you have the volume for it. It's going to be substantially better than paying for a sort of like pertoken type API. But, okay, sure, maybe it's cheap at scale, but is it fast? ", "mimetype": "text/plain", "start_char_idx": 8603, "end_char_idx": 10092, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.239

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: {"id_": "0659fff4-9937-405a-8175-c49579e80c91", "embedding": null, "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40899ea9-c030-4b43-9b99-b9fdb7c8effe", "node_type": "4", "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "hash": "d75ce9148990d33110afb7e21f57d22aefae30c8e07a82059c484c2b8c34141d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d4dad7ea-1f1e-4d41-935f-4775d6c7c359", "node_type": "1", "metadata": {}, "hash": "a10ee8c433d49c1f560dd8154c781b797d7fe7dec3ceb921339d3e8729ea5456", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[Music] Uh hello everyone. Thank you so much for being here for uh sticking around for this talk. Um I'm going to be talking about optimizing inference for voice models in production. I'm going to be talking mostly about the runtime component but also just a little bit on the infrastructure side. Um just a quick introduction. Um I'm Philip from B 10. Base 10 is a model inference platform. Uh we run production workloads for a wide variety of AI native startups and enterprises. Um I'm based in here in SF. Um I actually just moved here. It's really awesome. My favorite part about being in SF is much better sports teams than I had in Chicago. Um and uh one of my favorite voice models is Orpheus TTS. Um which we're going to be talking about a whole bunch today. Um quick agenda. So we're going to talk about TTS model architecture like what is a text to speech model actually when you look on the config in hugging face. Uh what sort of performance metrics are we looking at? What sort of optimization techniques can we do to make the model better? Um how do we measure whether or not we succeeded? And then finally, what can we do on the infrastructure and client code to not shoot ourselves in the foot after doing a ton of runtime work and then just adding all that latency back by not doing our client code correctly. So architecture, this is one of the things I've been learning this year which has been pretty great uh to to realize it's made life a lot simpler at the runtime level. Now this is wrong like the the the thing up here uh that that I'm going to say is that like everything is an LLM. uh that is that is wrong but it's useful. Um there's kind of like two types of models. There's autogressive transformers models that are LLM or very LLM adjacent. Um you see this in embeddings. You see this in transcription with stuff like whisper TTS um is another example. You also have the more like diffuser image type models. Um which is like a very different optimization problem. But something that's cool is because TTS models are so architecturally similar to LLMs or in many cases derived directly from LLMs, we can access the rich ecosystem of LLM tooling and use it to make TTS models better. So the TTS model that we're going to be using and as an example all day um is Orpheus TTS. We're using it for two reasons. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2337, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.238

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: {"id_": "28ff36bd-0d1f-472e-accc-106a7c9628f4", "embedding": null, "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40899ea9-c030-4b43-9b99-b9fdb7c8effe", "node_type": "4", "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "hash": "d75ce9148990d33110afb7e21f57d22aefae30c8e07a82059c484c2b8c34141d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3bf554e3-2532-4eb3-a454-6860f8476ff6", "node_type": "1", "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "hash": "dfcc239616ce1a1f0b944ced717f40901b6ec6b01376753fe88592a22eb7135a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Thank you so much and I'll let you go 8 seconds early. [Music]", "mimetype": "text/plain", "start_char_idx": 15257, "end_char_idx": 15319, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.164

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: So if you're using this in production, you want a code sample. By the way, this is all up on our GitHub. Um you'll want a code sample that looks a lot more like a benchmarking script where you're using a um multipprocess pool. um you're sharing the you're sharing the session between all of these different requests and you're actually sending traffic with the concurrency that allows you to, you know, saturate this benchmark with, you know, the the multiple concurrent requests. Finally, both of these code samples, they do sit on top of HTTP and HTTP streaming. Um, in many cases, if you're implementing voice pipelines, you're going to use something like LiveKit or Pipecat or something. And you're also potentially going to be using a different protocol. You're going to be using something like websockets or gRPC, which we do have support for. Um, and finally, I wanted to leave you on the thought that these uh, you know, these models are only one part of a voice agent pipeline. So like we can spend a lot more than 15 minutes actually talking about like the very detailed uh implementation mechanics of making your voice model faster of you know we we haven't even touched on stuff like fine-tuning the model um you know custom voices zeroot voice cloning um being able to you know remove static and popping at the end of messages there's there's there's a lot of work to do just on the voice part but it really only is onethird of the problem when I think about voice agents. I think about three parts, listening, thinking, talking. Um, and the most important thing here is again, while you can have great run times, the infrastructure to connect these three together is really what's going to determine your latency. Being able to go from one model to have the next one running in the same data center with, you know, minimal like minimal network overhead in between the two. uh even things as simple as not having to go off and do a hair pin um at the DNS level and come back. If that saves you 10 milliseconds on every step and your voice pipeline has this and you know a chunking algorithm, it's got an interruption model and so you end up having four or five steps. Well, there just hair pinning alone is costing you 40 or 50 milliseconds and that can be 10% of your SLA for for a voice model. So yeah, uh that's that's my that's my main point here is that as much fun as it is to talk about the runtime stuff and as much work as we do there, the the infrastructure and the client implementation is equally important if not more so. Anyway, thank uh so yeah, that's the that's the review. Um thank you all for coming through. Uh I have uh we're doing an event next week at uh Fogo to Chiao which is going to be pretty fun. I'm going to be talking in more detail about building some uh systems with open source models and there's also going to be a lot of stake. So definitely come on through uh if you're interested and um I'm on I'm on Twitter, I'm on LinkedIn, so it's base 10. Um hit me up if you have any questions about this or anything else model performance. 
Relevance score: 0.346

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: I mean they did a bunch of things to make it work but a couple things that are relevant here. There is a larger vocab size because you need all the speech specific tokens like laugh and stuff. Um and then they also extended the context length for thrope scaling. So we got to make sure everything we do supports that. So performance metrics like what do we want to actually do here? Uh we we think about LLM metrics um a little bit here. Uh we we just look at them a little bit differently. So in LLMs you talk about time to first token. Now we're talking about time to first bite or sometimes even time to first sentence. Uh we we need a little bit more of a useful output um from the model before we really start feeling good about our response time. We do think about tokens per second although we're going to think about it differently which I'll explain later. And we mostly think about throughput which is you know how many requests are we able to serve at a given time. So on that, you know, goals perspective, if you ask me like, "Hey, Philip, how do you want to optimize llama in general?" I'll like I'll say, "Well, we want a lot of TPS. We want hundred. We want 500 TPS. We want a thousand tokens per second. We want as many tokens per second as we can get." With voice models, you actually don't necessarily need that. In many cases, you only want as many tokens per second as you need for a real-time stream. for Orpheus that's like 83 tokens per second which for like a three billion parameter LLM is nothing. Um but what we actually want to do instead is we want to once we hit that mark start optimizing for time to first bite so that our latency is really good and start optimizing for concurrency so that we can get more connections and spend less on GPUs. So our goal in general if all of these very nice and definitely not AI generated people are all the different like voices that our that our model is capable of creating these are all the voice agents that we're running. How can we make all of these people fit on one or even less than one GPU? That's the goal. 
Relevance score: 0.270
        ---

        ## 4. Input Data
        User Query:
        how to optimize inference for voice models in production
        ---

        ## 5. Output Format
        Generate a natural language response to the user's query. Ensure your response is directly supported by the "Retrieved Context" section.


----------------------------------------------------------------
-------------------- New Query at 2025-09-01T03:19:35.804043 -------------

        # Chatbot Agent - Role and Guidelines

        ## 1. Role
        You are the Chatbot Agent, a conversational AI designed to answer user questions about AI agents, RAG, context engineering, and multi-agent systems. 
        Your responses must be friendly, concise, and professional. 
        Your primary function is to synthesize information from the provided context to answer the user's query. 
        It is critical that you *only* use the information in the provided context. 
        If the context does not contain a confident answer to the user's question, 
        you *must* respond with "I am sorry, but the provided information does not contain a confident answer to your question." 
        You must never make up or invent information.

        ## 2. Task
        Answer the user's query based solely on the provided context. 
        Maintain a polite and helpful tone. If the user's query is a follow-up, 
        use the chat history to inform your response while still grounding it in the provided context.

        ## 3. Context
        The following information was retrieved from our knowledge base of video transcripts. Use this information to formulate your answer.
        ---
        ### Retrieved Context:
        Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: {"id_": "3bf554e3-2532-4eb3-a454-6860f8476ff6", "embedding": null, "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40899ea9-c030-4b43-9b99-b9fdb7c8effe", "node_type": "4", "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "hash": "d75ce9148990d33110afb7e21f57d22aefae30c8e07a82059c484c2b8c34141d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "829f2359-ba77-4c21-942a-68c1864c7372", "node_type": "1", "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "hash": "610e259f8b2d9f30ccf2828d073cc1d42cc6ff86ac7ce5c7fad66f8836ba1437", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "28ff36bd-0d1f-472e-accc-106a7c9628f4", "node_type": "1", "metadata": {}, "hash": "ea9e974ba76398f55d64c69495a2869ce5daac951515c502958564173a835e6f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "So if you're using this in production, you want a code sample. By the way, this is all up on our GitHub. Um you'll want a code sample that looks a lot more like a benchmarking script where you're using a um multipprocess pool. um you're sharing the you're sharing the session between all of these different requests and you're actually sending traffic with the concurrency that allows you to, you know, saturate this benchmark with, you know, the the multiple concurrent requests. Finally, both of these code samples, they do sit on top of HTTP and HTTP streaming. Um, in many cases, if you're implementing voice pipelines, you're going to use something like LiveKit or Pipecat or something. And you're also potentially going to be using a different protocol. You're going to be using something like websockets or gRPC, which we do have support for. Um, and finally, I wanted to leave you on the thought that these uh, you know, these models are only one part of a voice agent pipeline. So like we can spend a lot more than 15 minutes actually talking about like the very detailed uh implementation mechanics of making your voice model faster of you know we we haven't even touched on stuff like fine-tuning the model um you know custom voices zeroot voice cloning um being able to you know remove static and popping at the end of messages there's there's there's a lot of work to do just on the voice part but it really only is onethird of the problem when I think about voice agents. I think about three parts, listening, thinking, talking. Um, and the most important thing here is again, while you can have great run times, the infrastructure to connect these three together is really what's going to determine your latency. Being able to go from one model to have the next one running in the same data center with, you know, minimal like minimal network overhead in between the two. uh even things as simple as not having to go off and do a hair pin um at the DNS level and come back. If that saves you 10 milliseconds on every step and your voice pipeline has this and you know a chunking algorithm, it's got an interruption model and so you end up having four or five steps. Well, there just hair pinning alone is costing you 40 or 50 milliseconds and that can be 10% of your SLA for for a voice model. So yeah, uh that's that's my that's my main point here is that as much fun as it is to talk about the runtime stuff and as much work as we do there, the the infrastructure and the client implementation is equally important if not more so. Anyway, thank uh so yeah, that's the that's the review. Um thank you all for coming through. Uh I have uh we're doing an event next week at uh Fogo to Chiao which is going to be pretty fun. I'm going to be talking in more detail about building some uh systems with open source models and there's also going to be a lot of stake. So definitely come on through uh if you're interested and um I'm on I'm on Twitter, I'm on LinkedIn, so it's base 10. Um hit me up if you have any questions about this or anything else model performance. ", "mimetype": "text/plain", "start_char_idx": 12176, "end_char_idx": 15257, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.313

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: {"id_": "6aa70f0c-ff5d-4a2c-8c33-7ffdef9a2183", "embedding": null, "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40899ea9-c030-4b43-9b99-b9fdb7c8effe", "node_type": "4", "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "hash": "d75ce9148990d33110afb7e21f57d22aefae30c8e07a82059c484c2b8c34141d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d4dad7ea-1f1e-4d41-935f-4775d6c7c359", "node_type": "1", "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "hash": "3734d78097d0a607a4758d81f74b774c777082346cace92f36fc58d6d021e798", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9aad7320-72b3-4678-92e2-62abcf545f26", "node_type": "1", "metadata": {}, "hash": "89f76e71d613ea13f0d10247d10928ec24f21d29a6f3005e154ff458a7e30a06", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "I mean they did a bunch of things to make it work but a couple things that are relevant here. There is a larger vocab size because you need all the speech specific tokens like laugh and stuff. Um and then they also extended the context length for thrope scaling. So we got to make sure everything we do supports that. So performance metrics like what do we want to actually do here? Uh we we think about LLM metrics um a little bit here. Uh we we just look at them a little bit differently. So in LLMs you talk about time to first token. Now we're talking about time to first bite or sometimes even time to first sentence. Uh we we need a little bit more of a useful output um from the model before we really start feeling good about our response time. We do think about tokens per second although we're going to think about it differently which I'll explain later. And we mostly think about throughput which is you know how many requests are we able to serve at a given time. So on that, you know, goals perspective, if you ask me like, \"Hey, Philip, how do you want to optimize llama in general?\" I'll like I'll say, \"Well, we want a lot of TPS. We want hundred. We want 500 TPS. We want a thousand tokens per second. We want as many tokens per second as we can get.\" With voice models, you actually don't necessarily need that. In many cases, you only want as many tokens per second as you need for a real-time stream. for Orpheus that's like 83 tokens per second which for like a three billion parameter LLM is nothing. Um but what we actually want to do instead is we want to once we hit that mark start optimizing for time to first bite so that our latency is really good and start optimizing for concurrency so that we can get more connections and spend less on GPUs. So our goal in general if all of these very nice and definitely not AI generated people are all the different like voices that our that our model is capable of creating these are all the voice agents that we're running. How can we make all of these people fit on one or even less than one GPU? That's the goal. ", "mimetype": "text/plain", "start_char_idx": 2897, "end_char_idx": 4983, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.257

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: {"id_": "0659fff4-9937-405a-8175-c49579e80c91", "embedding": null, "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40899ea9-c030-4b43-9b99-b9fdb7c8effe", "node_type": "4", "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "hash": "d75ce9148990d33110afb7e21f57d22aefae30c8e07a82059c484c2b8c34141d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d4dad7ea-1f1e-4d41-935f-4775d6c7c359", "node_type": "1", "metadata": {}, "hash": "a10ee8c433d49c1f560dd8154c781b797d7fe7dec3ceb921339d3e8729ea5456", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[Music] Uh hello everyone. Thank you so much for being here for uh sticking around for this talk. Um I'm going to be talking about optimizing inference for voice models in production. I'm going to be talking mostly about the runtime component but also just a little bit on the infrastructure side. Um just a quick introduction. Um I'm Philip from B 10. Base 10 is a model inference platform. Uh we run production workloads for a wide variety of AI native startups and enterprises. Um I'm based in here in SF. Um I actually just moved here. It's really awesome. My favorite part about being in SF is much better sports teams than I had in Chicago. Um and uh one of my favorite voice models is Orpheus TTS. Um which we're going to be talking about a whole bunch today. Um quick agenda. So we're going to talk about TTS model architecture like what is a text to speech model actually when you look on the config in hugging face. Uh what sort of performance metrics are we looking at? What sort of optimization techniques can we do to make the model better? Um how do we measure whether or not we succeeded? And then finally, what can we do on the infrastructure and client code to not shoot ourselves in the foot after doing a ton of runtime work and then just adding all that latency back by not doing our client code correctly. So architecture, this is one of the things I've been learning this year which has been pretty great uh to to realize it's made life a lot simpler at the runtime level. Now this is wrong like the the the thing up here uh that that I'm going to say is that like everything is an LLM. uh that is that is wrong but it's useful. Um there's kind of like two types of models. There's autogressive transformers models that are LLM or very LLM adjacent. Um you see this in embeddings. You see this in transcription with stuff like whisper TTS um is another example. You also have the more like diffuser image type models. Um which is like a very different optimization problem. But something that's cool is because TTS models are so architecturally similar to LLMs or in many cases derived directly from LLMs, we can access the rich ecosystem of LLM tooling and use it to make TTS models better. So the TTS model that we're going to be using and as an example all day um is Orpheus TTS. We're using it for two reasons. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2337, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.233

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: {"id_": "c0db8c0e-7c89-4efd-82e6-4e299957b5a4", "embedding": null, "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40899ea9-c030-4b43-9b99-b9fdb7c8effe", "node_type": "4", "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "hash": "d75ce9148990d33110afb7e21f57d22aefae30c8e07a82059c484c2b8c34141d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ad70bac5-5d03-4945-a244-90d038a9dae6", "node_type": "1", "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "hash": "bde958b709b9344db0674f2ef949cfc5eb00e329831d13c60dac1a01a62953d2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "829f2359-ba77-4c21-942a-68c1864c7372", "node_type": "1", "metadata": {}, "hash": "8978c328ea7231e7d821e69e6c73d4aec8767b92c9441da2e7b5163253bb881b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Uh let's see let's see if it's actually any faster. Um, so again, the number one thing is going to be simultaneous streams because you want to be able to be very costefficient and use fewer GPU resources to serve a, you know, large amount of traffic. And in this case, our base implementation, I don't necessarily want to like call anyone out because there's a lot of really good ways to run this model. Um, you can get really good performance with VLM. Um but this is just kind of like the offtheshelf um just take it run it completely standard uh implementation. So with variable traffic we're able to support 16 simultaneous streams and with constant traffic 24 simultaneous streams on um an H100 MIG. So this is actually half an H100 GPU. It's a skew that we do a lot because it's really good for these small models where you want the Hopper performance, the Hopper architecture uplift in Tensor RTLM, the FP8 support, but you don't want to pay for like an entire 80 gigabyte GPU for just a 3 billion parameter model. So, you know, we're seeing much better um much much better concurrency. So, if you kind of like price that out with like our list prices and stuff, um, you can get, you know, a few cents per hour of of conversation, um, which is going to be, you know, substantially better than if you're if you have the volume for it. It's going to be substantially better than paying for a sort of like pertoken type API. But, okay, sure, maybe it's cheap at scale, but is it fast? ", "mimetype": "text/plain", "start_char_idx": 8603, "end_char_idx": 10092, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.222

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: {"id_": "28ff36bd-0d1f-472e-accc-106a7c9628f4", "embedding": null, "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40899ea9-c030-4b43-9b99-b9fdb7c8effe", "node_type": "4", "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "hash": "d75ce9148990d33110afb7e21f57d22aefae30c8e07a82059c484c2b8c34141d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3bf554e3-2532-4eb3-a454-6860f8476ff6", "node_type": "1", "metadata": {"project_id": "3f9ad30d-8bbc-4379-b1e2-fc8f8579bc17", "source_url": "https://www.youtube.com/watch?v=gmTHs5T_YAE", "video_id": "e5973db3-9347-43c1-800e-eb21d30fcf43", "embedding_model": "ollama", "id": "25f05372-36de-454d-9e20-f0827aa0835e"}, "hash": "dfcc239616ce1a1f0b944ced717f40901b6ec6b01376753fe88592a22eb7135a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Thank you so much and I'll let you go 8 seconds early. [Music]", "mimetype": "text/plain", "start_char_idx": 15257, "end_char_idx": 15319, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}
Relevance score: 0.141

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: So if you're using this in production, you want a code sample. By the way, this is all up on our GitHub. Um you'll want a code sample that looks a lot more like a benchmarking script where you're using a um multipprocess pool. um you're sharing the you're sharing the session between all of these different requests and you're actually sending traffic with the concurrency that allows you to, you know, saturate this benchmark with, you know, the the multiple concurrent requests. Finally, both of these code samples, they do sit on top of HTTP and HTTP streaming. Um, in many cases, if you're implementing voice pipelines, you're going to use something like LiveKit or Pipecat or something. And you're also potentially going to be using a different protocol. You're going to be using something like websockets or gRPC, which we do have support for. Um, and finally, I wanted to leave you on the thought that these uh, you know, these models are only one part of a voice agent pipeline. So like we can spend a lot more than 15 minutes actually talking about like the very detailed uh implementation mechanics of making your voice model faster of you know we we haven't even touched on stuff like fine-tuning the model um you know custom voices zeroot voice cloning um being able to you know remove static and popping at the end of messages there's there's there's a lot of work to do just on the voice part but it really only is onethird of the problem when I think about voice agents. I think about three parts, listening, thinking, talking. Um, and the most important thing here is again, while you can have great run times, the infrastructure to connect these three together is really what's going to determine your latency. Being able to go from one model to have the next one running in the same data center with, you know, minimal like minimal network overhead in between the two. uh even things as simple as not having to go off and do a hair pin um at the DNS level and come back. If that saves you 10 milliseconds on every step and your voice pipeline has this and you know a chunking algorithm, it's got an interruption model and so you end up having four or five steps. Well, there just hair pinning alone is costing you 40 or 50 milliseconds and that can be 10% of your SLA for for a voice model. So yeah, uh that's that's my that's my main point here is that as much fun as it is to talk about the runtime stuff and as much work as we do there, the the infrastructure and the client implementation is equally important if not more so. Anyway, thank uh so yeah, that's the that's the review. Um thank you all for coming through. Uh I have uh we're doing an event next week at uh Fogo to Chiao which is going to be pretty fun. I'm going to be talking in more detail about building some uh systems with open source models and there's also going to be a lot of stake. So definitely come on through uh if you're interested and um I'm on I'm on Twitter, I'm on LinkedIn, so it's base 10. Um hit me up if you have any questions about this or anything else model performance. 
Relevance score: 0.313

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: I mean they did a bunch of things to make it work but a couple things that are relevant here. There is a larger vocab size because you need all the speech specific tokens like laugh and stuff. Um and then they also extended the context length for thrope scaling. So we got to make sure everything we do supports that. So performance metrics like what do we want to actually do here? Uh we we think about LLM metrics um a little bit here. Uh we we just look at them a little bit differently. So in LLMs you talk about time to first token. Now we're talking about time to first bite or sometimes even time to first sentence. Uh we we need a little bit more of a useful output um from the model before we really start feeling good about our response time. We do think about tokens per second although we're going to think about it differently which I'll explain later. And we mostly think about throughput which is you know how many requests are we able to serve at a given time. So on that, you know, goals perspective, if you ask me like, "Hey, Philip, how do you want to optimize llama in general?" I'll like I'll say, "Well, we want a lot of TPS. We want hundred. We want 500 TPS. We want a thousand tokens per second. We want as many tokens per second as we can get." With voice models, you actually don't necessarily need that. In many cases, you only want as many tokens per second as you need for a real-time stream. for Orpheus that's like 83 tokens per second which for like a three billion parameter LLM is nothing. Um but what we actually want to do instead is we want to once we hit that mark start optimizing for time to first bite so that our latency is really good and start optimizing for concurrency so that we can get more connections and spend less on GPUs. So our goal in general if all of these very nice and definitely not AI generated people are all the different like voices that our that our model is capable of creating these are all the voice agents that we're running. How can we make all of these people fit on one or even less than one GPU? That's the goal. 
Relevance score: 0.257
        ---

        ## 4. Input Data
        User Query:
        how to optimize inference for voice models in production
        ---

        ## 5. Output Format
        Generate a natural language response to the user's query. Ensure your response is directly supported by the "Retrieved Context" section.


----------------------------------------------------------------
-------------------- New Query at 2025-09-01T03:23:21.846408 -------------

        # Chatbot Agent - Role and Guidelines

        ## 1. Role
        You are the Chatbot Agent, a conversational AI designed to answer user questions about AI agents, RAG, context engineering, and multi-agent systems. 
        Your responses must be friendly, concise, and professional. 
        Your primary function is to synthesize information from the provided context to answer the user's query. 
        It is critical that you *only* use the information in the provided context. 
        If the context does not contain a confident answer to the user's question, 
        you *must* respond with "I am sorry, but the provided information does not contain a confident answer to your question." 
        You must never make up or invent information.

        ## 2. Task
        Answer the user's query based solely on the provided context. 
        Maintain a polite and helpful tone. If the user's query is a follow-up, 
        use the chat history to inform your response while still grounding it in the provided context.

        ## 3. Context
        The following information was retrieved from our knowledge base of video transcripts. Use this information to formulate your answer.
        ---
        ### Retrieved Context:
        Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: So if you're using this in production, you want a code sample. By the way, this is all up on our GitHub. Um you'll want a code sample that looks a lot more like a benchmarking script where you're using a um multipprocess pool. um you're sharing the you're sharing the session between all of these different requests and you're actually sending traffic with the concurrency that allows you to, you know, saturate this benchmark with, you know, the the multiple concurrent requests. Finally, both of these code samples, they do sit on top of HTTP and HTTP streaming. Um, in many cases, if you're implementing voice pipelines, you're going to use something like LiveKit or Pipecat or something. And you're also potentially going to be using a different protocol. You're going to be using something like websockets or gRPC, which we do have support for. Um, and finally, I wanted to leave you on the thought that these uh, you know, these models are only one part of a voice agent pipeline. So like we can spend a lot more than 15 minutes actually talking about like the very detailed uh implementation mechanics of making your voice model faster of you know we we haven't even touched on stuff like fine-tuning the model um you know custom voices zeroot voice cloning um being able to you know remove static and popping at the end of messages there's there's there's a lot of work to do just on the voice part but it really only is onethird of the problem when I think about voice agents. I think about three parts, listening, thinking, talking. Um, and the most important thing here is again, while you can have great run times, the infrastructure to connect these three together is really what's going to determine your latency. Being able to go from one model to have the next one running in the same data center with, you know, minimal like minimal network overhead in between the two. uh even things as simple as not having to go off and do a hair pin um at the DNS level and come back. If that saves you 10 milliseconds on every step and your voice pipeline has this and you know a chunking algorithm, it's got an interruption model and so you end up having four or five steps. Well, there just hair pinning alone is costing you 40 or 50 milliseconds and that can be 10% of your SLA for for a voice model. So yeah, uh that's that's my that's my main point here is that as much fun as it is to talk about the runtime stuff and as much work as we do there, the the infrastructure and the client implementation is equally important if not more so. Anyway, thank uh so yeah, that's the that's the review. Um thank you all for coming through. Uh I have uh we're doing an event next week at uh Fogo to Chiao which is going to be pretty fun. I'm going to be talking in more detail about building some uh systems with open source models and there's also going to be a lot of stake. So definitely come on through uh if you're interested and um I'm on I'm on Twitter, I'm on LinkedIn, so it's base 10. Um hit me up if you have any questions about this or anything else model performance. 
Relevance score: 0.361

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: [Music] Uh hello everyone. Thank you so much for being here for uh sticking around for this talk. Um I'm going to be talking about optimizing inference for voice models in production. I'm going to be talking mostly about the runtime component but also just a little bit on the infrastructure side. Um just a quick introduction. Um I'm Philip from B 10. Base 10 is a model inference platform. Uh we run production workloads for a wide variety of AI native startups and enterprises. Um I'm based in here in SF. Um I actually just moved here. It's really awesome. My favorite part about being in SF is much better sports teams than I had in Chicago. Um and uh one of my favorite voice models is Orpheus TTS. Um which we're going to be talking about a whole bunch today. Um quick agenda. So we're going to talk about TTS model architecture like what is a text to speech model actually when you look on the config in hugging face. Uh what sort of performance metrics are we looking at? What sort of optimization techniques can we do to make the model better? Um how do we measure whether or not we succeeded? And then finally, what can we do on the infrastructure and client code to not shoot ourselves in the foot after doing a ton of runtime work and then just adding all that latency back by not doing our client code correctly. So architecture, this is one of the things I've been learning this year which has been pretty great uh to to realize it's made life a lot simpler at the runtime level. Now this is wrong like the the the thing up here uh that that I'm going to say is that like everything is an LLM. uh that is that is wrong but it's useful. Um there's kind of like two types of models. There's autogressive transformers models that are LLM or very LLM adjacent. Um you see this in embeddings. You see this in transcription with stuff like whisper TTS um is another example. You also have the more like diffuser image type models. Um which is like a very different optimization problem. But something that's cool is because TTS models are so architecturally similar to LLMs or in many cases derived directly from LLMs, we can access the rich ecosystem of LLM tooling and use it to make TTS models better. So the TTS model that we're going to be using and as an example all day um is Orpheus TTS. We're using it for two reasons. 
Relevance score: 0.266
        ---

        ## 4. Input Data
        User Query:
        how to optimize inference for voice models in production
        ---

        ## 5. Output Format
        Generate a natural language response to the user's query. Ensure your response is directly supported by the "Retrieved Context" section.


----------------------------------------------------------------
-------------------- New Query at 2025-09-01T03:26:13.436836 -------------

        ## 1. Role
        You are the Chatbot Agent, a conversational AI designed to answer user questions about AI agents, RAG, context engineering, and multi-agent systems. 
        Your responses must be friendly, concise, and professional. 
        Your primary function is to synthesize information from the provided context to answer the user's query. 
        It is critical that you *only* use the information in the provided context. 
        If the context does not contain a confident answer to the user's question, 
        you *must* respond with "I am sorry, but the provided information does not contain a confident answer to your question." 
        You must never make up or invent information.

        ## 2. Task
        Answer the user's query based solely on the provided context. 
        Maintain a polite and helpful tone. If the user's query is a follow-up, 
        use the chat history to inform your response while still grounding it in the provided context.

        ## 3. Context
        The following information was retrieved from our knowledge base of video transcripts. Use this information to formulate your answer.
        ---
        ### Retrieved Context:
        Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: Thank you so much and I'll let you go 8 seconds early. [Music]
Relevance score: 0.250

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: So how do we do it? Bunch of ways. 
Relevance score: 0.235

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: [Music] Uh hello everyone. Thank you so much for being here for uh sticking around for this talk. Um I'm going to be talking about optimizing inference for voice models in production. I'm going to be talking mostly about the runtime component but also just a little bit on the infrastructure side. Um just a quick introduction. Um I'm Philip from B 10. Base 10 is a model inference platform. Uh we run production workloads for a wide variety of AI native startups and enterprises. Um I'm based in here in SF. Um I actually just moved here. It's really awesome. My favorite part about being in SF is much better sports teams than I had in Chicago. Um and uh one of my favorite voice models is Orpheus TTS. Um which we're going to be talking about a whole bunch today. Um quick agenda. So we're going to talk about TTS model architecture like what is a text to speech model actually when you look on the config in hugging face. Uh what sort of performance metrics are we looking at? What sort of optimization techniques can we do to make the model better? Um how do we measure whether or not we succeeded? And then finally, what can we do on the infrastructure and client code to not shoot ourselves in the foot after doing a ton of runtime work and then just adding all that latency back by not doing our client code correctly. So architecture, this is one of the things I've been learning this year which has been pretty great uh to to realize it's made life a lot simpler at the runtime level. Now this is wrong like the the the thing up here uh that that I'm going to say is that like everything is an LLM. uh that is that is wrong but it's useful. Um there's kind of like two types of models. There's autogressive transformers models that are LLM or very LLM adjacent. Um you see this in embeddings. You see this in transcription with stuff like whisper TTS um is another example. You also have the more like diffuser image type models. Um which is like a very different optimization problem. But something that's cool is because TTS models are so architecturally similar to LLMs or in many cases derived directly from LLMs, we can access the rich ecosystem of LLM tooling and use it to make TTS models better. So the TTS model that we're going to be using and as an example all day um is Orpheus TTS. We're using it for two reasons. 
Relevance score: 0.673

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: I mean they did a bunch of things to make it work but a couple things that are relevant here. There is a larger vocab size because you need all the speech specific tokens like laugh and stuff. Um and then they also extended the context length for thrope scaling. So we got to make sure everything we do supports that. So performance metrics like what do we want to actually do here? Uh we we think about LLM metrics um a little bit here. Uh we we just look at them a little bit differently. So in LLMs you talk about time to first token. Now we're talking about time to first bite or sometimes even time to first sentence. Uh we we need a little bit more of a useful output um from the model before we really start feeling good about our response time. We do think about tokens per second although we're going to think about it differently which I'll explain later. And we mostly think about throughput which is you know how many requests are we able to serve at a given time. So on that, you know, goals perspective, if you ask me like, "Hey, Philip, how do you want to optimize llama in general?" I'll like I'll say, "Well, we want a lot of TPS. We want hundred. We want 500 TPS. We want a thousand tokens per second. We want as many tokens per second as we can get." With voice models, you actually don't necessarily need that. In many cases, you only want as many tokens per second as you need for a real-time stream. for Orpheus that's like 83 tokens per second which for like a three billion parameter LLM is nothing. Um but what we actually want to do instead is we want to once we hit that mark start optimizing for time to first bite so that our latency is really good and start optimizing for concurrency so that we can get more connections and spend less on GPUs. So our goal in general if all of these very nice and definitely not AI generated people are all the different like voices that our that our model is capable of creating these are all the voice agents that we're running. How can we make all of these people fit on one or even less than one GPU? That's the goal. 
Relevance score: 0.474
        ---

        ## 4. Input Data
        User Query:
        how to optimize inference for voice models in production
        ---

        ## 5. Output Format
        Generate a natural language response to the user's query. Ensure your response is directly supported by the "Retrieved Context" section.


----------------------------------------------------------------
-------------------- New Query at 2025-09-01T03:30:20.279886 -------------

        ## 1. Role
        You are the Chatbot Agent, a conversational AI designed to answer user questions about AI agents, RAG, context engineering, and multi-agent systems. 
        Your responses must be friendly, concise, and professional. 
        Your primary function is to synthesize information from the provided context to answer the user's query. 
        It is critical that you *only* use the information in the provided context. 
        If the context does not contain a confident answer to the user's question, 
        you *must* respond with "I am sorry, but the provided information does not contain a confident answer to your question." 
        You must never make up or invent information.

        ## 2. Task
        Answer the user's query based solely on the provided context. 
        Maintain a polite and helpful tone. If the user's query is a follow-up, 
        use the chat history to inform your response while still grounding it in the provided context.

        ## 3. Context
        The following information was retrieved from our knowledge base of video transcripts. Use this information to formulate your answer.
        ---
        ### Retrieved Context:
        Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: So if you're using this in production, you want a code sample. By the way, this is all up on our GitHub. Um you'll want a code sample that looks a lot more like a benchmarking script where you're using a um multipprocess pool. um you're sharing the you're sharing the session between all of these different requests and you're actually sending traffic with the concurrency that allows you to, you know, saturate this benchmark with, you know, the the multiple concurrent requests. Finally, both of these code samples, they do sit on top of HTTP and HTTP streaming. Um, in many cases, if you're implementing voice pipelines, you're going to use something like LiveKit or Pipecat or something. And you're also potentially going to be using a different protocol. You're going to be using something like websockets or gRPC, which we do have support for. Um, and finally, I wanted to leave you on the thought that these uh, you know, these models are only one part of a voice agent pipeline. So like we can spend a lot more than 15 minutes actually talking about like the very detailed uh implementation mechanics of making your voice model faster of you know we we haven't even touched on stuff like fine-tuning the model um you know custom voices zeroot voice cloning um being able to you know remove static and popping at the end of messages there's there's there's a lot of work to do just on the voice part but it really only is onethird of the problem when I think about voice agents. I think about three parts, listening, thinking, talking. Um, and the most important thing here is again, while you can have great run times, the infrastructure to connect these three together is really what's going to determine your latency. Being able to go from one model to have the next one running in the same data center with, you know, minimal like minimal network overhead in between the two. uh even things as simple as not having to go off and do a hair pin um at the DNS level and come back. If that saves you 10 milliseconds on every step and your voice pipeline has this and you know a chunking algorithm, it's got an interruption model and so you end up having four or five steps. Well, there just hair pinning alone is costing you 40 or 50 milliseconds and that can be 10% of your SLA for for a voice model. So yeah, uh that's that's my that's my main point here is that as much fun as it is to talk about the runtime stuff and as much work as we do there, the the infrastructure and the client implementation is equally important if not more so. Anyway, thank uh so yeah, that's the that's the review. Um thank you all for coming through. Uh I have uh we're doing an event next week at uh Fogo to Chiao which is going to be pretty fun. I'm going to be talking in more detail about building some uh systems with open source models and there's also going to be a lot of stake. So definitely come on through uh if you're interested and um I'm on I'm on Twitter, I'm on LinkedIn, so it's base 10. Um hit me up if you have any questions about this or anything else model performance. 
Relevance score: 0.385

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: I mean they did a bunch of things to make it work but a couple things that are relevant here. There is a larger vocab size because you need all the speech specific tokens like laugh and stuff. Um and then they also extended the context length for thrope scaling. So we got to make sure everything we do supports that. So performance metrics like what do we want to actually do here? Uh we we think about LLM metrics um a little bit here. Uh we we just look at them a little bit differently. So in LLMs you talk about time to first token. Now we're talking about time to first bite or sometimes even time to first sentence. Uh we we need a little bit more of a useful output um from the model before we really start feeling good about our response time. We do think about tokens per second although we're going to think about it differently which I'll explain later. And we mostly think about throughput which is you know how many requests are we able to serve at a given time. So on that, you know, goals perspective, if you ask me like, "Hey, Philip, how do you want to optimize llama in general?" I'll like I'll say, "Well, we want a lot of TPS. We want hundred. We want 500 TPS. We want a thousand tokens per second. We want as many tokens per second as we can get." With voice models, you actually don't necessarily need that. In many cases, you only want as many tokens per second as you need for a real-time stream. for Orpheus that's like 83 tokens per second which for like a three billion parameter LLM is nothing. Um but what we actually want to do instead is we want to once we hit that mark start optimizing for time to first bite so that our latency is really good and start optimizing for concurrency so that we can get more connections and spend less on GPUs. So our goal in general if all of these very nice and definitely not AI generated people are all the different like voices that our that our model is capable of creating these are all the voice agents that we're running. How can we make all of these people fit on one or even less than one GPU? That's the goal. 
Relevance score: 0.326
        ---

        ## 4. Input Data
        User Query:
        how to optimize inference for voice models in production
        ---

        ## 5. Output Format
        Generate a natural language response to the user's query. Ensure your response is directly supported by the "Retrieved Context" section.


----------------------------------------------------------------
-------------------- New Query at 2025-09-01T03:32:19.310627 -------------

        ## 1. Role
        You are the Chatbot Agent, a conversational AI designed to answer user questions about AI agents, RAG, context engineering, and multi-agent systems. 
        Your responses must be friendly, concise, and professional. 
        Your primary function is to synthesize information from the provided context to answer the user's query. 
        It is critical that you *only* use the information in the provided context. 
        If the context does not contain a confident answer to the user's question, 
        you *must* respond with "I am sorry, but the provided information does not contain a confident answer to your question." 
        You must never make up or invent information.

        ## 2. Task
        Answer the user's query based solely on the provided context. 
        Maintain a polite and helpful tone. If the user's query is a follow-up, 
        use the chat history to inform your response while still grounding it in the provided context.

        ## 3. Context
        The following information was retrieved from our knowledge base of video transcripts. Use this information to formulate your answer.
        ---
        ### Retrieved Context:
        Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: So if you're using this in production, you want a code sample. By the way, this is all up on our GitHub. Um you'll want a code sample that looks a lot more like a benchmarking script where you're using a um multipprocess pool. um you're sharing the you're sharing the session between all of these different requests and you're actually sending traffic with the concurrency that allows you to, you know, saturate this benchmark with, you know, the the multiple concurrent requests. Finally, both of these code samples, they do sit on top of HTTP and HTTP streaming. Um, in many cases, if you're implementing voice pipelines, you're going to use something like LiveKit or Pipecat or something. And you're also potentially going to be using a different protocol. You're going to be using something like websockets or gRPC, which we do have support for. Um, and finally, I wanted to leave you on the thought that these uh, you know, these models are only one part of a voice agent pipeline. So like we can spend a lot more than 15 minutes actually talking about like the very detailed uh implementation mechanics of making your voice model faster of you know we we haven't even touched on stuff like fine-tuning the model um you know custom voices zeroot voice cloning um being able to you know remove static and popping at the end of messages there's there's there's a lot of work to do just on the voice part but it really only is onethird of the problem when I think about voice agents. I think about three parts, listening, thinking, talking. Um, and the most important thing here is again, while you can have great run times, the infrastructure to connect these three together is really what's going to determine your latency. Being able to go from one model to have the next one running in the same data center with, you know, minimal like minimal network overhead in between the two. uh even things as simple as not having to go off and do a hair pin um at the DNS level and come back. If that saves you 10 milliseconds on every step and your voice pipeline has this and you know a chunking algorithm, it's got an interruption model and so you end up having four or five steps. Well, there just hair pinning alone is costing you 40 or 50 milliseconds and that can be 10% of your SLA for for a voice model. So yeah, uh that's that's my that's my main point here is that as much fun as it is to talk about the runtime stuff and as much work as we do there, the the infrastructure and the client implementation is equally important if not more so. Anyway, thank uh so yeah, that's the that's the review. Um thank you all for coming through. Uh I have uh we're doing an event next week at uh Fogo to Chiao which is going to be pretty fun. I'm going to be talking in more detail about building some uh systems with open source models and there's also going to be a lot of stake. So definitely come on through uh if you're interested and um I'm on I'm on Twitter, I'm on LinkedIn, so it's base 10. Um hit me up if you have any questions about this or anything else model performance. 
Relevance score: 0.350

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: I mean they did a bunch of things to make it work but a couple things that are relevant here. There is a larger vocab size because you need all the speech specific tokens like laugh and stuff. Um and then they also extended the context length for thrope scaling. So we got to make sure everything we do supports that. So performance metrics like what do we want to actually do here? Uh we we think about LLM metrics um a little bit here. Uh we we just look at them a little bit differently. So in LLMs you talk about time to first token. Now we're talking about time to first bite or sometimes even time to first sentence. Uh we we need a little bit more of a useful output um from the model before we really start feeling good about our response time. We do think about tokens per second although we're going to think about it differently which I'll explain later. And we mostly think about throughput which is you know how many requests are we able to serve at a given time. So on that, you know, goals perspective, if you ask me like, "Hey, Philip, how do you want to optimize llama in general?" I'll like I'll say, "Well, we want a lot of TPS. We want hundred. We want 500 TPS. We want a thousand tokens per second. We want as many tokens per second as we can get." With voice models, you actually don't necessarily need that. In many cases, you only want as many tokens per second as you need for a real-time stream. for Orpheus that's like 83 tokens per second which for like a three billion parameter LLM is nothing. Um but what we actually want to do instead is we want to once we hit that mark start optimizing for time to first bite so that our latency is really good and start optimizing for concurrency so that we can get more connections and spend less on GPUs. So our goal in general if all of these very nice and definitely not AI generated people are all the different like voices that our that our model is capable of creating these are all the voice agents that we're running. How can we make all of these people fit on one or even less than one GPU? That's the goal. 
Relevance score: 0.287

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: [Music] Uh hello everyone. Thank you so much for being here for uh sticking around for this talk. Um I'm going to be talking about optimizing inference for voice models in production. I'm going to be talking mostly about the runtime component but also just a little bit on the infrastructure side. Um just a quick introduction. Um I'm Philip from B 10. Base 10 is a model inference platform. Uh we run production workloads for a wide variety of AI native startups and enterprises. Um I'm based in here in SF. Um I actually just moved here. It's really awesome. My favorite part about being in SF is much better sports teams than I had in Chicago. Um and uh one of my favorite voice models is Orpheus TTS. Um which we're going to be talking about a whole bunch today. Um quick agenda. So we're going to talk about TTS model architecture like what is a text to speech model actually when you look on the config in hugging face. Uh what sort of performance metrics are we looking at? What sort of optimization techniques can we do to make the model better? Um how do we measure whether or not we succeeded? And then finally, what can we do on the infrastructure and client code to not shoot ourselves in the foot after doing a ton of runtime work and then just adding all that latency back by not doing our client code correctly. So architecture, this is one of the things I've been learning this year which has been pretty great uh to to realize it's made life a lot simpler at the runtime level. Now this is wrong like the the the thing up here uh that that I'm going to say is that like everything is an LLM. uh that is that is wrong but it's useful. Um there's kind of like two types of models. There's autogressive transformers models that are LLM or very LLM adjacent. Um you see this in embeddings. You see this in transcription with stuff like whisper TTS um is another example. You also have the more like diffuser image type models. Um which is like a very different optimization problem. But something that's cool is because TTS models are so architecturally similar to LLMs or in many cases derived directly from LLMs, we can access the rich ecosystem of LLM tooling and use it to make TTS models better. So the TTS model that we're going to be using and as an example all day um is Orpheus TTS. We're using it for two reasons. 
Relevance score: 0.677
        ---

        ## 4. Input Data
        User Query:
        how to optimize inference for voice models in production
        ---

        ## 5. Output Format
        Generate a natural language response to the user's query. Ensure your response is directly supported by the "Retrieved Context" section.


----------------------------------------------------------------
-------------------- New Query at 2025-09-01T03:35:38.476816 -------------

        ## 1. Role
        You are the Chatbot Agent, a conversational AI designed to answer user questions about AI agents, RAG, context engineering, and multi-agent systems. 
        Your responses must be friendly, concise, and professional. 
        Your primary function is to synthesize information from the provided context to answer the user's query. 
        It is critical that you *only* use the information in the provided context. 
        If the context does not contain a confident answer to the user's question, 
        you *must* respond with "I am sorry, but the provided information does not contain a confident answer to your question." 
        You must never make up or invent information.

        ## 2. Task
        Answer the user's query based solely on the provided context. 
        Maintain a polite and helpful tone. If the user's query is a follow-up, 
        use the chat history to inform your response while still grounding it in the provided context.

        ## 3. Context
        The following information was retrieved from our knowledge base of video transcripts. Use this information to formulate your answer.
        ---
        ### Retrieved Context:
        Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: Thank you so much and I'll let you go 8 seconds early. [Music]
Relevance score: 0.113

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: So if you're using this in production, you want a code sample. By the way, this is all up on our GitHub. Um you'll want a code sample that looks a lot more like a benchmarking script where you're using a um multipprocess pool. um you're sharing the you're sharing the session between all of these different requests and you're actually sending traffic with the concurrency that allows you to, you know, saturate this benchmark with, you know, the the multiple concurrent requests. Finally, both of these code samples, they do sit on top of HTTP and HTTP streaming. Um, in many cases, if you're implementing voice pipelines, you're going to use something like LiveKit or Pipecat or something. And you're also potentially going to be using a different protocol. You're going to be using something like websockets or gRPC, which we do have support for. Um, and finally, I wanted to leave you on the thought that these uh, you know, these models are only one part of a voice agent pipeline. So like we can spend a lot more than 15 minutes actually talking about like the very detailed uh implementation mechanics of making your voice model faster of you know we we haven't even touched on stuff like fine-tuning the model um you know custom voices zeroot voice cloning um being able to you know remove static and popping at the end of messages there's there's there's a lot of work to do just on the voice part but it really only is onethird of the problem when I think about voice agents. I think about three parts, listening, thinking, talking. Um, and the most important thing here is again, while you can have great run times, the infrastructure to connect these three together is really what's going to determine your latency. Being able to go from one model to have the next one running in the same data center with, you know, minimal like minimal network overhead in between the two. uh even things as simple as not having to go off and do a hair pin um at the DNS level and come back. If that saves you 10 milliseconds on every step and your voice pipeline has this and you know a chunking algorithm, it's got an interruption model and so you end up having four or five steps. Well, there just hair pinning alone is costing you 40 or 50 milliseconds and that can be 10% of your SLA for for a voice model. So yeah, uh that's that's my that's my main point here is that as much fun as it is to talk about the runtime stuff and as much work as we do there, the the infrastructure and the client implementation is equally important if not more so. Anyway, thank uh so yeah, that's the that's the review. Um thank you all for coming through. Uh I have uh we're doing an event next week at uh Fogo to Chiao which is going to be pretty fun. I'm going to be talking in more detail about building some uh systems with open source models and there's also going to be a lot of stake. So definitely come on through uh if you're interested and um I'm on I'm on Twitter, I'm on LinkedIn, so it's base 10. Um hit me up if you have any questions about this or anything else model performance. 
Relevance score: 0.085

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: [Music] Uh hello everyone. Thank you so much for being here for uh sticking around for this talk. Um I'm going to be talking about optimizing inference for voice models in production. I'm going to be talking mostly about the runtime component but also just a little bit on the infrastructure side. Um just a quick introduction. Um I'm Philip from B 10. Base 10 is a model inference platform. Uh we run production workloads for a wide variety of AI native startups and enterprises. Um I'm based in here in SF. Um I actually just moved here. It's really awesome. My favorite part about being in SF is much better sports teams than I had in Chicago. Um and uh one of my favorite voice models is Orpheus TTS. Um which we're going to be talking about a whole bunch today. Um quick agenda. So we're going to talk about TTS model architecture like what is a text to speech model actually when you look on the config in hugging face. Uh what sort of performance metrics are we looking at? What sort of optimization techniques can we do to make the model better? Um how do we measure whether or not we succeeded? And then finally, what can we do on the infrastructure and client code to not shoot ourselves in the foot after doing a ton of runtime work and then just adding all that latency back by not doing our client code correctly. So architecture, this is one of the things I've been learning this year which has been pretty great uh to to realize it's made life a lot simpler at the runtime level. Now this is wrong like the the the thing up here uh that that I'm going to say is that like everything is an LLM. uh that is that is wrong but it's useful. Um there's kind of like two types of models. There's autogressive transformers models that are LLM or very LLM adjacent. Um you see this in embeddings. You see this in transcription with stuff like whisper TTS um is another example. You also have the more like diffuser image type models. Um which is like a very different optimization problem. But something that's cool is because TTS models are so architecturally similar to LLMs or in many cases derived directly from LLMs, we can access the rich ecosystem of LLM tooling and use it to make TTS models better. So the TTS model that we're going to be using and as an example all day um is Orpheus TTS. We're using it for two reasons. 
Relevance score: 0.664

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: I mean they did a bunch of things to make it work but a couple things that are relevant here. There is a larger vocab size because you need all the speech specific tokens like laugh and stuff. Um and then they also extended the context length for thrope scaling. So we got to make sure everything we do supports that. So performance metrics like what do we want to actually do here? Uh we we think about LLM metrics um a little bit here. Uh we we just look at them a little bit differently. So in LLMs you talk about time to first token. Now we're talking about time to first bite or sometimes even time to first sentence. Uh we we need a little bit more of a useful output um from the model before we really start feeling good about our response time. We do think about tokens per second although we're going to think about it differently which I'll explain later. And we mostly think about throughput which is you know how many requests are we able to serve at a given time. So on that, you know, goals perspective, if you ask me like, "Hey, Philip, how do you want to optimize llama in general?" I'll like I'll say, "Well, we want a lot of TPS. We want hundred. We want 500 TPS. We want a thousand tokens per second. We want as many tokens per second as we can get." With voice models, you actually don't necessarily need that. In many cases, you only want as many tokens per second as you need for a real-time stream. for Orpheus that's like 83 tokens per second which for like a three billion parameter LLM is nothing. Um but what we actually want to do instead is we want to once we hit that mark start optimizing for time to first bite so that our latency is really good and start optimizing for concurrency so that we can get more connections and spend less on GPUs. So our goal in general if all of these very nice and definitely not AI generated people are all the different like voices that our that our model is capable of creating these are all the voice agents that we're running. How can we make all of these people fit on one or even less than one GPU? That's the goal. 
Relevance score: 0.464
        ---

        ## 4. Input Data
        User Query:
        how to optimize inference for voice models in production
        ---

        ## 5. Output Format
        Generate a natural language response to the user's query. Ensure your response is directly supported by the "Retrieved Context" section.


----------------------------------------------------------------
-------------------- New Query at 2025-09-01T03:37:02.974454 -------------

        ## 1. Role
        You are the Chatbot Agent, a conversational AI designed to answer user questions about AI agents, RAG, context engineering, and multi-agent systems. 
        Your responses must be friendly, concise, and professional. 
        Your primary function is to synthesize information from the provided context to answer the user's query. 
        It is critical that you *only* use the information in the provided context. 
        If the context does not contain a confident answer to the user's question, 
        you *must* respond with "I am sorry, but the provided information does not contain a confident answer to your question." 
        You must never make up or invent information.

        ## 2. Task
        Answer the user's query based solely on the provided context. 
        Maintain a polite and helpful tone. If the user's query is a follow-up, 
        use the chat history to inform your response while still grounding it in the provided context.

        ## 3. Context
        The following information was retrieved from our knowledge base of video transcripts. Use this information to formulate your answer.
        ---
        ### Retrieved Context:
        Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: Thank you so much and I'll let you go 8 seconds early. [Music]
Relevance score: 0.113

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: So if you're using this in production, you want a code sample. By the way, this is all up on our GitHub. Um you'll want a code sample that looks a lot more like a benchmarking script where you're using a um multipprocess pool. um you're sharing the you're sharing the session between all of these different requests and you're actually sending traffic with the concurrency that allows you to, you know, saturate this benchmark with, you know, the the multiple concurrent requests. Finally, both of these code samples, they do sit on top of HTTP and HTTP streaming. Um, in many cases, if you're implementing voice pipelines, you're going to use something like LiveKit or Pipecat or something. And you're also potentially going to be using a different protocol. You're going to be using something like websockets or gRPC, which we do have support for. Um, and finally, I wanted to leave you on the thought that these uh, you know, these models are only one part of a voice agent pipeline. So like we can spend a lot more than 15 minutes actually talking about like the very detailed uh implementation mechanics of making your voice model faster of you know we we haven't even touched on stuff like fine-tuning the model um you know custom voices zeroot voice cloning um being able to you know remove static and popping at the end of messages there's there's there's a lot of work to do just on the voice part but it really only is onethird of the problem when I think about voice agents. I think about three parts, listening, thinking, talking. Um, and the most important thing here is again, while you can have great run times, the infrastructure to connect these three together is really what's going to determine your latency. Being able to go from one model to have the next one running in the same data center with, you know, minimal like minimal network overhead in between the two. uh even things as simple as not having to go off and do a hair pin um at the DNS level and come back. If that saves you 10 milliseconds on every step and your voice pipeline has this and you know a chunking algorithm, it's got an interruption model and so you end up having four or five steps. Well, there just hair pinning alone is costing you 40 or 50 milliseconds and that can be 10% of your SLA for for a voice model. So yeah, uh that's that's my that's my main point here is that as much fun as it is to talk about the runtime stuff and as much work as we do there, the the infrastructure and the client implementation is equally important if not more so. Anyway, thank uh so yeah, that's the that's the review. Um thank you all for coming through. Uh I have uh we're doing an event next week at uh Fogo to Chiao which is going to be pretty fun. I'm going to be talking in more detail about building some uh systems with open source models and there's also going to be a lot of stake. So definitely come on through uh if you're interested and um I'm on I'm on Twitter, I'm on LinkedIn, so it's base 10. Um hit me up if you have any questions about this or anything else model performance. 
Relevance score: 0.085

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: [Music] Uh hello everyone. Thank you so much for being here for uh sticking around for this talk. Um I'm going to be talking about optimizing inference for voice models in production. I'm going to be talking mostly about the runtime component but also just a little bit on the infrastructure side. Um just a quick introduction. Um I'm Philip from B 10. Base 10 is a model inference platform. Uh we run production workloads for a wide variety of AI native startups and enterprises. Um I'm based in here in SF. Um I actually just moved here. It's really awesome. My favorite part about being in SF is much better sports teams than I had in Chicago. Um and uh one of my favorite voice models is Orpheus TTS. Um which we're going to be talking about a whole bunch today. Um quick agenda. So we're going to talk about TTS model architecture like what is a text to speech model actually when you look on the config in hugging face. Uh what sort of performance metrics are we looking at? What sort of optimization techniques can we do to make the model better? Um how do we measure whether or not we succeeded? And then finally, what can we do on the infrastructure and client code to not shoot ourselves in the foot after doing a ton of runtime work and then just adding all that latency back by not doing our client code correctly. So architecture, this is one of the things I've been learning this year which has been pretty great uh to to realize it's made life a lot simpler at the runtime level. Now this is wrong like the the the thing up here uh that that I'm going to say is that like everything is an LLM. uh that is that is wrong but it's useful. Um there's kind of like two types of models. There's autogressive transformers models that are LLM or very LLM adjacent. Um you see this in embeddings. You see this in transcription with stuff like whisper TTS um is another example. You also have the more like diffuser image type models. Um which is like a very different optimization problem. But something that's cool is because TTS models are so architecturally similar to LLMs or in many cases derived directly from LLMs, we can access the rich ecosystem of LLM tooling and use it to make TTS models better. So the TTS model that we're going to be using and as an example all day um is Orpheus TTS. We're using it for two reasons. 
Relevance score: 0.664

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: I mean they did a bunch of things to make it work but a couple things that are relevant here. There is a larger vocab size because you need all the speech specific tokens like laugh and stuff. Um and then they also extended the context length for thrope scaling. So we got to make sure everything we do supports that. So performance metrics like what do we want to actually do here? Uh we we think about LLM metrics um a little bit here. Uh we we just look at them a little bit differently. So in LLMs you talk about time to first token. Now we're talking about time to first bite or sometimes even time to first sentence. Uh we we need a little bit more of a useful output um from the model before we really start feeling good about our response time. We do think about tokens per second although we're going to think about it differently which I'll explain later. And we mostly think about throughput which is you know how many requests are we able to serve at a given time. So on that, you know, goals perspective, if you ask me like, "Hey, Philip, how do you want to optimize llama in general?" I'll like I'll say, "Well, we want a lot of TPS. We want hundred. We want 500 TPS. We want a thousand tokens per second. We want as many tokens per second as we can get." With voice models, you actually don't necessarily need that. In many cases, you only want as many tokens per second as you need for a real-time stream. for Orpheus that's like 83 tokens per second which for like a three billion parameter LLM is nothing. Um but what we actually want to do instead is we want to once we hit that mark start optimizing for time to first bite so that our latency is really good and start optimizing for concurrency so that we can get more connections and spend less on GPUs. So our goal in general if all of these very nice and definitely not AI generated people are all the different like voices that our that our model is capable of creating these are all the voice agents that we're running. How can we make all of these people fit on one or even less than one GPU? That's the goal. 
Relevance score: 0.464

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: Yes, it's fast. Um so with the you know with the tot implementation on the MIGs and on the H100s we can actually get all the way down to 150 millisecond time to first bite um in like real world uh testing that we've done. Now that we'll we'll talk in a minute like that doesn't mean your whole pipeline is that fast. That's just like one part of the pipeline. Uh but it's it's important because you know you definitely don't want to be spending a lot of time waiting around for that first token. So to kind of transition into that that discussion of like what can go wrong here like you have this graph and you have this you know nice uh config that I had up here and you're like all right cool I'm going to take this I'm going to put it in production and I'm going to see the results that uh that he put up on screen and it's going to work great and the answer is no it's not it's a little bit harder than that. Um, so the thing is like non-runtime factors when we get especially with these small models and with these multimodal systems can actually be like way more important than your runtime. Um, and that's your lat your infrastructure and your client code because you know I I showed here all right maybe maybe I got it you know I I cut the runtime in in half um from from the base implementation I I saved a couple hundred milliseconds very easy to add those couple hundred milliseconds back and well beyond that by you know sending my query to New York instead of California or by having to establish a session every time I uh you know run my client code. Um, so a few like pitfalls to avoid. Um, number one, like if you go in, you know, our our model library or something and we're just trying to get you started very quickly um with this with this kind of inference sample. Um, it's basically going to be, hey, use requests, make a stream, stream it to your local computer and start, you know, playing it on ffmpeg or something. The issue is that here like the requests are going to be sent sequentially and you need to create a new session every time. um that takes time. 
Relevance score: 0.174

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: Um so first off it's an LLM. Um, if you are running an LLM with like VLM for example, um, you can generally in many cases get better performance with Tensor Rotlm. Uh, Tensor Rot is something that we've been using at Base 10 a lot. I like to joke that I'm the unofficial marketing department for Tensor RT LLM because of how much I talk about it. Uh, but it really is fast. It can be a little bit complicated from a developer experience perspective to get up and running with it, but once you are up and running, um, it it works really well. Uh, we can also just like quantize the model. Um, even though it's small, you can always make it faster by making it smaller. Uh, with Hopper architecture, we quantize this model to FP8 pretty successfully. I know usually quantizing really small models like this can lead to performance degragation, but for this model, it's working pretty well in FP8, even when we quantize the KV cache. And then a lot of the other runtime stuff is actually more like audio specific than it is LLM specific. So, one of the big challenges that we don't have with LLMs, which are just parsing nice, convenient bits of text back and forth, is you have your audio, you have your audio codec, you have your decoding, all that kind of stuff. So, we use snack, which I was very disappointed to learn is not an actual tasty snack, um, but an audio decoder. And we actually use torch compile, um, and torch compile you might be used to running on, you know, a a model u compiling your model weights to make your runtime faster. We're actually using the same kind of system with torch compile and with um PyTorch inference mode on the audio decoder and running that on the GPU. Um we make sure that all the token batching um token level batching works well throughout the entire pipeline and support multiple streaming protocols. Um yeah, so these are the engine settings that you would need. um you've got the you know quantization type of FP8 KV the FP8 context uh FMHA um in order to you know support the uh support the um hopper architecture and the uh quantization there. Um and here's a quick code sample of I got a little ahead of my slides I guess. Here's a little quick code sample of the audio decoding. Um so we are basically you know batching. Um usually we would talk about continuous batching when we're talking about LLM optimization. We want to package all those tokens together. In this case we are doing dynamic batching. Um so we're trying to pack as much into a batch as we can but every 15 milliseconds we're going to uh shoot it out. Um you've got that timeout setup here. Um if you want to trade off um for a little bit of latency for more throughput you can make that batch bigger. Um, so yeah, we don't have token level continuous batching yet here, but we do have dynamic batching, which is going to get you pretty close. Um, and because of this, actually something that's that I was surprised about uh when we profiled this is that our TTS imple implementation with Orpheus is actually in many cases CPUbound, which is kind of where you want to be. Um, you can throw more CPUs at a resource uh pretty pretty efficiently. Um even though the next token production and the audio co decoding are both on the GPU um both of those loops hit the CPU at different points um and that can actually be the bottleneck in the number of simultaneous streams that we're able to create. So how do we do like I just showed you a lot of code and talked through it really quickly without really getting into depth. Uh that could all just be smoke and mirrors. 
Relevance score: 0.156

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: Uh let's see let's see if it's actually any faster. Um, so again, the number one thing is going to be simultaneous streams because you want to be able to be very costefficient and use fewer GPU resources to serve a, you know, large amount of traffic. And in this case, our base implementation, I don't necessarily want to like call anyone out because there's a lot of really good ways to run this model. Um, you can get really good performance with VLM. Um but this is just kind of like the offtheshelf um just take it run it completely standard uh implementation. So with variable traffic we're able to support 16 simultaneous streams and with constant traffic 24 simultaneous streams on um an H100 MIG. So this is actually half an H100 GPU. It's a skew that we do a lot because it's really good for these small models where you want the Hopper performance, the Hopper architecture uplift in Tensor RTLM, the FP8 support, but you don't want to pay for like an entire 80 gigabyte GPU for just a 3 billion parameter model. So, you know, we're seeing much better um much much better concurrency. So, if you kind of like price that out with like our list prices and stuff, um, you can get, you know, a few cents per hour of of conversation, um, which is going to be, you know, substantially better than if you're if you have the volume for it. It's going to be substantially better than paying for a sort of like pertoken type API. But, okay, sure, maybe it's cheap at scale, but is it fast? 
Relevance score: 0.318

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: Okay, three reasons. The two reasons are because it's open source and it's really good. And also I think uh Elias and Amu and everyone at Canopy Labs is really awesome. So uh that's the that's the third reason we're we're talking about their model. But it's a Llama 3.2 3B backbone. So like if you look at this is the little like config from hugging face copy and pasted onto the screen. It's a llama for causal LM architecture. And so because of that we can do like all of our normal llama stuff to this model and make it faster. Um they did a couple things. 
Relevance score: 0.429
        ---

        ## 4. Input Data
        User Query:
        how to optimize inference for voice models in production
        ---

        ## 5. Output Format
        Generate a natural language response to the user's query. Ensure your response is directly supported by the "Retrieved Context" section.


----------------------------------------------------------------
-------------------- New Query at 2025-09-01T03:38:56.513234 -------------

        ## 1. Role
        You are the Chatbot Agent, a conversational AI designed to answer user questions about AI agents, RAG, context engineering, and multi-agent systems. 
        Your responses must be friendly, concise, and professional. 
        Your primary function is to synthesize information from the provided context to answer the user's query. 
        It is critical that you *only* use the information in the provided context. 
        If the context does not contain a confident answer to the user's question, 
        you *must* respond with "I am sorry, but the provided information does not contain a confident answer to your question." 
        You must never make up or invent information.

        ## 2. Task
        Answer the user's query based solely on the provided context. 
        Maintain a polite and helpful tone. If the user's query is a follow-up, 
        use the chat history to inform your response while still grounding it in the provided context.

        ## 3. Context
        The following information was retrieved from our knowledge base of video transcripts. Use this information to formulate your answer.
        ---
        ### Retrieved Context:
        Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: [Music] Uh hello everyone. Thank you so much for being here for uh sticking around for this talk. Um I'm going to be talking about optimizing inference for voice models in production. I'm going to be talking mostly about the runtime component but also just a little bit on the infrastructure side. Um just a quick introduction. Um I'm Philip from B 10. Base 10 is a model inference platform. Uh we run production workloads for a wide variety of AI native startups and enterprises. Um I'm based in here in SF. Um I actually just moved here. It's really awesome. My favorite part about being in SF is much better sports teams than I had in Chicago. Um and uh one of my favorite voice models is Orpheus TTS. Um which we're going to be talking about a whole bunch today. Um quick agenda. So we're going to talk about TTS model architecture like what is a text to speech model actually when you look on the config in hugging face. Uh what sort of performance metrics are we looking at? What sort of optimization techniques can we do to make the model better? Um how do we measure whether or not we succeeded? And then finally, what can we do on the infrastructure and client code to not shoot ourselves in the foot after doing a ton of runtime work and then just adding all that latency back by not doing our client code correctly. So architecture, this is one of the things I've been learning this year which has been pretty great uh to to realize it's made life a lot simpler at the runtime level. Now this is wrong like the the the thing up here uh that that I'm going to say is that like everything is an LLM. uh that is that is wrong but it's useful. Um there's kind of like two types of models. There's autogressive transformers models that are LLM or very LLM adjacent. Um you see this in embeddings. You see this in transcription with stuff like whisper TTS um is another example. You also have the more like diffuser image type models. Um which is like a very different optimization problem. But something that's cool is because TTS models are so architecturally similar to LLMs or in many cases derived directly from LLMs, we can access the rich ecosystem of LLM tooling and use it to make TTS models better. So the TTS model that we're going to be using and as an example all day um is Orpheus TTS. We're using it for two reasons. 
Relevance score: 0.107

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: So if you're using this in production, you want a code sample. By the way, this is all up on our GitHub. Um you'll want a code sample that looks a lot more like a benchmarking script where you're using a um multipprocess pool. um you're sharing the you're sharing the session between all of these different requests and you're actually sending traffic with the concurrency that allows you to, you know, saturate this benchmark with, you know, the the multiple concurrent requests. Finally, both of these code samples, they do sit on top of HTTP and HTTP streaming. Um, in many cases, if you're implementing voice pipelines, you're going to use something like LiveKit or Pipecat or something. And you're also potentially going to be using a different protocol. You're going to be using something like websockets or gRPC, which we do have support for. Um, and finally, I wanted to leave you on the thought that these uh, you know, these models are only one part of a voice agent pipeline. So like we can spend a lot more than 15 minutes actually talking about like the very detailed uh implementation mechanics of making your voice model faster of you know we we haven't even touched on stuff like fine-tuning the model um you know custom voices zeroot voice cloning um being able to you know remove static and popping at the end of messages there's there's there's a lot of work to do just on the voice part but it really only is onethird of the problem when I think about voice agents. I think about three parts, listening, thinking, talking. Um, and the most important thing here is again, while you can have great run times, the infrastructure to connect these three together is really what's going to determine your latency. Being able to go from one model to have the next one running in the same data center with, you know, minimal like minimal network overhead in between the two. uh even things as simple as not having to go off and do a hair pin um at the DNS level and come back. If that saves you 10 milliseconds on every step and your voice pipeline has this and you know a chunking algorithm, it's got an interruption model and so you end up having four or five steps. Well, there just hair pinning alone is costing you 40 or 50 milliseconds and that can be 10% of your SLA for for a voice model. So yeah, uh that's that's my that's my main point here is that as much fun as it is to talk about the runtime stuff and as much work as we do there, the the infrastructure and the client implementation is equally important if not more so. Anyway, thank uh so yeah, that's the that's the review. Um thank you all for coming through. Uh I have uh we're doing an event next week at uh Fogo to Chiao which is going to be pretty fun. I'm going to be talking in more detail about building some uh systems with open source models and there's also going to be a lot of stake. So definitely come on through uh if you're interested and um I'm on I'm on Twitter, I'm on LinkedIn, so it's base 10. Um hit me up if you have any questions about this or anything else model performance. 
Relevance score: 0.078

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: I mean they did a bunch of things to make it work but a couple things that are relevant here. There is a larger vocab size because you need all the speech specific tokens like laugh and stuff. Um and then they also extended the context length for thrope scaling. So we got to make sure everything we do supports that. So performance metrics like what do we want to actually do here? Uh we we think about LLM metrics um a little bit here. Uh we we just look at them a little bit differently. So in LLMs you talk about time to first token. Now we're talking about time to first bite or sometimes even time to first sentence. Uh we we need a little bit more of a useful output um from the model before we really start feeling good about our response time. We do think about tokens per second although we're going to think about it differently which I'll explain later. And we mostly think about throughput which is you know how many requests are we able to serve at a given time. So on that, you know, goals perspective, if you ask me like, "Hey, Philip, how do you want to optimize llama in general?" I'll like I'll say, "Well, we want a lot of TPS. We want hundred. We want 500 TPS. We want a thousand tokens per second. We want as many tokens per second as we can get." With voice models, you actually don't necessarily need that. In many cases, you only want as many tokens per second as you need for a real-time stream. for Orpheus that's like 83 tokens per second which for like a three billion parameter LLM is nothing. Um but what we actually want to do instead is we want to once we hit that mark start optimizing for time to first bite so that our latency is really good and start optimizing for concurrency so that we can get more connections and spend less on GPUs. So our goal in general if all of these very nice and definitely not AI generated people are all the different like voices that our that our model is capable of creating these are all the voice agents that we're running. How can we make all of these people fit on one or even less than one GPU? That's the goal. 
Relevance score: 0.478

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: Okay, three reasons. The two reasons are because it's open source and it's really good. And also I think uh Elias and Amu and everyone at Canopy Labs is really awesome. So uh that's the that's the third reason we're we're talking about their model. But it's a Llama 3.2 3B backbone. So like if you look at this is the little like config from hugging face copy and pasted onto the screen. It's a llama for causal LM architecture. And so because of that we can do like all of our normal llama stuff to this model and make it faster. Um they did a couple things. 
Relevance score: 0.181

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: Uh let's see let's see if it's actually any faster. Um, so again, the number one thing is going to be simultaneous streams because you want to be able to be very costefficient and use fewer GPU resources to serve a, you know, large amount of traffic. And in this case, our base implementation, I don't necessarily want to like call anyone out because there's a lot of really good ways to run this model. Um, you can get really good performance with VLM. Um but this is just kind of like the offtheshelf um just take it run it completely standard uh implementation. So with variable traffic we're able to support 16 simultaneous streams and with constant traffic 24 simultaneous streams on um an H100 MIG. So this is actually half an H100 GPU. It's a skew that we do a lot because it's really good for these small models where you want the Hopper performance, the Hopper architecture uplift in Tensor RTLM, the FP8 support, but you don't want to pay for like an entire 80 gigabyte GPU for just a 3 billion parameter model. So, you know, we're seeing much better um much much better concurrency. So, if you kind of like price that out with like our list prices and stuff, um, you can get, you know, a few cents per hour of of conversation, um, which is going to be, you know, substantially better than if you're if you have the volume for it. It's going to be substantially better than paying for a sort of like pertoken type API. But, okay, sure, maybe it's cheap at scale, but is it fast? 
Relevance score: 0.349

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: Um so first off it's an LLM. Um, if you are running an LLM with like VLM for example, um, you can generally in many cases get better performance with Tensor Rotlm. Uh, Tensor Rot is something that we've been using at Base 10 a lot. I like to joke that I'm the unofficial marketing department for Tensor RT LLM because of how much I talk about it. Uh, but it really is fast. It can be a little bit complicated from a developer experience perspective to get up and running with it, but once you are up and running, um, it it works really well. Uh, we can also just like quantize the model. Um, even though it's small, you can always make it faster by making it smaller. Uh, with Hopper architecture, we quantize this model to FP8 pretty successfully. I know usually quantizing really small models like this can lead to performance degragation, but for this model, it's working pretty well in FP8, even when we quantize the KV cache. And then a lot of the other runtime stuff is actually more like audio specific than it is LLM specific. So, one of the big challenges that we don't have with LLMs, which are just parsing nice, convenient bits of text back and forth, is you have your audio, you have your audio codec, you have your decoding, all that kind of stuff. So, we use snack, which I was very disappointed to learn is not an actual tasty snack, um, but an audio decoder. And we actually use torch compile, um, and torch compile you might be used to running on, you know, a a model u compiling your model weights to make your runtime faster. We're actually using the same kind of system with torch compile and with um PyTorch inference mode on the audio decoder and running that on the GPU. Um we make sure that all the token batching um token level batching works well throughout the entire pipeline and support multiple streaming protocols. Um yeah, so these are the engine settings that you would need. um you've got the you know quantization type of FP8 KV the FP8 context uh FMHA um in order to you know support the uh support the um hopper architecture and the uh quantization there. Um and here's a quick code sample of I got a little ahead of my slides I guess. Here's a little quick code sample of the audio decoding. Um so we are basically you know batching. Um usually we would talk about continuous batching when we're talking about LLM optimization. We want to package all those tokens together. In this case we are doing dynamic batching. Um so we're trying to pack as much into a batch as we can but every 15 milliseconds we're going to uh shoot it out. Um you've got that timeout setup here. Um if you want to trade off um for a little bit of latency for more throughput you can make that batch bigger. Um, so yeah, we don't have token level continuous batching yet here, but we do have dynamic batching, which is going to get you pretty close. Um, and because of this, actually something that's that I was surprised about uh when we profiled this is that our TTS imple implementation with Orpheus is actually in many cases CPUbound, which is kind of where you want to be. Um, you can throw more CPUs at a resource uh pretty pretty efficiently. Um even though the next token production and the audio co decoding are both on the GPU um both of those loops hit the CPU at different points um and that can actually be the bottleneck in the number of simultaneous streams that we're able to create. So how do we do like I just showed you a lot of code and talked through it really quickly without really getting into depth. Uh that could all just be smoke and mirrors. 
Relevance score: 0.315
        ---

        ## 4. Input Data
        User Query:
        how to optimize inference for voice models in production
        ---

        ## 5. Output Format
        Generate a natural language response to the user's query. Ensure your response is directly supported by the "Retrieved Context" section.


----------------------------------------------------------------
-------------------- New Query at 2025-09-01T03:40:42.945645 -------------

        ## 1. Role
        You are the Chatbot Agent, a conversational AI designed to answer user questions about AI agents, RAG, context engineering, and multi-agent systems. 
        Your responses must be friendly, concise, and professional. 
        Your primary function is to synthesize information from the provided context to answer the user's query. 
        It is critical that you *only* use the information in the provided context. 
        If the context does not contain a confident answer to the user's question, 
        you *must* respond with "I am sorry, but the provided information does not contain a confident answer to your question." 
        You must never make up or invent information.

        ## 2. Task
        Answer the user's query based solely on the provided context. 
        Maintain a polite and helpful tone. If the user's query is a follow-up, 
        use the chat history to inform your response while still grounding it in the provided context.

        ## 3. Context
        The following information was retrieved from our knowledge base of video transcripts. Use this information to formulate your answer.
        ---
        ### Retrieved Context:
        Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: [Music] Uh hello everyone. Thank you so much for being here for uh sticking around for this talk. Um I'm going to be talking about optimizing inference for voice models in production. I'm going to be talking mostly about the runtime component but also just a little bit on the infrastructure side. Um just a quick introduction. Um I'm Philip from B 10. Base 10 is a model inference platform. Uh we run production workloads for a wide variety of AI native startups and enterprises. Um I'm based in here in SF. Um I actually just moved here. It's really awesome. My favorite part about being in SF is much better sports teams than I had in Chicago. Um and uh one of my favorite voice models is Orpheus TTS. Um which we're going to be talking about a whole bunch today. Um quick agenda. So we're going to talk about TTS model architecture like what is a text to speech model actually when you look on the config in hugging face. Uh what sort of performance metrics are we looking at? What sort of optimization techniques can we do to make the model better? Um how do we measure whether or not we succeeded? And then finally, what can we do on the infrastructure and client code to not shoot ourselves in the foot after doing a ton of runtime work and then just adding all that latency back by not doing our client code correctly. So architecture, this is one of the things I've been learning this year which has been pretty great uh to to realize it's made life a lot simpler at the runtime level. Now this is wrong like the the the thing up here uh that that I'm going to say is that like everything is an LLM. uh that is that is wrong but it's useful. Um there's kind of like two types of models. There's autogressive transformers models that are LLM or very LLM adjacent. Um you see this in embeddings. You see this in transcription with stuff like whisper TTS um is another example. You also have the more like diffuser image type models. Um which is like a very different optimization problem. But something that's cool is because TTS models are so architecturally similar to LLMs or in many cases derived directly from LLMs, we can access the rich ecosystem of LLM tooling and use it to make TTS models better. So the TTS model that we're going to be using and as an example all day um is Orpheus TTS. We're using it for two reasons. 
Relevance score: 0.105

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: So if you're using this in production, you want a code sample. By the way, this is all up on our GitHub. Um you'll want a code sample that looks a lot more like a benchmarking script where you're using a um multipprocess pool. um you're sharing the you're sharing the session between all of these different requests and you're actually sending traffic with the concurrency that allows you to, you know, saturate this benchmark with, you know, the the multiple concurrent requests. Finally, both of these code samples, they do sit on top of HTTP and HTTP streaming. Um, in many cases, if you're implementing voice pipelines, you're going to use something like LiveKit or Pipecat or something. And you're also potentially going to be using a different protocol. You're going to be using something like websockets or gRPC, which we do have support for. Um, and finally, I wanted to leave you on the thought that these uh, you know, these models are only one part of a voice agent pipeline. So like we can spend a lot more than 15 minutes actually talking about like the very detailed uh implementation mechanics of making your voice model faster of you know we we haven't even touched on stuff like fine-tuning the model um you know custom voices zeroot voice cloning um being able to you know remove static and popping at the end of messages there's there's there's a lot of work to do just on the voice part but it really only is onethird of the problem when I think about voice agents. I think about three parts, listening, thinking, talking. Um, and the most important thing here is again, while you can have great run times, the infrastructure to connect these three together is really what's going to determine your latency. Being able to go from one model to have the next one running in the same data center with, you know, minimal like minimal network overhead in between the two. uh even things as simple as not having to go off and do a hair pin um at the DNS level and come back. If that saves you 10 milliseconds on every step and your voice pipeline has this and you know a chunking algorithm, it's got an interruption model and so you end up having four or five steps. Well, there just hair pinning alone is costing you 40 or 50 milliseconds and that can be 10% of your SLA for for a voice model. So yeah, uh that's that's my that's my main point here is that as much fun as it is to talk about the runtime stuff and as much work as we do there, the the infrastructure and the client implementation is equally important if not more so. Anyway, thank uh so yeah, that's the that's the review. Um thank you all for coming through. Uh I have uh we're doing an event next week at uh Fogo to Chiao which is going to be pretty fun. I'm going to be talking in more detail about building some uh systems with open source models and there's also going to be a lot of stake. So definitely come on through uh if you're interested and um I'm on I'm on Twitter, I'm on LinkedIn, so it's base 10. Um hit me up if you have any questions about this or anything else model performance. 
Relevance score: 0.098

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: I mean they did a bunch of things to make it work but a couple things that are relevant here. There is a larger vocab size because you need all the speech specific tokens like laugh and stuff. Um and then they also extended the context length for thrope scaling. So we got to make sure everything we do supports that. So performance metrics like what do we want to actually do here? Uh we we think about LLM metrics um a little bit here. Uh we we just look at them a little bit differently. So in LLMs you talk about time to first token. Now we're talking about time to first bite or sometimes even time to first sentence. Uh we we need a little bit more of a useful output um from the model before we really start feeling good about our response time. We do think about tokens per second although we're going to think about it differently which I'll explain later. And we mostly think about throughput which is you know how many requests are we able to serve at a given time. So on that, you know, goals perspective, if you ask me like, "Hey, Philip, how do you want to optimize llama in general?" I'll like I'll say, "Well, we want a lot of TPS. We want hundred. We want 500 TPS. We want a thousand tokens per second. We want as many tokens per second as we can get." With voice models, you actually don't necessarily need that. In many cases, you only want as many tokens per second as you need for a real-time stream. for Orpheus that's like 83 tokens per second which for like a three billion parameter LLM is nothing. Um but what we actually want to do instead is we want to once we hit that mark start optimizing for time to first bite so that our latency is really good and start optimizing for concurrency so that we can get more connections and spend less on GPUs. So our goal in general if all of these very nice and definitely not AI generated people are all the different like voices that our that our model is capable of creating these are all the voice agents that we're running. How can we make all of these people fit on one or even less than one GPU? That's the goal. 
Relevance score: 0.478

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: Okay, three reasons. The two reasons are because it's open source and it's really good. And also I think uh Elias and Amu and everyone at Canopy Labs is really awesome. So uh that's the that's the third reason we're we're talking about their model. But it's a Llama 3.2 3B backbone. So like if you look at this is the little like config from hugging face copy and pasted onto the screen. It's a llama for causal LM architecture. And so because of that we can do like all of our normal llama stuff to this model and make it faster. Um they did a couple things. 
Relevance score: 0.247
        ---

        ## 4. Input Data
        User Query:
        how to optimize inference for voice models in production
        ---

        ## 5. Output Format
        Generate a natural language response to the user's query. Ensure your response is directly supported by the "Retrieved Context" section.


----------------------------------------------------------------
-------------------- New Query at 2025-09-01T03:42:20.880834 -------------

        ## 1. Role
        You are the Chatbot Agent, a conversational AI designed to answer user questions about AI agents, RAG, context engineering, and multi-agent systems. 
        Your responses must be friendly, concise, and professional. 
        Your primary function is to synthesize information from the provided context to answer the user's query. 
        It is critical that you *only* use the information in the provided context. 
        If the context does not contain a confident answer to the user's question, 
        you *must* respond with "I am sorry, but the provided information does not contain a confident answer to your question." 
        You must never make up or invent information.

        ## 2. Task
        Answer the user's query based solely on the provided context. 
        Maintain a polite and helpful tone. If the user's query is a follow-up, 
        use the chat history to inform your response while still grounding it in the provided context.

        ## 3. Context
        The following information was retrieved from our knowledge base of video transcripts. Use this information to formulate your answer.
        ---
        ### Retrieved Context:
        Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: So if you're using this in production, you want a code sample. By the way, this is all up on our GitHub. Um you'll want a code sample that looks a lot more like a benchmarking script where you're using a um multipprocess pool. um you're sharing the you're sharing the session between all of these different requests and you're actually sending traffic with the concurrency that allows you to, you know, saturate this benchmark with, you know, the the multiple concurrent requests. Finally, both of these code samples, they do sit on top of HTTP and HTTP streaming. Um, in many cases, if you're implementing voice pipelines, you're going to use something like LiveKit or Pipecat or something. And you're also potentially going to be using a different protocol. You're going to be using something like websockets or gRPC, which we do have support for. Um, and finally, I wanted to leave you on the thought that these uh, you know, these models are only one part of a voice agent pipeline. So like we can spend a lot more than 15 minutes actually talking about like the very detailed uh implementation mechanics of making your voice model faster of you know we we haven't even touched on stuff like fine-tuning the model um you know custom voices zeroot voice cloning um being able to you know remove static and popping at the end of messages there's there's there's a lot of work to do just on the voice part but it really only is onethird of the problem when I think about voice agents. I think about three parts, listening, thinking, talking. Um, and the most important thing here is again, while you can have great run times, the infrastructure to connect these three together is really what's going to determine your latency. Being able to go from one model to have the next one running in the same data center with, you know, minimal like minimal network overhead in between the two. uh even things as simple as not having to go off and do a hair pin um at the DNS level and come back. If that saves you 10 milliseconds on every step and your voice pipeline has this and you know a chunking algorithm, it's got an interruption model and so you end up having four or five steps. Well, there just hair pinning alone is costing you 40 or 50 milliseconds and that can be 10% of your SLA for for a voice model. So yeah, uh that's that's my that's my main point here is that as much fun as it is to talk about the runtime stuff and as much work as we do there, the the infrastructure and the client implementation is equally important if not more so. Anyway, thank uh so yeah, that's the that's the review. Um thank you all for coming through. Uh I have uh we're doing an event next week at uh Fogo to Chiao which is going to be pretty fun. I'm going to be talking in more detail about building some uh systems with open source models and there's also going to be a lot of stake. So definitely come on through uh if you're interested and um I'm on I'm on Twitter, I'm on LinkedIn, so it's base 10. Um hit me up if you have any questions about this or anything else model performance. 
Relevance score: 0.293

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: I mean they did a bunch of things to make it work but a couple things that are relevant here. There is a larger vocab size because you need all the speech specific tokens like laugh and stuff. Um and then they also extended the context length for thrope scaling. So we got to make sure everything we do supports that. So performance metrics like what do we want to actually do here? Uh we we think about LLM metrics um a little bit here. Uh we we just look at them a little bit differently. So in LLMs you talk about time to first token. Now we're talking about time to first bite or sometimes even time to first sentence. Uh we we need a little bit more of a useful output um from the model before we really start feeling good about our response time. We do think about tokens per second although we're going to think about it differently which I'll explain later. And we mostly think about throughput which is you know how many requests are we able to serve at a given time. So on that, you know, goals perspective, if you ask me like, "Hey, Philip, how do you want to optimize llama in general?" I'll like I'll say, "Well, we want a lot of TPS. We want hundred. We want 500 TPS. We want a thousand tokens per second. We want as many tokens per second as we can get." With voice models, you actually don't necessarily need that. In many cases, you only want as many tokens per second as you need for a real-time stream. for Orpheus that's like 83 tokens per second which for like a three billion parameter LLM is nothing. Um but what we actually want to do instead is we want to once we hit that mark start optimizing for time to first bite so that our latency is really good and start optimizing for concurrency so that we can get more connections and spend less on GPUs. So our goal in general if all of these very nice and definitely not AI generated people are all the different like voices that our that our model is capable of creating these are all the voice agents that we're running. How can we make all of these people fit on one or even less than one GPU? That's the goal. 
Relevance score: 0.225

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: [Music] Uh hello everyone. Thank you so much for being here for uh sticking around for this talk. Um I'm going to be talking about optimizing inference for voice models in production. I'm going to be talking mostly about the runtime component but also just a little bit on the infrastructure side. Um just a quick introduction. Um I'm Philip from B 10. Base 10 is a model inference platform. Uh we run production workloads for a wide variety of AI native startups and enterprises. Um I'm based in here in SF. Um I actually just moved here. It's really awesome. My favorite part about being in SF is much better sports teams than I had in Chicago. Um and uh one of my favorite voice models is Orpheus TTS. Um which we're going to be talking about a whole bunch today. Um quick agenda. So we're going to talk about TTS model architecture like what is a text to speech model actually when you look on the config in hugging face. Uh what sort of performance metrics are we looking at? What sort of optimization techniques can we do to make the model better? Um how do we measure whether or not we succeeded? And then finally, what can we do on the infrastructure and client code to not shoot ourselves in the foot after doing a ton of runtime work and then just adding all that latency back by not doing our client code correctly. So architecture, this is one of the things I've been learning this year which has been pretty great uh to to realize it's made life a lot simpler at the runtime level. Now this is wrong like the the the thing up here uh that that I'm going to say is that like everything is an LLM. uh that is that is wrong but it's useful. Um there's kind of like two types of models. There's autogressive transformers models that are LLM or very LLM adjacent. Um you see this in embeddings. You see this in transcription with stuff like whisper TTS um is another example. You also have the more like diffuser image type models. Um which is like a very different optimization problem. But something that's cool is because TTS models are so architecturally similar to LLMs or in many cases derived directly from LLMs, we can access the rich ecosystem of LLM tooling and use it to make TTS models better. So the TTS model that we're going to be using and as an example all day um is Orpheus TTS. We're using it for two reasons. 
Relevance score: 0.676

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: Okay, three reasons. The two reasons are because it's open source and it's really good. And also I think uh Elias and Amu and everyone at Canopy Labs is really awesome. So uh that's the that's the third reason we're we're talking about their model. But it's a Llama 3.2 3B backbone. So like if you look at this is the little like config from hugging face copy and pasted onto the screen. It's a llama for causal LM architecture. And so because of that we can do like all of our normal llama stuff to this model and make it faster. Um they did a couple things. 
Relevance score: 0.171
        ---

        ## 4. Input Data
        User Query:
        how to optimize inference for voice models in production
        ---

        ## 5. Output Format
        Generate a natural language response to the user's query. Ensure your response is directly supported by the "Retrieved Context" section.


----------------------------------------------------------------
-------------------- New Query at 2025-09-01T03:43:36.669369 -------------

        ## 1. Role
        You are the Chatbot Agent, a conversational AI designed to answer user questions about AI agents, RAG, context engineering, and multi-agent systems. 
        Your responses must be friendly, concise, and professional. 
        Your primary function is to synthesize information from the provided context to answer the user's query. 
        It is critical that you *only* use the information in the provided context. 
        If the context does not contain a confident answer to the user's question, 
        you *must* respond with "I am sorry, but the provided information does not contain a confident answer to your question." 
        You must never make up or invent information.

        ## 2. Task
        Answer the user's query based solely on the provided context. 
        Maintain a polite and helpful tone. If the user's query is a follow-up, 
        use the chat history to inform your response while still grounding it in the provided context.

        ## 3. Context
        The following information was retrieved from our knowledge base of video transcripts. Use this information to formulate your answer.
        ---
        ### Retrieved Context:
        Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: So if you're using this in production, you want a code sample. By the way, this is all up on our GitHub. Um you'll want a code sample that looks a lot more like a benchmarking script where you're using a um multipprocess pool. um you're sharing the you're sharing the session between all of these different requests and you're actually sending traffic with the concurrency that allows you to, you know, saturate this benchmark with, you know, the the multiple concurrent requests. Finally, both of these code samples, they do sit on top of HTTP and HTTP streaming. Um, in many cases, if you're implementing voice pipelines, you're going to use something like LiveKit or Pipecat or something. And you're also potentially going to be using a different protocol. You're going to be using something like websockets or gRPC, which we do have support for. Um, and finally, I wanted to leave you on the thought that these uh, you know, these models are only one part of a voice agent pipeline. So like we can spend a lot more than 15 minutes actually talking about like the very detailed uh implementation mechanics of making your voice model faster of you know we we haven't even touched on stuff like fine-tuning the model um you know custom voices zeroot voice cloning um being able to you know remove static and popping at the end of messages there's there's there's a lot of work to do just on the voice part but it really only is onethird of the problem when I think about voice agents. I think about three parts, listening, thinking, talking. Um, and the most important thing here is again, while you can have great run times, the infrastructure to connect these three together is really what's going to determine your latency. Being able to go from one model to have the next one running in the same data center with, you know, minimal like minimal network overhead in between the two. uh even things as simple as not having to go off and do a hair pin um at the DNS level and come back. If that saves you 10 milliseconds on every step and your voice pipeline has this and you know a chunking algorithm, it's got an interruption model and so you end up having four or five steps. Well, there just hair pinning alone is costing you 40 or 50 milliseconds and that can be 10% of your SLA for for a voice model. So yeah, uh that's that's my that's my main point here is that as much fun as it is to talk about the runtime stuff and as much work as we do there, the the infrastructure and the client implementation is equally important if not more so. Anyway, thank uh so yeah, that's the that's the review. Um thank you all for coming through. Uh I have uh we're doing an event next week at uh Fogo to Chiao which is going to be pretty fun. I'm going to be talking in more detail about building some uh systems with open source models and there's also going to be a lot of stake. So definitely come on through uh if you're interested and um I'm on I'm on Twitter, I'm on LinkedIn, so it's base 10. Um hit me up if you have any questions about this or anything else model performance. 
Relevance score: 0.090

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: [Music] Uh hello everyone. Thank you so much for being here for uh sticking around for this talk. Um I'm going to be talking about optimizing inference for voice models in production. I'm going to be talking mostly about the runtime component but also just a little bit on the infrastructure side. Um just a quick introduction. Um I'm Philip from B 10. Base 10 is a model inference platform. Uh we run production workloads for a wide variety of AI native startups and enterprises. Um I'm based in here in SF. Um I actually just moved here. It's really awesome. My favorite part about being in SF is much better sports teams than I had in Chicago. Um and uh one of my favorite voice models is Orpheus TTS. Um which we're going to be talking about a whole bunch today. Um quick agenda. So we're going to talk about TTS model architecture like what is a text to speech model actually when you look on the config in hugging face. Uh what sort of performance metrics are we looking at? What sort of optimization techniques can we do to make the model better? Um how do we measure whether or not we succeeded? And then finally, what can we do on the infrastructure and client code to not shoot ourselves in the foot after doing a ton of runtime work and then just adding all that latency back by not doing our client code correctly. So architecture, this is one of the things I've been learning this year which has been pretty great uh to to realize it's made life a lot simpler at the runtime level. Now this is wrong like the the the thing up here uh that that I'm going to say is that like everything is an LLM. uh that is that is wrong but it's useful. Um there's kind of like two types of models. There's autogressive transformers models that are LLM or very LLM adjacent. Um you see this in embeddings. You see this in transcription with stuff like whisper TTS um is another example. You also have the more like diffuser image type models. Um which is like a very different optimization problem. But something that's cool is because TTS models are so architecturally similar to LLMs or in many cases derived directly from LLMs, we can access the rich ecosystem of LLM tooling and use it to make TTS models better. So the TTS model that we're going to be using and as an example all day um is Orpheus TTS. We're using it for two reasons. 
Relevance score: 0.068

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: I mean they did a bunch of things to make it work but a couple things that are relevant here. There is a larger vocab size because you need all the speech specific tokens like laugh and stuff. Um and then they also extended the context length for thrope scaling. So we got to make sure everything we do supports that. So performance metrics like what do we want to actually do here? Uh we we think about LLM metrics um a little bit here. Uh we we just look at them a little bit differently. So in LLMs you talk about time to first token. Now we're talking about time to first bite or sometimes even time to first sentence. Uh we we need a little bit more of a useful output um from the model before we really start feeling good about our response time. We do think about tokens per second although we're going to think about it differently which I'll explain later. And we mostly think about throughput which is you know how many requests are we able to serve at a given time. So on that, you know, goals perspective, if you ask me like, "Hey, Philip, how do you want to optimize llama in general?" I'll like I'll say, "Well, we want a lot of TPS. We want hundred. We want 500 TPS. We want a thousand tokens per second. We want as many tokens per second as we can get." With voice models, you actually don't necessarily need that. In many cases, you only want as many tokens per second as you need for a real-time stream. for Orpheus that's like 83 tokens per second which for like a three billion parameter LLM is nothing. Um but what we actually want to do instead is we want to once we hit that mark start optimizing for time to first bite so that our latency is really good and start optimizing for concurrency so that we can get more connections and spend less on GPUs. So our goal in general if all of these very nice and definitely not AI generated people are all the different like voices that our that our model is capable of creating these are all the voice agents that we're running. How can we make all of these people fit on one or even less than one GPU? That's the goal. 
Relevance score: 0.478

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: Okay, three reasons. The two reasons are because it's open source and it's really good. And also I think uh Elias and Amu and everyone at Canopy Labs is really awesome. So uh that's the that's the third reason we're we're talking about their model. But it's a Llama 3.2 3B backbone. So like if you look at this is the little like config from hugging face copy and pasted onto the screen. It's a llama for causal LM architecture. And so because of that we can do like all of our normal llama stuff to this model and make it faster. Um they did a couple things. 
Relevance score: 0.183
        ---

        ## 4. Input Data
        User Query:
        how to optimize inference for voice models in production
        ---

        ## 5. Output Format
        Generate a natural language response to the user's query. Ensure your response is directly supported by the "Retrieved Context" section.


----------------------------------------------------------------
-------------------- New Query at 2025-09-01T03:45:12.159023 -------------

        ## 1. Role
        You are the Chatbot Agent, a conversational AI designed to answer user questions about AI agents, RAG, context engineering, and multi-agent systems. 
        Your responses must be friendly, concise, and professional. 
        Your primary function is to synthesize information from the provided context to answer the user's query. 
        It is critical that you *only* use the information in the provided context. 
        If the context does not contain a confident answer to the user's question, 
        you *must* respond with "I am sorry, but the provided information does not contain a confident answer to your question." 
        You must never make up or invent information.

        ## 2. Task
        Answer the user's query based solely on the provided context. 
        Maintain a polite and helpful tone. If the user's query is a follow-up, 
        use the chat history to inform your response while still grounding it in the provided context.

        ## 3. Context
        The following information was retrieved from our knowledge base of video transcripts. Use this information to formulate your answer.
        ---
        ### Retrieved Context:
        Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: [Music] Uh hello everyone. Thank you so much for being here for uh sticking around for this talk. Um I'm going to be talking about optimizing inference for voice models in production. I'm going to be talking mostly about the runtime component but also just a little bit on the infrastructure side. Um just a quick introduction. Um I'm Philip from B 10. Base 10 is a model inference platform. Uh we run production workloads for a wide variety of AI native startups and enterprises. Um I'm based in here in SF. Um I actually just moved here. It's really awesome. My favorite part about being in SF is much better sports teams than I had in Chicago. Um and uh one of my favorite voice models is Orpheus TTS. Um which we're going to be talking about a whole bunch today. Um quick agenda. So we're going to talk about TTS model architecture like what is a text to speech model actually when you look on the config in hugging face. Uh what sort of performance metrics are we looking at? What sort of optimization techniques can we do to make the model better? Um how do we measure whether or not we succeeded? And then finally, what can we do on the infrastructure and client code to not shoot ourselves in the foot after doing a ton of runtime work and then just adding all that latency back by not doing our client code correctly. So architecture, this is one of the things I've been learning this year which has been pretty great uh to to realize it's made life a lot simpler at the runtime level. Now this is wrong like the the the thing up here uh that that I'm going to say is that like everything is an LLM. uh that is that is wrong but it's useful. Um there's kind of like two types of models. There's autogressive transformers models that are LLM or very LLM adjacent. Um you see this in embeddings. You see this in transcription with stuff like whisper TTS um is another example. You also have the more like diffuser image type models. Um which is like a very different optimization problem. But something that's cool is because TTS models are so architecturally similar to LLMs or in many cases derived directly from LLMs, we can access the rich ecosystem of LLM tooling and use it to make TTS models better. So the TTS model that we're going to be using and as an example all day um is Orpheus TTS. We're using it for two reasons. 
Relevance score: 0.676

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: I mean they did a bunch of things to make it work but a couple things that are relevant here. There is a larger vocab size because you need all the speech specific tokens like laugh and stuff. Um and then they also extended the context length for thrope scaling. So we got to make sure everything we do supports that. So performance metrics like what do we want to actually do here? Uh we we think about LLM metrics um a little bit here. Uh we we just look at them a little bit differently. So in LLMs you talk about time to first token. Now we're talking about time to first bite or sometimes even time to first sentence. Uh we we need a little bit more of a useful output um from the model before we really start feeling good about our response time. We do think about tokens per second although we're going to think about it differently which I'll explain later. And we mostly think about throughput which is you know how many requests are we able to serve at a given time. So on that, you know, goals perspective, if you ask me like, "Hey, Philip, how do you want to optimize llama in general?" I'll like I'll say, "Well, we want a lot of TPS. We want hundred. We want 500 TPS. We want a thousand tokens per second. We want as many tokens per second as we can get." With voice models, you actually don't necessarily need that. In many cases, you only want as many tokens per second as you need for a real-time stream. for Orpheus that's like 83 tokens per second which for like a three billion parameter LLM is nothing. Um but what we actually want to do instead is we want to once we hit that mark start optimizing for time to first bite so that our latency is really good and start optimizing for concurrency so that we can get more connections and spend less on GPUs. So our goal in general if all of these very nice and definitely not AI generated people are all the different like voices that our that our model is capable of creating these are all the voice agents that we're running. How can we make all of these people fit on one or even less than one GPU? That's the goal. 
Relevance score: 0.478

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: So if you're using this in production, you want a code sample. By the way, this is all up on our GitHub. Um you'll want a code sample that looks a lot more like a benchmarking script where you're using a um multipprocess pool. um you're sharing the you're sharing the session between all of these different requests and you're actually sending traffic with the concurrency that allows you to, you know, saturate this benchmark with, you know, the the multiple concurrent requests. Finally, both of these code samples, they do sit on top of HTTP and HTTP streaming. Um, in many cases, if you're implementing voice pipelines, you're going to use something like LiveKit or Pipecat or something. And you're also potentially going to be using a different protocol. You're going to be using something like websockets or gRPC, which we do have support for. Um, and finally, I wanted to leave you on the thought that these uh, you know, these models are only one part of a voice agent pipeline. So like we can spend a lot more than 15 minutes actually talking about like the very detailed uh implementation mechanics of making your voice model faster of you know we we haven't even touched on stuff like fine-tuning the model um you know custom voices zeroot voice cloning um being able to you know remove static and popping at the end of messages there's there's there's a lot of work to do just on the voice part but it really only is onethird of the problem when I think about voice agents. I think about three parts, listening, thinking, talking. Um, and the most important thing here is again, while you can have great run times, the infrastructure to connect these three together is really what's going to determine your latency. Being able to go from one model to have the next one running in the same data center with, you know, minimal like minimal network overhead in between the two. uh even things as simple as not having to go off and do a hair pin um at the DNS level and come back. If that saves you 10 milliseconds on every step and your voice pipeline has this and you know a chunking algorithm, it's got an interruption model and so you end up having four or five steps. Well, there just hair pinning alone is costing you 40 or 50 milliseconds and that can be 10% of your SLA for for a voice model. So yeah, uh that's that's my that's my main point here is that as much fun as it is to talk about the runtime stuff and as much work as we do there, the the infrastructure and the client implementation is equally important if not more so. Anyway, thank uh so yeah, that's the that's the review. Um thank you all for coming through. Uh I have uh we're doing an event next week at uh Fogo to Chiao which is going to be pretty fun. I'm going to be talking in more detail about building some uh systems with open source models and there's also going to be a lot of stake. So definitely come on through uh if you're interested and um I'm on I'm on Twitter, I'm on LinkedIn, so it's base 10. Um hit me up if you have any questions about this or anything else model performance. 
Relevance score: 0.220

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: Okay, three reasons. The two reasons are because it's open source and it's really good. And also I think uh Elias and Amu and everyone at Canopy Labs is really awesome. So uh that's the that's the third reason we're we're talking about their model. But it's a Llama 3.2 3B backbone. So like if you look at this is the little like config from hugging face copy and pasted onto the screen. It's a llama for causal LM architecture. And so because of that we can do like all of our normal llama stuff to this model and make it faster. Um they did a couple things. 
Relevance score: 0.159
        ---

        ## 4. Input Data
        User Query:
        how to optimize inference for voice models in production
        ---

        ## 5. Output Format
        Generate a natural language response to the user's query. Ensure your response is directly supported by the "Retrieved Context" section.


----------------------------------------------------------------
-------------------- New Query at 2025-09-01T03:48:18.208835 -------------

        ## 1. Role
        You are the Chatbot Agent, a conversational AI designed to answer user questions about AI agents, RAG, context engineering, and multi-agent systems. 
        Your responses must be friendly, concise, and professional. 
        Your primary function is to synthesize information from the provided context to answer the user's query. 
        It is critical that you *only* use the information in the provided context. 
        If the context does not contain a confident answer to the user's question, 
        you *must* respond with "I am sorry, but the provided information does not contain a confident answer to your question." 
        You must never make up or invent information.

        ## 2. Task
        Answer the user's query based solely on the provided context. 
        Maintain a polite and helpful tone. If the user's query is a follow-up, 
        use the chat history to inform your response while still grounding it in the provided context.

        ## 3. Context
        The following information was retrieved from our knowledge base of video transcripts. Use this information to formulate your answer.
        ---
        ### Retrieved Context:
        Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: [Music] Uh hello everyone. Thank you so much for being here for uh sticking around for this talk. Um I'm going to be talking about optimizing inference for voice models in production. I'm going to be talking mostly about the runtime component but also just a little bit on the infrastructure side. Um just a quick introduction. Um I'm Philip from B 10. Base 10 is a model inference platform. Uh we run production workloads for a wide variety of AI native startups and enterprises. Um I'm based in here in SF. Um I actually just moved here. It's really awesome. My favorite part about being in SF is much better sports teams than I had in Chicago. Um and uh one of my favorite voice models is Orpheus TTS. Um which we're going to be talking about a whole bunch today. Um quick agenda. So we're going to talk about TTS model architecture like what is a text to speech model actually when you look on the config in hugging face. Uh what sort of performance metrics are we looking at? What sort of optimization techniques can we do to make the model better? Um how do we measure whether or not we succeeded? And then finally, what can we do on the infrastructure and client code to not shoot ourselves in the foot after doing a ton of runtime work and then just adding all that latency back by not doing our client code correctly. So architecture, this is one of the things I've been learning this year which has been pretty great uh to to realize it's made life a lot simpler at the runtime level. Now this is wrong like the the the thing up here uh that that I'm going to say is that like everything is an LLM. uh that is that is wrong but it's useful. Um there's kind of like two types of models. There's autogressive transformers models that are LLM or very LLM adjacent. Um you see this in embeddings. You see this in transcription with stuff like whisper TTS um is another example. You also have the more like diffuser image type models. Um which is like a very different optimization problem. But something that's cool is because TTS models are so architecturally similar to LLMs or in many cases derived directly from LLMs, we can access the rich ecosystem of LLM tooling and use it to make TTS models better. So the TTS model that we're going to be using and as an example all day um is Orpheus TTS. We're using it for two reasons. 
Relevance score: 0.147

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: I mean they did a bunch of things to make it work but a couple things that are relevant here. There is a larger vocab size because you need all the speech specific tokens like laugh and stuff. Um and then they also extended the context length for thrope scaling. So we got to make sure everything we do supports that. So performance metrics like what do we want to actually do here? Uh we we think about LLM metrics um a little bit here. Uh we we just look at them a little bit differently. So in LLMs you talk about time to first token. Now we're talking about time to first bite or sometimes even time to first sentence. Uh we we need a little bit more of a useful output um from the model before we really start feeling good about our response time. We do think about tokens per second although we're going to think about it differently which I'll explain later. And we mostly think about throughput which is you know how many requests are we able to serve at a given time. So on that, you know, goals perspective, if you ask me like, "Hey, Philip, how do you want to optimize llama in general?" I'll like I'll say, "Well, we want a lot of TPS. We want hundred. We want 500 TPS. We want a thousand tokens per second. We want as many tokens per second as we can get." With voice models, you actually don't necessarily need that. In many cases, you only want as many tokens per second as you need for a real-time stream. for Orpheus that's like 83 tokens per second which for like a three billion parameter LLM is nothing. Um but what we actually want to do instead is we want to once we hit that mark start optimizing for time to first bite so that our latency is really good and start optimizing for concurrency so that we can get more connections and spend less on GPUs. So our goal in general if all of these very nice and definitely not AI generated people are all the different like voices that our that our model is capable of creating these are all the voice agents that we're running. How can we make all of these people fit on one or even less than one GPU? That's the goal. 
Relevance score: 0.137

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: Okay, three reasons. The two reasons are because it's open source and it's really good. And also I think uh Elias and Amu and everyone at Canopy Labs is really awesome. So uh that's the that's the third reason we're we're talking about their model. But it's a Llama 3.2 3B backbone. So like if you look at this is the little like config from hugging face copy and pasted onto the screen. It's a llama for causal LM architecture. And so because of that we can do like all of our normal llama stuff to this model and make it faster. Um they did a couple things. 
Relevance score: 0.250

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: So if you're using this in production, you want a code sample. By the way, this is all up on our GitHub. Um you'll want a code sample that looks a lot more like a benchmarking script where you're using a um multipprocess pool. um you're sharing the you're sharing the session between all of these different requests and you're actually sending traffic with the concurrency that allows you to, you know, saturate this benchmark with, you know, the the multiple concurrent requests. Finally, both of these code samples, they do sit on top of HTTP and HTTP streaming. Um, in many cases, if you're implementing voice pipelines, you're going to use something like LiveKit or Pipecat or something. And you're also potentially going to be using a different protocol. You're going to be using something like websockets or gRPC, which we do have support for. Um, and finally, I wanted to leave you on the thought that these uh, you know, these models are only one part of a voice agent pipeline. So like we can spend a lot more than 15 minutes actually talking about like the very detailed uh implementation mechanics of making your voice model faster of you know we we haven't even touched on stuff like fine-tuning the model um you know custom voices zeroot voice cloning um being able to you know remove static and popping at the end of messages there's there's there's a lot of work to do just on the voice part but it really only is onethird of the problem when I think about voice agents. I think about three parts, listening, thinking, talking. Um, and the most important thing here is again, while you can have great run times, the infrastructure to connect these three together is really what's going to determine your latency. Being able to go from one model to have the next one running in the same data center with, you know, minimal like minimal network overhead in between the two. uh even things as simple as not having to go off and do a hair pin um at the DNS level and come back. If that saves you 10 milliseconds on every step and your voice pipeline has this and you know a chunking algorithm, it's got an interruption model and so you end up having four or five steps. Well, there just hair pinning alone is costing you 40 or 50 milliseconds and that can be 10% of your SLA for for a voice model. So yeah, uh that's that's my that's my main point here is that as much fun as it is to talk about the runtime stuff and as much work as we do there, the the infrastructure and the client implementation is equally important if not more so. Anyway, thank uh so yeah, that's the that's the review. Um thank you all for coming through. Uh I have uh we're doing an event next week at uh Fogo to Chiao which is going to be pretty fun. I'm going to be talking in more detail about building some uh systems with open source models and there's also going to be a lot of stake. So definitely come on through uh if you're interested and um I'm on I'm on Twitter, I'm on LinkedIn, so it's base 10. Um hit me up if you have any questions about this or anything else model performance. 
Relevance score: 0.232
        ---

        ## 4. Input Data
        User Query:
        how to optimize inference for voice models in production
        ---

        ## 5. Output Format
        Generate a natural language response to the user's query. Ensure your response is directly supported by the "Retrieved Context" section.


----------------------------------------------------------------
-------------------- New Query at 2025-09-01T03:48:50.143002 -------------

        ## 1. Role
        You are the Chatbot Agent, a conversational AI designed to answer user questions about AI agents, RAG, context engineering, and multi-agent systems. 
        Your responses must be friendly, concise, and professional. 
        Your primary function is to synthesize information from the provided context to answer the user's query. 
        It is critical that you *only* use the information in the provided context. 
        If the context does not contain a confident answer to the user's question, 
        you *must* respond with "I am sorry, but the provided information does not contain a confident answer to your question." 
        You must never make up or invent information.

        ## 2. Task
        Answer the user's query based solely on the provided context. 
        Maintain a polite and helpful tone. If the user's query is a follow-up, 
        use the chat history to inform your response while still grounding it in the provided context.

        ## 3. Context
        The following information was retrieved from our knowledge base of video transcripts. Use this information to formulate your answer.
        ---
        ### Retrieved Context:
        Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: [Music] Uh hello everyone. Thank you so much for being here for uh sticking around for this talk. Um I'm going to be talking about optimizing inference for voice models in production. I'm going to be talking mostly about the runtime component but also just a little bit on the infrastructure side. Um just a quick introduction. Um I'm Philip from B 10. Base 10 is a model inference platform. Uh we run production workloads for a wide variety of AI native startups and enterprises. Um I'm based in here in SF. Um I actually just moved here. It's really awesome. My favorite part about being in SF is much better sports teams than I had in Chicago. Um and uh one of my favorite voice models is Orpheus TTS. Um which we're going to be talking about a whole bunch today. Um quick agenda. So we're going to talk about TTS model architecture like what is a text to speech model actually when you look on the config in hugging face. Uh what sort of performance metrics are we looking at? What sort of optimization techniques can we do to make the model better? Um how do we measure whether or not we succeeded? And then finally, what can we do on the infrastructure and client code to not shoot ourselves in the foot after doing a ton of runtime work and then just adding all that latency back by not doing our client code correctly. So architecture, this is one of the things I've been learning this year which has been pretty great uh to to realize it's made life a lot simpler at the runtime level. Now this is wrong like the the the thing up here uh that that I'm going to say is that like everything is an LLM. uh that is that is wrong but it's useful. Um there's kind of like two types of models. There's autogressive transformers models that are LLM or very LLM adjacent. Um you see this in embeddings. You see this in transcription with stuff like whisper TTS um is another example. You also have the more like diffuser image type models. Um which is like a very different optimization problem. But something that's cool is because TTS models are so architecturally similar to LLMs or in many cases derived directly from LLMs, we can access the rich ecosystem of LLM tooling and use it to make TTS models better. So the TTS model that we're going to be using and as an example all day um is Orpheus TTS. We're using it for two reasons. 
Relevance score: 0.502

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: I mean they did a bunch of things to make it work but a couple things that are relevant here. There is a larger vocab size because you need all the speech specific tokens like laugh and stuff. Um and then they also extended the context length for thrope scaling. So we got to make sure everything we do supports that. So performance metrics like what do we want to actually do here? Uh we we think about LLM metrics um a little bit here. Uh we we just look at them a little bit differently. So in LLMs you talk about time to first token. Now we're talking about time to first bite or sometimes even time to first sentence. Uh we we need a little bit more of a useful output um from the model before we really start feeling good about our response time. We do think about tokens per second although we're going to think about it differently which I'll explain later. And we mostly think about throughput which is you know how many requests are we able to serve at a given time. So on that, you know, goals perspective, if you ask me like, "Hey, Philip, how do you want to optimize llama in general?" I'll like I'll say, "Well, we want a lot of TPS. We want hundred. We want 500 TPS. We want a thousand tokens per second. We want as many tokens per second as we can get." With voice models, you actually don't necessarily need that. In many cases, you only want as many tokens per second as you need for a real-time stream. for Orpheus that's like 83 tokens per second which for like a three billion parameter LLM is nothing. Um but what we actually want to do instead is we want to once we hit that mark start optimizing for time to first bite so that our latency is really good and start optimizing for concurrency so that we can get more connections and spend less on GPUs. So our goal in general if all of these very nice and definitely not AI generated people are all the different like voices that our that our model is capable of creating these are all the voice agents that we're running. How can we make all of these people fit on one or even less than one GPU? That's the goal. 
Relevance score: 0.437

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: Yes, it's fast. Um so with the you know with the tot implementation on the MIGs and on the H100s we can actually get all the way down to 150 millisecond time to first bite um in like real world uh testing that we've done. Now that we'll we'll talk in a minute like that doesn't mean your whole pipeline is that fast. That's just like one part of the pipeline. Uh but it's it's important because you know you definitely don't want to be spending a lot of time waiting around for that first token. So to kind of transition into that that discussion of like what can go wrong here like you have this graph and you have this you know nice uh config that I had up here and you're like all right cool I'm going to take this I'm going to put it in production and I'm going to see the results that uh that he put up on screen and it's going to work great and the answer is no it's not it's a little bit harder than that. Um, so the thing is like non-runtime factors when we get especially with these small models and with these multimodal systems can actually be like way more important than your runtime. Um, and that's your lat your infrastructure and your client code because you know I I showed here all right maybe maybe I got it you know I I cut the runtime in in half um from from the base implementation I I saved a couple hundred milliseconds very easy to add those couple hundred milliseconds back and well beyond that by you know sending my query to New York instead of California or by having to establish a session every time I uh you know run my client code. Um, so a few like pitfalls to avoid. Um, number one, like if you go in, you know, our our model library or something and we're just trying to get you started very quickly um with this with this kind of inference sample. Um, it's basically going to be, hey, use requests, make a stream, stream it to your local computer and start, you know, playing it on ffmpeg or something. The issue is that here like the requests are going to be sent sequentially and you need to create a new session every time. um that takes time. 
Relevance score: 0.349

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: Okay, three reasons. The two reasons are because it's open source and it's really good. And also I think uh Elias and Amu and everyone at Canopy Labs is really awesome. So uh that's the that's the third reason we're we're talking about their model. But it's a Llama 3.2 3B backbone. So like if you look at this is the little like config from hugging face copy and pasted onto the screen. It's a llama for causal LM architecture. And so because of that we can do like all of our normal llama stuff to this model and make it faster. Um they did a couple things. 
Relevance score: 0.258

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: Uh let's see let's see if it's actually any faster. Um, so again, the number one thing is going to be simultaneous streams because you want to be able to be very costefficient and use fewer GPU resources to serve a, you know, large amount of traffic. And in this case, our base implementation, I don't necessarily want to like call anyone out because there's a lot of really good ways to run this model. Um, you can get really good performance with VLM. Um but this is just kind of like the offtheshelf um just take it run it completely standard uh implementation. So with variable traffic we're able to support 16 simultaneous streams and with constant traffic 24 simultaneous streams on um an H100 MIG. So this is actually half an H100 GPU. It's a skew that we do a lot because it's really good for these small models where you want the Hopper performance, the Hopper architecture uplift in Tensor RTLM, the FP8 support, but you don't want to pay for like an entire 80 gigabyte GPU for just a 3 billion parameter model. So, you know, we're seeing much better um much much better concurrency. So, if you kind of like price that out with like our list prices and stuff, um, you can get, you know, a few cents per hour of of conversation, um, which is going to be, you know, substantially better than if you're if you have the volume for it. It's going to be substantially better than paying for a sort of like pertoken type API. But, okay, sure, maybe it's cheap at scale, but is it fast? 
Relevance score: 0.225
        ---

        ## 4. Input Data
        User Query:
        how to optimize inference for voice models in production
        ---

        ## 5. Output Format
        Generate a natural language response to the user's query. Ensure your response is directly supported by the "Retrieved Context" section.


----------------------------------------------------------------
-------------------- New Query at 2025-09-01T03:50:29.213120 -------------

        ## 1. Role
        You are the Chatbot Agent, a conversational AI designed to answer user questions about AI agents, RAG, context engineering, and multi-agent systems. 
        Your responses must be friendly, concise, and professional. 
        Your primary function is to synthesize information from the provided context to answer the user's query. 
        It is critical that you *only* use the information in the provided context. 
        If the context does not contain a confident answer to the user's question, 
        you *must* respond with "I am sorry, but the provided information does not contain a confident answer to your question." 
        You must never make up or invent information.

        ## 2. Task
        Answer the user's query based solely on the provided context. 
        Maintain a polite and helpful tone. If the user's query is a follow-up, 
        use the chat history to inform your response while still grounding it in the provided context.

        ## 3. Context
        The following information was retrieved from our knowledge base of video transcripts. Use this information to formulate your answer.
        ---
        ### Retrieved Context:
        Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: So if you're using this in production, you want a code sample. By the way, this is all up on our GitHub. Um you'll want a code sample that looks a lot more like a benchmarking script where you're using a um multipprocess pool. um you're sharing the you're sharing the session between all of these different requests and you're actually sending traffic with the concurrency that allows you to, you know, saturate this benchmark with, you know, the the multiple concurrent requests. Finally, both of these code samples, they do sit on top of HTTP and HTTP streaming. Um, in many cases, if you're implementing voice pipelines, you're going to use something like LiveKit or Pipecat or something. And you're also potentially going to be using a different protocol. You're going to be using something like websockets or gRPC, which we do have support for. Um, and finally, I wanted to leave you on the thought that these uh, you know, these models are only one part of a voice agent pipeline. So like we can spend a lot more than 15 minutes actually talking about like the very detailed uh implementation mechanics of making your voice model faster of you know we we haven't even touched on stuff like fine-tuning the model um you know custom voices zeroot voice cloning um being able to you know remove static and popping at the end of messages there's there's there's a lot of work to do just on the voice part but it really only is onethird of the problem when I think about voice agents. I think about three parts, listening, thinking, talking. Um, and the most important thing here is again, while you can have great run times, the infrastructure to connect these three together is really what's going to determine your latency. Being able to go from one model to have the next one running in the same data center with, you know, minimal like minimal network overhead in between the two. uh even things as simple as not having to go off and do a hair pin um at the DNS level and come back. If that saves you 10 milliseconds on every step and your voice pipeline has this and you know a chunking algorithm, it's got an interruption model and so you end up having four or five steps. Well, there just hair pinning alone is costing you 40 or 50 milliseconds and that can be 10% of your SLA for for a voice model. So yeah, uh that's that's my that's my main point here is that as much fun as it is to talk about the runtime stuff and as much work as we do there, the the infrastructure and the client implementation is equally important if not more so. Anyway, thank uh so yeah, that's the that's the review. Um thank you all for coming through. Uh I have uh we're doing an event next week at uh Fogo to Chiao which is going to be pretty fun. I'm going to be talking in more detail about building some uh systems with open source models and there's also going to be a lot of stake. So definitely come on through uh if you're interested and um I'm on I'm on Twitter, I'm on LinkedIn, so it's base 10. Um hit me up if you have any questions about this or anything else model performance. 
Relevance score: 0.301

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: I mean they did a bunch of things to make it work but a couple things that are relevant here. There is a larger vocab size because you need all the speech specific tokens like laugh and stuff. Um and then they also extended the context length for thrope scaling. So we got to make sure everything we do supports that. So performance metrics like what do we want to actually do here? Uh we we think about LLM metrics um a little bit here. Uh we we just look at them a little bit differently. So in LLMs you talk about time to first token. Now we're talking about time to first bite or sometimes even time to first sentence. Uh we we need a little bit more of a useful output um from the model before we really start feeling good about our response time. We do think about tokens per second although we're going to think about it differently which I'll explain later. And we mostly think about throughput which is you know how many requests are we able to serve at a given time. So on that, you know, goals perspective, if you ask me like, "Hey, Philip, how do you want to optimize llama in general?" I'll like I'll say, "Well, we want a lot of TPS. We want hundred. We want 500 TPS. We want a thousand tokens per second. We want as many tokens per second as we can get." With voice models, you actually don't necessarily need that. In many cases, you only want as many tokens per second as you need for a real-time stream. for Orpheus that's like 83 tokens per second which for like a three billion parameter LLM is nothing. Um but what we actually want to do instead is we want to once we hit that mark start optimizing for time to first bite so that our latency is really good and start optimizing for concurrency so that we can get more connections and spend less on GPUs. So our goal in general if all of these very nice and definitely not AI generated people are all the different like voices that our that our model is capable of creating these are all the voice agents that we're running. How can we make all of these people fit on one or even less than one GPU? That's the goal. 
Relevance score: 0.240

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: Okay, three reasons. The two reasons are because it's open source and it's really good. And also I think uh Elias and Amu and everyone at Canopy Labs is really awesome. So uh that's the that's the third reason we're we're talking about their model. But it's a Llama 3.2 3B backbone. So like if you look at this is the little like config from hugging face copy and pasted onto the screen. It's a llama for causal LM architecture. And so because of that we can do like all of our normal llama stuff to this model and make it faster. Um they did a couple things. 
Relevance score: 0.250

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: [Music] Uh hello everyone. Thank you so much for being here for uh sticking around for this talk. Um I'm going to be talking about optimizing inference for voice models in production. I'm going to be talking mostly about the runtime component but also just a little bit on the infrastructure side. Um just a quick introduction. Um I'm Philip from B 10. Base 10 is a model inference platform. Uh we run production workloads for a wide variety of AI native startups and enterprises. Um I'm based in here in SF. Um I actually just moved here. It's really awesome. My favorite part about being in SF is much better sports teams than I had in Chicago. Um and uh one of my favorite voice models is Orpheus TTS. Um which we're going to be talking about a whole bunch today. Um quick agenda. So we're going to talk about TTS model architecture like what is a text to speech model actually when you look on the config in hugging face. Uh what sort of performance metrics are we looking at? What sort of optimization techniques can we do to make the model better? Um how do we measure whether or not we succeeded? And then finally, what can we do on the infrastructure and client code to not shoot ourselves in the foot after doing a ton of runtime work and then just adding all that latency back by not doing our client code correctly. So architecture, this is one of the things I've been learning this year which has been pretty great uh to to realize it's made life a lot simpler at the runtime level. Now this is wrong like the the the thing up here uh that that I'm going to say is that like everything is an LLM. uh that is that is wrong but it's useful. Um there's kind of like two types of models. There's autogressive transformers models that are LLM or very LLM adjacent. Um you see this in embeddings. You see this in transcription with stuff like whisper TTS um is another example. You also have the more like diffuser image type models. Um which is like a very different optimization problem. But something that's cool is because TTS models are so architecturally similar to LLMs or in many cases derived directly from LLMs, we can access the rich ecosystem of LLM tooling and use it to make TTS models better. So the TTS model that we're going to be using and as an example all day um is Orpheus TTS. We're using it for two reasons. 
Relevance score: 0.320
        ---

        ## 4. Input Data
        User Query:
        how to optimize inference for voice models in production
        ---

        ## 5. Output Format
        Generate a natural language response to the user's query. Ensure your response is directly supported by the "Retrieved Context" section.


----------------------------------------------------------------
-------------------- New Query at 2025-09-01T03:51:22.394206 -------------

        ## 1. Role
        You are the Chatbot Agent, a conversational AI designed to answer user questions about AI agents, RAG, context engineering, and multi-agent systems. 
        Your responses must be friendly, concise, and professional. 
        Your primary function is to synthesize information from the provided context to answer the user's query. 
        It is critical that you *only* use the information in the provided context. 
        If the context does not contain a confident answer to the user's question, 
        you *must* respond with "I am sorry, but the provided information does not contain a confident answer to your question." 
        You must never make up or invent information.

        ## 2. Task
        Answer the user's query based solely on the provided context. 
        Maintain a polite and helpful tone. If the user's query is a follow-up, 
        use the chat history to inform your response while still grounding it in the provided context.

        ## 3. Context
        The following information was retrieved from our knowledge base of video transcripts. Use this information to formulate your answer.
        ---
        ### Retrieved Context:
        Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: [Music] Uh hello everyone. Thank you so much for being here for uh sticking around for this talk. Um I'm going to be talking about optimizing inference for voice models in production. I'm going to be talking mostly about the runtime component but also just a little bit on the infrastructure side. Um just a quick introduction. Um I'm Philip from B 10. Base 10 is a model inference platform. Uh we run production workloads for a wide variety of AI native startups and enterprises. Um I'm based in here in SF. Um I actually just moved here. It's really awesome. My favorite part about being in SF is much better sports teams than I had in Chicago. Um and uh one of my favorite voice models is Orpheus TTS. Um which we're going to be talking about a whole bunch today. Um quick agenda. So we're going to talk about TTS model architecture like what is a text to speech model actually when you look on the config in hugging face. Uh what sort of performance metrics are we looking at? What sort of optimization techniques can we do to make the model better? Um how do we measure whether or not we succeeded? And then finally, what can we do on the infrastructure and client code to not shoot ourselves in the foot after doing a ton of runtime work and then just adding all that latency back by not doing our client code correctly. So architecture, this is one of the things I've been learning this year which has been pretty great uh to to realize it's made life a lot simpler at the runtime level. Now this is wrong like the the the thing up here uh that that I'm going to say is that like everything is an LLM. uh that is that is wrong but it's useful. Um there's kind of like two types of models. There's autogressive transformers models that are LLM or very LLM adjacent. Um you see this in embeddings. You see this in transcription with stuff like whisper TTS um is another example. You also have the more like diffuser image type models. Um which is like a very different optimization problem. But something that's cool is because TTS models are so architecturally similar to LLMs or in many cases derived directly from LLMs, we can access the rich ecosystem of LLM tooling and use it to make TTS models better. So the TTS model that we're going to be using and as an example all day um is Orpheus TTS. We're using it for two reasons. 
Relevance score: 0.098

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: So if you're using this in production, you want a code sample. By the way, this is all up on our GitHub. Um you'll want a code sample that looks a lot more like a benchmarking script where you're using a um multipprocess pool. um you're sharing the you're sharing the session between all of these different requests and you're actually sending traffic with the concurrency that allows you to, you know, saturate this benchmark with, you know, the the multiple concurrent requests. Finally, both of these code samples, they do sit on top of HTTP and HTTP streaming. Um, in many cases, if you're implementing voice pipelines, you're going to use something like LiveKit or Pipecat or something. And you're also potentially going to be using a different protocol. You're going to be using something like websockets or gRPC, which we do have support for. Um, and finally, I wanted to leave you on the thought that these uh, you know, these models are only one part of a voice agent pipeline. So like we can spend a lot more than 15 minutes actually talking about like the very detailed uh implementation mechanics of making your voice model faster of you know we we haven't even touched on stuff like fine-tuning the model um you know custom voices zeroot voice cloning um being able to you know remove static and popping at the end of messages there's there's there's a lot of work to do just on the voice part but it really only is onethird of the problem when I think about voice agents. I think about three parts, listening, thinking, talking. Um, and the most important thing here is again, while you can have great run times, the infrastructure to connect these three together is really what's going to determine your latency. Being able to go from one model to have the next one running in the same data center with, you know, minimal like minimal network overhead in between the two. uh even things as simple as not having to go off and do a hair pin um at the DNS level and come back. If that saves you 10 milliseconds on every step and your voice pipeline has this and you know a chunking algorithm, it's got an interruption model and so you end up having four or five steps. Well, there just hair pinning alone is costing you 40 or 50 milliseconds and that can be 10% of your SLA for for a voice model. So yeah, uh that's that's my that's my main point here is that as much fun as it is to talk about the runtime stuff and as much work as we do there, the the infrastructure and the client implementation is equally important if not more so. Anyway, thank uh so yeah, that's the that's the review. Um thank you all for coming through. Uh I have uh we're doing an event next week at uh Fogo to Chiao which is going to be pretty fun. I'm going to be talking in more detail about building some uh systems with open source models and there's also going to be a lot of stake. So definitely come on through uh if you're interested and um I'm on I'm on Twitter, I'm on LinkedIn, so it's base 10. Um hit me up if you have any questions about this or anything else model performance. 
Relevance score: 0.092

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: I mean they did a bunch of things to make it work but a couple things that are relevant here. There is a larger vocab size because you need all the speech specific tokens like laugh and stuff. Um and then they also extended the context length for thrope scaling. So we got to make sure everything we do supports that. So performance metrics like what do we want to actually do here? Uh we we think about LLM metrics um a little bit here. Uh we we just look at them a little bit differently. So in LLMs you talk about time to first token. Now we're talking about time to first bite or sometimes even time to first sentence. Uh we we need a little bit more of a useful output um from the model before we really start feeling good about our response time. We do think about tokens per second although we're going to think about it differently which I'll explain later. And we mostly think about throughput which is you know how many requests are we able to serve at a given time. So on that, you know, goals perspective, if you ask me like, "Hey, Philip, how do you want to optimize llama in general?" I'll like I'll say, "Well, we want a lot of TPS. We want hundred. We want 500 TPS. We want a thousand tokens per second. We want as many tokens per second as we can get." With voice models, you actually don't necessarily need that. In many cases, you only want as many tokens per second as you need for a real-time stream. for Orpheus that's like 83 tokens per second which for like a three billion parameter LLM is nothing. Um but what we actually want to do instead is we want to once we hit that mark start optimizing for time to first bite so that our latency is really good and start optimizing for concurrency so that we can get more connections and spend less on GPUs. So our goal in general if all of these very nice and definitely not AI generated people are all the different like voices that our that our model is capable of creating these are all the voice agents that we're running. How can we make all of these people fit on one or even less than one GPU? That's the goal. 
Relevance score: 0.478

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: Okay, three reasons. The two reasons are because it's open source and it's really good. And also I think uh Elias and Amu and everyone at Canopy Labs is really awesome. So uh that's the that's the third reason we're we're talking about their model. But it's a Llama 3.2 3B backbone. So like if you look at this is the little like config from hugging face copy and pasted onto the screen. It's a llama for causal LM architecture. And so because of that we can do like all of our normal llama stuff to this model and make it faster. Um they did a couple things. 
Relevance score: 0.283

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: Yes, it's fast. Um so with the you know with the tot implementation on the MIGs and on the H100s we can actually get all the way down to 150 millisecond time to first bite um in like real world uh testing that we've done. Now that we'll we'll talk in a minute like that doesn't mean your whole pipeline is that fast. That's just like one part of the pipeline. Uh but it's it's important because you know you definitely don't want to be spending a lot of time waiting around for that first token. So to kind of transition into that that discussion of like what can go wrong here like you have this graph and you have this you know nice uh config that I had up here and you're like all right cool I'm going to take this I'm going to put it in production and I'm going to see the results that uh that he put up on screen and it's going to work great and the answer is no it's not it's a little bit harder than that. Um, so the thing is like non-runtime factors when we get especially with these small models and with these multimodal systems can actually be like way more important than your runtime. Um, and that's your lat your infrastructure and your client code because you know I I showed here all right maybe maybe I got it you know I I cut the runtime in in half um from from the base implementation I I saved a couple hundred milliseconds very easy to add those couple hundred milliseconds back and well beyond that by you know sending my query to New York instead of California or by having to establish a session every time I uh you know run my client code. Um, so a few like pitfalls to avoid. Um, number one, like if you go in, you know, our our model library or something and we're just trying to get you started very quickly um with this with this kind of inference sample. Um, it's basically going to be, hey, use requests, make a stream, stream it to your local computer and start, you know, playing it on ffmpeg or something. The issue is that here like the requests are going to be sent sequentially and you need to create a new session every time. um that takes time. 
Relevance score: 0.181

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: Uh let's see let's see if it's actually any faster. Um, so again, the number one thing is going to be simultaneous streams because you want to be able to be very costefficient and use fewer GPU resources to serve a, you know, large amount of traffic. And in this case, our base implementation, I don't necessarily want to like call anyone out because there's a lot of really good ways to run this model. Um, you can get really good performance with VLM. Um but this is just kind of like the offtheshelf um just take it run it completely standard uh implementation. So with variable traffic we're able to support 16 simultaneous streams and with constant traffic 24 simultaneous streams on um an H100 MIG. So this is actually half an H100 GPU. It's a skew that we do a lot because it's really good for these small models where you want the Hopper performance, the Hopper architecture uplift in Tensor RTLM, the FP8 support, but you don't want to pay for like an entire 80 gigabyte GPU for just a 3 billion parameter model. So, you know, we're seeing much better um much much better concurrency. So, if you kind of like price that out with like our list prices and stuff, um, you can get, you know, a few cents per hour of of conversation, um, which is going to be, you know, substantially better than if you're if you have the volume for it. It's going to be substantially better than paying for a sort of like pertoken type API. But, okay, sure, maybe it's cheap at scale, but is it fast? 
Relevance score: 0.313
        ---

        ## 4. Input Data
        User Query:
        how to optimize inference for voice models in production
        ---

        ## 5. Output Format
        Generate a natural language response to the user's query. Ensure your response is directly supported by the "Retrieved Context" section.


----------------------------------------------------------------
-------------------- New Query at 2025-09-01T03:57:03.099633 -------------

        ## 1. Role
        You are the Chatbot Agent, a conversational AI designed to answer user questions about AI agents, RAG, context engineering, and multi-agent systems. 
        Your responses must be friendly, concise, and professional. 
        Your primary function is to synthesize information from the provided context to answer the user's query. 
        It is critical that you *only* use the information in the provided context. 
        If the context does not contain a confident answer to the user's question, 
        you *must* respond with "I am sorry, but the provided information does not contain a confident answer to your question." 
        You must never make up or invent information.

        ## 2. Task
        Answer the user's query based solely on the provided context. 
        Maintain a polite and helpful tone. If the user's query is a follow-up, 
        use the chat history to inform your response while still grounding it in the provided context.

        ## 3. Context
        The following information was retrieved from our knowledge base of video transcripts. Use this information to formulate your answer.
        ---
        ### Retrieved Context:
        Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: So if you're using this in production, you want a code sample. By the way, this is all up on our GitHub. Um you'll want a code sample that looks a lot more like a benchmarking script where you're using a um multipprocess pool. um you're sharing the you're sharing the session between all of these different requests and you're actually sending traffic with the concurrency that allows you to, you know, saturate this benchmark with, you know, the the multiple concurrent requests. Finally, both of these code samples, they do sit on top of HTTP and HTTP streaming. Um, in many cases, if you're implementing voice pipelines, you're going to use something like LiveKit or Pipecat or something. And you're also potentially going to be using a different protocol. You're going to be using something like websockets or gRPC, which we do have support for. Um, and finally, I wanted to leave you on the thought that these uh, you know, these models are only one part of a voice agent pipeline. So like we can spend a lot more than 15 minutes actually talking about like the very detailed uh implementation mechanics of making your voice model faster of you know we we haven't even touched on stuff like fine-tuning the model um you know custom voices zeroot voice cloning um being able to you know remove static and popping at the end of messages there's there's there's a lot of work to do just on the voice part but it really only is onethird of the problem when I think about voice agents. I think about three parts, listening, thinking, talking. Um, and the most important thing here is again, while you can have great run times, the infrastructure to connect these three together is really what's going to determine your latency. Being able to go from one model to have the next one running in the same data center with, you know, minimal like minimal network overhead in between the two. uh even things as simple as not having to go off and do a hair pin um at the DNS level and come back. If that saves you 10 milliseconds on every step and your voice pipeline has this and you know a chunking algorithm, it's got an interruption model and so you end up having four or five steps. Well, there just hair pinning alone is costing you 40 or 50 milliseconds and that can be 10% of your SLA for for a voice model. So yeah, uh that's that's my that's my main point here is that as much fun as it is to talk about the runtime stuff and as much work as we do there, the the infrastructure and the client implementation is equally important if not more so. Anyway, thank uh so yeah, that's the that's the review. Um thank you all for coming through. Uh I have uh we're doing an event next week at uh Fogo to Chiao which is going to be pretty fun. I'm going to be talking in more detail about building some uh systems with open source models and there's also going to be a lot of stake. So definitely come on through uh if you're interested and um I'm on I'm on Twitter, I'm on LinkedIn, so it's base 10. Um hit me up if you have any questions about this or anything else model performance. 
Relevance score: 0.293

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: I mean they did a bunch of things to make it work but a couple things that are relevant here. There is a larger vocab size because you need all the speech specific tokens like laugh and stuff. Um and then they also extended the context length for thrope scaling. So we got to make sure everything we do supports that. So performance metrics like what do we want to actually do here? Uh we we think about LLM metrics um a little bit here. Uh we we just look at them a little bit differently. So in LLMs you talk about time to first token. Now we're talking about time to first bite or sometimes even time to first sentence. Uh we we need a little bit more of a useful output um from the model before we really start feeling good about our response time. We do think about tokens per second although we're going to think about it differently which I'll explain later. And we mostly think about throughput which is you know how many requests are we able to serve at a given time. So on that, you know, goals perspective, if you ask me like, "Hey, Philip, how do you want to optimize llama in general?" I'll like I'll say, "Well, we want a lot of TPS. We want hundred. We want 500 TPS. We want a thousand tokens per second. We want as many tokens per second as we can get." With voice models, you actually don't necessarily need that. In many cases, you only want as many tokens per second as you need for a real-time stream. for Orpheus that's like 83 tokens per second which for like a three billion parameter LLM is nothing. Um but what we actually want to do instead is we want to once we hit that mark start optimizing for time to first bite so that our latency is really good and start optimizing for concurrency so that we can get more connections and spend less on GPUs. So our goal in general if all of these very nice and definitely not AI generated people are all the different like voices that our that our model is capable of creating these are all the voice agents that we're running. How can we make all of these people fit on one or even less than one GPU? That's the goal. 
Relevance score: 0.225

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: [Music] Uh hello everyone. Thank you so much for being here for uh sticking around for this talk. Um I'm going to be talking about optimizing inference for voice models in production. I'm going to be talking mostly about the runtime component but also just a little bit on the infrastructure side. Um just a quick introduction. Um I'm Philip from B 10. Base 10 is a model inference platform. Uh we run production workloads for a wide variety of AI native startups and enterprises. Um I'm based in here in SF. Um I actually just moved here. It's really awesome. My favorite part about being in SF is much better sports teams than I had in Chicago. Um and uh one of my favorite voice models is Orpheus TTS. Um which we're going to be talking about a whole bunch today. Um quick agenda. So we're going to talk about TTS model architecture like what is a text to speech model actually when you look on the config in hugging face. Uh what sort of performance metrics are we looking at? What sort of optimization techniques can we do to make the model better? Um how do we measure whether or not we succeeded? And then finally, what can we do on the infrastructure and client code to not shoot ourselves in the foot after doing a ton of runtime work and then just adding all that latency back by not doing our client code correctly. So architecture, this is one of the things I've been learning this year which has been pretty great uh to to realize it's made life a lot simpler at the runtime level. Now this is wrong like the the the thing up here uh that that I'm going to say is that like everything is an LLM. uh that is that is wrong but it's useful. Um there's kind of like two types of models. There's autogressive transformers models that are LLM or very LLM adjacent. Um you see this in embeddings. You see this in transcription with stuff like whisper TTS um is another example. You also have the more like diffuser image type models. Um which is like a very different optimization problem. But something that's cool is because TTS models are so architecturally similar to LLMs or in many cases derived directly from LLMs, we can access the rich ecosystem of LLM tooling and use it to make TTS models better. So the TTS model that we're going to be using and as an example all day um is Orpheus TTS. We're using it for two reasons. 
Relevance score: 0.676
        ---

        ## 4. Input Data
        User Query:
        how to optimize inference for voice models in production
        ---

        ## 5. Output Format
        Generate a natural language response to the user's query. Ensure your response is directly supported by the "Retrieved Context" section.


----------------------------------------------------------------
-------------------- New Query at 2025-09-01T03:57:39.745594 -------------

        ## 1. Role
        You are the Chatbot Agent, a conversational AI designed to answer user questions about AI agents, RAG, context engineering, and multi-agent systems. 
        Your responses must be friendly, concise, and professional. 
        Your primary function is to synthesize information from the provided context to answer the user's query. 
        It is critical that you *only* use the information in the provided context. 
        If the context does not contain a confident answer to the user's question, 
        you *must* respond with "I am sorry, but the provided information does not contain a confident answer to your question." 
        You must never make up or invent information.

        ## 2. Task
        Answer the user's query based solely on the provided context. 
        Maintain a polite and helpful tone. If the user's query is a follow-up, 
        use the chat history to inform your response while still grounding it in the provided context.

        ## 3. Context
        The following information was retrieved from our knowledge base of video transcripts. Use this information to formulate your answer.
        ---
        ### Retrieved Context:
        Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: [Music] Uh hello everyone. Thank you so much for being here for uh sticking around for this talk. Um I'm going to be talking about optimizing inference for voice models in production. I'm going to be talking mostly about the runtime component but also just a little bit on the infrastructure side. Um just a quick introduction. Um I'm Philip from B 10. Base 10 is a model inference platform. Uh we run production workloads for a wide variety of AI native startups and enterprises. Um I'm based in here in SF. Um I actually just moved here. It's really awesome. My favorite part about being in SF is much better sports teams than I had in Chicago. Um and uh one of my favorite voice models is Orpheus TTS. Um which we're going to be talking about a whole bunch today. Um quick agenda. So we're going to talk about TTS model architecture like what is a text to speech model actually when you look on the config in hugging face. Uh what sort of performance metrics are we looking at? What sort of optimization techniques can we do to make the model better? Um how do we measure whether or not we succeeded? And then finally, what can we do on the infrastructure and client code to not shoot ourselves in the foot after doing a ton of runtime work and then just adding all that latency back by not doing our client code correctly. So architecture, this is one of the things I've been learning this year which has been pretty great uh to to realize it's made life a lot simpler at the runtime level. Now this is wrong like the the the thing up here uh that that I'm going to say is that like everything is an LLM. uh that is that is wrong but it's useful. Um there's kind of like two types of models. There's autogressive transformers models that are LLM or very LLM adjacent. Um you see this in embeddings. You see this in transcription with stuff like whisper TTS um is another example. You also have the more like diffuser image type models. Um which is like a very different optimization problem. But something that's cool is because TTS models are so architecturally similar to LLMs or in many cases derived directly from LLMs, we can access the rich ecosystem of LLM tooling and use it to make TTS models better. So the TTS model that we're going to be using and as an example all day um is Orpheus TTS. We're using it for two reasons. 
Relevance score: 0.563

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: I mean they did a bunch of things to make it work but a couple things that are relevant here. There is a larger vocab size because you need all the speech specific tokens like laugh and stuff. Um and then they also extended the context length for thrope scaling. So we got to make sure everything we do supports that. So performance metrics like what do we want to actually do here? Uh we we think about LLM metrics um a little bit here. Uh we we just look at them a little bit differently. So in LLMs you talk about time to first token. Now we're talking about time to first bite or sometimes even time to first sentence. Uh we we need a little bit more of a useful output um from the model before we really start feeling good about our response time. We do think about tokens per second although we're going to think about it differently which I'll explain later. And we mostly think about throughput which is you know how many requests are we able to serve at a given time. So on that, you know, goals perspective, if you ask me like, "Hey, Philip, how do you want to optimize llama in general?" I'll like I'll say, "Well, we want a lot of TPS. We want hundred. We want 500 TPS. We want a thousand tokens per second. We want as many tokens per second as we can get." With voice models, you actually don't necessarily need that. In many cases, you only want as many tokens per second as you need for a real-time stream. for Orpheus that's like 83 tokens per second which for like a three billion parameter LLM is nothing. Um but what we actually want to do instead is we want to once we hit that mark start optimizing for time to first bite so that our latency is really good and start optimizing for concurrency so that we can get more connections and spend less on GPUs. So our goal in general if all of these very nice and definitely not AI generated people are all the different like voices that our that our model is capable of creating these are all the voice agents that we're running. How can we make all of these people fit on one or even less than one GPU? That's the goal. 
Relevance score: 0.419

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: Uh let's see let's see if it's actually any faster. Um, so again, the number one thing is going to be simultaneous streams because you want to be able to be very costefficient and use fewer GPU resources to serve a, you know, large amount of traffic. And in this case, our base implementation, I don't necessarily want to like call anyone out because there's a lot of really good ways to run this model. Um, you can get really good performance with VLM. Um but this is just kind of like the offtheshelf um just take it run it completely standard uh implementation. So with variable traffic we're able to support 16 simultaneous streams and with constant traffic 24 simultaneous streams on um an H100 MIG. So this is actually half an H100 GPU. It's a skew that we do a lot because it's really good for these small models where you want the Hopper performance, the Hopper architecture uplift in Tensor RTLM, the FP8 support, but you don't want to pay for like an entire 80 gigabyte GPU for just a 3 billion parameter model. So, you know, we're seeing much better um much much better concurrency. So, if you kind of like price that out with like our list prices and stuff, um, you can get, you know, a few cents per hour of of conversation, um, which is going to be, you know, substantially better than if you're if you have the volume for it. It's going to be substantially better than paying for a sort of like pertoken type API. But, okay, sure, maybe it's cheap at scale, but is it fast? 
Relevance score: 0.409
        ---

        ## 4. Input Data
        User Query:
        how to optimize inference for voice models in production
        ---

        ## 5. Output Format
        Generate a natural language response to the user's query. Ensure your response is directly supported by the "Retrieved Context" section.


----------------------------------------------------------------
-------------------- New Query at 2025-09-01T04:02:50.024711 -------------

        ## 1. Role
        You are the Chatbot Agent, a conversational AI designed to answer user questions about AI agents, RAG, context engineering, and multi-agent systems. 
        Your responses must be friendly, concise, and professional. 
        Your primary function is to synthesize information from the provided context to answer the user's query. 
        It is critical that you *only* use the information in the provided context. 
        If the context does not contain a confident answer to the user's question, 
        you *must* respond with "I am sorry, but the provided information does not contain a confident answer to your question." 
        You must never make up or invent information.

        ## 2. Task
        Answer the user's query based solely on the provided context. 
        Maintain a polite and helpful tone. If the user's query is a follow-up, 
        use the chat history to inform your response while still grounding it in the provided context.

        ## 3. Context
        The following information was retrieved from our knowledge base of video transcripts. Use this information to formulate your answer.
        ---
        ### Retrieved Context:
        Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: Yes, it's fast. Um so with the you know with the tot implementation on the MIGs and on the H100s we can actually get all the way down to 150 millisecond time to first bite um in like real world uh testing that we've done. Now that we'll we'll talk in a minute like that doesn't mean your whole pipeline is that fast. That's just like one part of the pipeline. Uh but it's it's important because you know you definitely don't want to be spending a lot of time waiting around for that first token. So to kind of transition into that that discussion of like what can go wrong here like you have this graph and you have this you know nice uh config that I had up here and you're like all right cool I'm going to take this I'm going to put it in production and I'm going to see the results that uh that he put up on screen and it's going to work great and the answer is no it's not it's a little bit harder than that. Um, so the thing is like non-runtime factors when we get especially with these small models and with these multimodal systems can actually be like way more important than your runtime. Um, and that's your lat your infrastructure and your client code because you know I I showed here all right maybe maybe I got it you know I I cut the runtime in in half um from from the base implementation I I saved a couple hundred milliseconds very easy to add those couple hundred milliseconds back and well beyond that by you know sending my query to New York instead of California or by having to establish a session every time I uh you know run my client code. Um, so a few like pitfalls to avoid. Um, number one, like if you go in, you know, our our model library or something and we're just trying to get you started very quickly um with this with this kind of inference sample. Um, it's basically going to be, hey, use requests, make a stream, stream it to your local computer and start, you know, playing it on ffmpeg or something. The issue is that here like the requests are going to be sent sequentially and you need to create a new session every time. um that takes time. 
Relevance score: 0.161

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: Um so first off it's an LLM. Um, if you are running an LLM with like VLM for example, um, you can generally in many cases get better performance with Tensor Rotlm. Uh, Tensor Rot is something that we've been using at Base 10 a lot. I like to joke that I'm the unofficial marketing department for Tensor RT LLM because of how much I talk about it. Uh, but it really is fast. It can be a little bit complicated from a developer experience perspective to get up and running with it, but once you are up and running, um, it it works really well. Uh, we can also just like quantize the model. Um, even though it's small, you can always make it faster by making it smaller. Uh, with Hopper architecture, we quantize this model to FP8 pretty successfully. I know usually quantizing really small models like this can lead to performance degragation, but for this model, it's working pretty well in FP8, even when we quantize the KV cache. And then a lot of the other runtime stuff is actually more like audio specific than it is LLM specific. So, one of the big challenges that we don't have with LLMs, which are just parsing nice, convenient bits of text back and forth, is you have your audio, you have your audio codec, you have your decoding, all that kind of stuff. So, we use snack, which I was very disappointed to learn is not an actual tasty snack, um, but an audio decoder. And we actually use torch compile, um, and torch compile you might be used to running on, you know, a a model u compiling your model weights to make your runtime faster. We're actually using the same kind of system with torch compile and with um PyTorch inference mode on the audio decoder and running that on the GPU. Um we make sure that all the token batching um token level batching works well throughout the entire pipeline and support multiple streaming protocols. Um yeah, so these are the engine settings that you would need. um you've got the you know quantization type of FP8 KV the FP8 context uh FMHA um in order to you know support the uh support the um hopper architecture and the uh quantization there. Um and here's a quick code sample of I got a little ahead of my slides I guess. Here's a little quick code sample of the audio decoding. Um so we are basically you know batching. Um usually we would talk about continuous batching when we're talking about LLM optimization. We want to package all those tokens together. In this case we are doing dynamic batching. Um so we're trying to pack as much into a batch as we can but every 15 milliseconds we're going to uh shoot it out. Um you've got that timeout setup here. Um if you want to trade off um for a little bit of latency for more throughput you can make that batch bigger. Um, so yeah, we don't have token level continuous batching yet here, but we do have dynamic batching, which is going to get you pretty close. Um, and because of this, actually something that's that I was surprised about uh when we profiled this is that our TTS imple implementation with Orpheus is actually in many cases CPUbound, which is kind of where you want to be. Um, you can throw more CPUs at a resource uh pretty pretty efficiently. Um even though the next token production and the audio co decoding are both on the GPU um both of those loops hit the CPU at different points um and that can actually be the bottleneck in the number of simultaneous streams that we're able to create. So how do we do like I just showed you a lot of code and talked through it really quickly without really getting into depth. Uh that could all just be smoke and mirrors. 
Relevance score: 0.136

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: Uh let's see let's see if it's actually any faster. Um, so again, the number one thing is going to be simultaneous streams because you want to be able to be very costefficient and use fewer GPU resources to serve a, you know, large amount of traffic. And in this case, our base implementation, I don't necessarily want to like call anyone out because there's a lot of really good ways to run this model. Um, you can get really good performance with VLM. Um but this is just kind of like the offtheshelf um just take it run it completely standard uh implementation. So with variable traffic we're able to support 16 simultaneous streams and with constant traffic 24 simultaneous streams on um an H100 MIG. So this is actually half an H100 GPU. It's a skew that we do a lot because it's really good for these small models where you want the Hopper performance, the Hopper architecture uplift in Tensor RTLM, the FP8 support, but you don't want to pay for like an entire 80 gigabyte GPU for just a 3 billion parameter model. So, you know, we're seeing much better um much much better concurrency. So, if you kind of like price that out with like our list prices and stuff, um, you can get, you know, a few cents per hour of of conversation, um, which is going to be, you know, substantially better than if you're if you have the volume for it. It's going to be substantially better than paying for a sort of like pertoken type API. But, okay, sure, maybe it's cheap at scale, but is it fast? 
Relevance score: 0.315

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: So if you're using this in production, you want a code sample. By the way, this is all up on our GitHub. Um you'll want a code sample that looks a lot more like a benchmarking script where you're using a um multipprocess pool. um you're sharing the you're sharing the session between all of these different requests and you're actually sending traffic with the concurrency that allows you to, you know, saturate this benchmark with, you know, the the multiple concurrent requests. Finally, both of these code samples, they do sit on top of HTTP and HTTP streaming. Um, in many cases, if you're implementing voice pipelines, you're going to use something like LiveKit or Pipecat or something. And you're also potentially going to be using a different protocol. You're going to be using something like websockets or gRPC, which we do have support for. Um, and finally, I wanted to leave you on the thought that these uh, you know, these models are only one part of a voice agent pipeline. So like we can spend a lot more than 15 minutes actually talking about like the very detailed uh implementation mechanics of making your voice model faster of you know we we haven't even touched on stuff like fine-tuning the model um you know custom voices zeroot voice cloning um being able to you know remove static and popping at the end of messages there's there's there's a lot of work to do just on the voice part but it really only is onethird of the problem when I think about voice agents. I think about three parts, listening, thinking, talking. Um, and the most important thing here is again, while you can have great run times, the infrastructure to connect these three together is really what's going to determine your latency. Being able to go from one model to have the next one running in the same data center with, you know, minimal like minimal network overhead in between the two. uh even things as simple as not having to go off and do a hair pin um at the DNS level and come back. If that saves you 10 milliseconds on every step and your voice pipeline has this and you know a chunking algorithm, it's got an interruption model and so you end up having four or five steps. Well, there just hair pinning alone is costing you 40 or 50 milliseconds and that can be 10% of your SLA for for a voice model. So yeah, uh that's that's my that's my main point here is that as much fun as it is to talk about the runtime stuff and as much work as we do there, the the infrastructure and the client implementation is equally important if not more so. Anyway, thank uh so yeah, that's the that's the review. Um thank you all for coming through. Uh I have uh we're doing an event next week at uh Fogo to Chiao which is going to be pretty fun. I'm going to be talking in more detail about building some uh systems with open source models and there's also going to be a lot of stake. So definitely come on through uh if you're interested and um I'm on I'm on Twitter, I'm on LinkedIn, so it's base 10. Um hit me up if you have any questions about this or anything else model performance. 
Relevance score: 0.314
        ---

        ## 4. Input Data
        User Query:
        how to optimize inference for voice models in production
        ---

        ## 5. Output Format
        Generate a natural language response to the user's query. Ensure your response is directly supported by the "Retrieved Context" section.


----------------------------------------------------------------
-------------------- New Query at 2025-09-01T04:05:24.722487 -------------

        ## Role
        You are a chatbot that answers only about AI agents, RAG, context engineering, and multi-agent systems.  
        Be friendly and concise. Use only the given context.  
        If the context does not answer, reply exactly:  
        "I am sorry, but the provided information does not contain a confident answer to your question."  

        ## Task
        Answer the users query based only on the retrieved context.  
        Use chat history if relevant, but never go beyond the given context.  

        ## Context
        Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: So if you're using this in production, you want a code sample. By the way, this is all up on our GitHub. Um you'll want a code sample that looks a lot more like a benchmarking script where you're using a um multipprocess pool. um you're sharing the you're sharing the session between all of these different requests and you're actually sending traffic with the concurrency that allows you to, you know, saturate this benchmark with, you know, the the multiple concurrent requests. Finally, both of these code samples, they do sit on top of HTTP and HTTP streaming. Um, in many cases, if you're implementing voice pipelines, you're going to use something like LiveKit or Pipecat or something. And you're also potentially going to be using a different protocol. You're going to be using something like websockets or gRPC, which we do have support for. Um, and finally, I wanted to leave you on the thought that these uh, you know, these models are only one part of a voice agent pipeline. So like we can spend a lot more than 15 minutes actually talking about like the very detailed uh implementation mechanics of making your voice model faster of you know we we haven't even touched on stuff like fine-tuning the model um you know custom voices zeroot voice cloning um being able to you know remove static and popping at the end of messages there's there's there's a lot of work to do just on the voice part but it really only is onethird of the problem when I think about voice agents. I think about three parts, listening, thinking, talking. Um, and the most important thing here is again, while you can have great run times, the infrastructure to connect these three together is really what's going to determine your latency. Being able to go from one model to have the next one running in the same data center with, you know, minimal like minimal network overhead in between the two. uh even things as simple as not having to go off and do a hair pin um at the DNS level and come back. If that saves you 10 milliseconds on every step and your voice pipeline has this and you know a chunking algorithm, it's got an interruption model and so you end up having four or five steps. Well, there just hair pinning alone is costing you 40 or 50 milliseconds and that can be 10% of your SLA for for a voice model. So yeah, uh that's that's my that's my main point here is that as much fun as it is to talk about the runtime stuff and as much work as we do there, the the infrastructure and the client implementation is equally important if not more so. Anyway, thank uh so yeah, that's the that's the review. Um thank you all for coming through. Uh I have uh we're doing an event next week at uh Fogo to Chiao which is going to be pretty fun. I'm going to be talking in more detail about building some uh systems with open source models and there's also going to be a lot of stake. So definitely come on through uh if you're interested and um I'm on I'm on Twitter, I'm on LinkedIn, so it's base 10. Um hit me up if you have any questions about this or anything else model performance. 
Relevance score: 0.229

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: Okay, three reasons. The two reasons are because it's open source and it's really good. And also I think uh Elias and Amu and everyone at Canopy Labs is really awesome. So uh that's the that's the third reason we're we're talking about their model. But it's a Llama 3.2 3B backbone. So like if you look at this is the little like config from hugging face copy and pasted onto the screen. It's a llama for causal LM architecture. And so because of that we can do like all of our normal llama stuff to this model and make it faster. Um they did a couple things. 
Relevance score: 0.171

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: [Music] Uh hello everyone. Thank you so much for being here for uh sticking around for this talk. Um I'm going to be talking about optimizing inference for voice models in production. I'm going to be talking mostly about the runtime component but also just a little bit on the infrastructure side. Um just a quick introduction. Um I'm Philip from B 10. Base 10 is a model inference platform. Uh we run production workloads for a wide variety of AI native startups and enterprises. Um I'm based in here in SF. Um I actually just moved here. It's really awesome. My favorite part about being in SF is much better sports teams than I had in Chicago. Um and uh one of my favorite voice models is Orpheus TTS. Um which we're going to be talking about a whole bunch today. Um quick agenda. So we're going to talk about TTS model architecture like what is a text to speech model actually when you look on the config in hugging face. Uh what sort of performance metrics are we looking at? What sort of optimization techniques can we do to make the model better? Um how do we measure whether or not we succeeded? And then finally, what can we do on the infrastructure and client code to not shoot ourselves in the foot after doing a ton of runtime work and then just adding all that latency back by not doing our client code correctly. So architecture, this is one of the things I've been learning this year which has been pretty great uh to to realize it's made life a lot simpler at the runtime level. Now this is wrong like the the the thing up here uh that that I'm going to say is that like everything is an LLM. uh that is that is wrong but it's useful. Um there's kind of like two types of models. There's autogressive transformers models that are LLM or very LLM adjacent. Um you see this in embeddings. You see this in transcription with stuff like whisper TTS um is another example. You also have the more like diffuser image type models. Um which is like a very different optimization problem. But something that's cool is because TTS models are so architecturally similar to LLMs or in many cases derived directly from LLMs, we can access the rich ecosystem of LLM tooling and use it to make TTS models better. So the TTS model that we're going to be using and as an example all day um is Orpheus TTS. We're using it for two reasons. 
Relevance score: 0.367  

        ## Input
        how to optimize inference for voice models in production  

        ## Output
        Write a natural, context-based answer.



----------------------------------------------------------------
-------------------- New Query at 2025-09-01T04:10:17.024592 -------------

        ## Task
        Answer the users query based only on the retrieved context.  
        Use chat history if relevant, but never go beyond the given context.  

        ## Context
        Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: [Music] Uh hello everyone. Thank you so much for being here for uh sticking around for this talk. Um I'm going to be talking about optimizing inference for voice models in production. I'm going to be talking mostly about the runtime component but also just a little bit on the infrastructure side. Um just a quick introduction. Um I'm Philip from B 10. Base 10 is a model inference platform. Uh we run production workloads for a wide variety of AI native startups and enterprises. Um I'm based in here in SF. Um I actually just moved here. It's really awesome. My favorite part about being in SF is much better sports teams than I had in Chicago. Um and uh one of my favorite voice models is Orpheus TTS. Um which we're going to be talking about a whole bunch today. Um quick agenda. So we're going to talk about TTS model architecture like what is a text to speech model actually when you look on the config in hugging face. Uh what sort of performance metrics are we looking at? What sort of optimization techniques can we do to make the model better? Um how do we measure whether or not we succeeded? And then finally, what can we do on the infrastructure and client code to not shoot ourselves in the foot after doing a ton of runtime work and then just adding all that latency back by not doing our client code correctly. So architecture, this is one of the things I've been learning this year which has been pretty great uh to to realize it's made life a lot simpler at the runtime level. Now this is wrong like the the the thing up here uh that that I'm going to say is that like everything is an LLM. uh that is that is wrong but it's useful. Um there's kind of like two types of models. There's autogressive transformers models that are LLM or very LLM adjacent. Um you see this in embeddings. You see this in transcription with stuff like whisper TTS um is another example. You also have the more like diffuser image type models. Um which is like a very different optimization problem. But something that's cool is because TTS models are so architecturally similar to LLMs or in many cases derived directly from LLMs, we can access the rich ecosystem of LLM tooling and use it to make TTS models better. So the TTS model that we're going to be using and as an example all day um is Orpheus TTS. We're using it for two reasons. 
Relevance score: 0.676

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: I mean they did a bunch of things to make it work but a couple things that are relevant here. There is a larger vocab size because you need all the speech specific tokens like laugh and stuff. Um and then they also extended the context length for thrope scaling. So we got to make sure everything we do supports that. So performance metrics like what do we want to actually do here? Uh we we think about LLM metrics um a little bit here. Uh we we just look at them a little bit differently. So in LLMs you talk about time to first token. Now we're talking about time to first bite or sometimes even time to first sentence. Uh we we need a little bit more of a useful output um from the model before we really start feeling good about our response time. We do think about tokens per second although we're going to think about it differently which I'll explain later. And we mostly think about throughput which is you know how many requests are we able to serve at a given time. So on that, you know, goals perspective, if you ask me like, "Hey, Philip, how do you want to optimize llama in general?" I'll like I'll say, "Well, we want a lot of TPS. We want hundred. We want 500 TPS. We want a thousand tokens per second. We want as many tokens per second as we can get." With voice models, you actually don't necessarily need that. In many cases, you only want as many tokens per second as you need for a real-time stream. for Orpheus that's like 83 tokens per second which for like a three billion parameter LLM is nothing. Um but what we actually want to do instead is we want to once we hit that mark start optimizing for time to first bite so that our latency is really good and start optimizing for concurrency so that we can get more connections and spend less on GPUs. So our goal in general if all of these very nice and definitely not AI generated people are all the different like voices that our that our model is capable of creating these are all the voice agents that we're running. How can we make all of these people fit on one or even less than one GPU? That's the goal. 
Relevance score: 0.478

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: Okay, three reasons. The two reasons are because it's open source and it's really good. And also I think uh Elias and Amu and everyone at Canopy Labs is really awesome. So uh that's the that's the third reason we're we're talking about their model. But it's a Llama 3.2 3B backbone. So like if you look at this is the little like config from hugging face copy and pasted onto the screen. It's a llama for causal LM architecture. And so because of that we can do like all of our normal llama stuff to this model and make it faster. Um they did a couple things. 
Relevance score: 0.241

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: So if you're using this in production, you want a code sample. By the way, this is all up on our GitHub. Um you'll want a code sample that looks a lot more like a benchmarking script where you're using a um multipprocess pool. um you're sharing the you're sharing the session between all of these different requests and you're actually sending traffic with the concurrency that allows you to, you know, saturate this benchmark with, you know, the the multiple concurrent requests. Finally, both of these code samples, they do sit on top of HTTP and HTTP streaming. Um, in many cases, if you're implementing voice pipelines, you're going to use something like LiveKit or Pipecat or something. And you're also potentially going to be using a different protocol. You're going to be using something like websockets or gRPC, which we do have support for. Um, and finally, I wanted to leave you on the thought that these uh, you know, these models are only one part of a voice agent pipeline. So like we can spend a lot more than 15 minutes actually talking about like the very detailed uh implementation mechanics of making your voice model faster of you know we we haven't even touched on stuff like fine-tuning the model um you know custom voices zeroot voice cloning um being able to you know remove static and popping at the end of messages there's there's there's a lot of work to do just on the voice part but it really only is onethird of the problem when I think about voice agents. I think about three parts, listening, thinking, talking. Um, and the most important thing here is again, while you can have great run times, the infrastructure to connect these three together is really what's going to determine your latency. Being able to go from one model to have the next one running in the same data center with, you know, minimal like minimal network overhead in between the two. uh even things as simple as not having to go off and do a hair pin um at the DNS level and come back. If that saves you 10 milliseconds on every step and your voice pipeline has this and you know a chunking algorithm, it's got an interruption model and so you end up having four or five steps. Well, there just hair pinning alone is costing you 40 or 50 milliseconds and that can be 10% of your SLA for for a voice model. So yeah, uh that's that's my that's my main point here is that as much fun as it is to talk about the runtime stuff and as much work as we do there, the the infrastructure and the client implementation is equally important if not more so. Anyway, thank uh so yeah, that's the that's the review. Um thank you all for coming through. Uh I have uh we're doing an event next week at uh Fogo to Chiao which is going to be pretty fun. I'm going to be talking in more detail about building some uh systems with open source models and there's also going to be a lot of stake. So definitely come on through uh if you're interested and um I'm on I'm on Twitter, I'm on LinkedIn, so it's base 10. Um hit me up if you have any questions about this or anything else model performance. 
Relevance score: 0.211  

        ## Input
        how to optimize inference for voice models in production  

        ## Output
        Write a natural, context-based answer.



----------------------------------------------------------------
-------------------- New Query at 2025-09-01T04:13:43.089153 -------------

        ## Task
        Answer the users query based only on the retrieved context.  
        Use chat history if relevant, but never go beyond the given context.  

        ## Context
        Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: [Music] Uh hello everyone. Thank you so much for being here for uh sticking around for this talk. Um I'm going to be talking about optimizing inference for voice models in production. I'm going to be talking mostly about the runtime component but also just a little bit on the infrastructure side. Um just a quick introduction. Um I'm Philip from B 10. Base 10 is a model inference platform. Uh we run production workloads for a wide variety of AI native startups and enterprises. Um I'm based in here in SF. Um I actually just moved here. It's really awesome. My favorite part about being in SF is much better sports teams than I had in Chicago. Um and uh one of my favorite voice models is Orpheus TTS. Um which we're going to be talking about a whole bunch today. Um quick agenda. So we're going to talk about TTS model architecture like what is a text to speech model actually when you look on the config in hugging face. Uh what sort of performance metrics are we looking at? What sort of optimization techniques can we do to make the model better? Um how do we measure whether or not we succeeded? And then finally, what can we do on the infrastructure and client code to not shoot ourselves in the foot after doing a ton of runtime work and then just adding all that latency back by not doing our client code correctly. So architecture, this is one of the things I've been learning this year which has been pretty great uh to to realize it's made life a lot simpler at the runtime level. Now this is wrong like the the the thing up here uh that that I'm going to say is that like everything is an LLM. uh that is that is wrong but it's useful. Um there's kind of like two types of models. There's autogressive transformers models that are LLM or very LLM adjacent. Um you see this in embeddings. You see this in transcription with stuff like whisper TTS um is another example. You also have the more like diffuser image type models. Um which is like a very different optimization problem. But something that's cool is because TTS models are so architecturally similar to LLMs or in many cases derived directly from LLMs, we can access the rich ecosystem of LLM tooling and use it to make TTS models better. So the TTS model that we're going to be using and as an example all day um is Orpheus TTS. We're using it for two reasons. 
Relevance score: 0.676

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: I mean they did a bunch of things to make it work but a couple things that are relevant here. There is a larger vocab size because you need all the speech specific tokens like laugh and stuff. Um and then they also extended the context length for thrope scaling. So we got to make sure everything we do supports that. So performance metrics like what do we want to actually do here? Uh we we think about LLM metrics um a little bit here. Uh we we just look at them a little bit differently. So in LLMs you talk about time to first token. Now we're talking about time to first bite or sometimes even time to first sentence. Uh we we need a little bit more of a useful output um from the model before we really start feeling good about our response time. We do think about tokens per second although we're going to think about it differently which I'll explain later. And we mostly think about throughput which is you know how many requests are we able to serve at a given time. So on that, you know, goals perspective, if you ask me like, "Hey, Philip, how do you want to optimize llama in general?" I'll like I'll say, "Well, we want a lot of TPS. We want hundred. We want 500 TPS. We want a thousand tokens per second. We want as many tokens per second as we can get." With voice models, you actually don't necessarily need that. In many cases, you only want as many tokens per second as you need for a real-time stream. for Orpheus that's like 83 tokens per second which for like a three billion parameter LLM is nothing. Um but what we actually want to do instead is we want to once we hit that mark start optimizing for time to first bite so that our latency is really good and start optimizing for concurrency so that we can get more connections and spend less on GPUs. So our goal in general if all of these very nice and definitely not AI generated people are all the different like voices that our that our model is capable of creating these are all the voice agents that we're running. How can we make all of these people fit on one or even less than one GPU? That's the goal. 
Relevance score: 0.478

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: So if you're using this in production, you want a code sample. By the way, this is all up on our GitHub. Um you'll want a code sample that looks a lot more like a benchmarking script where you're using a um multipprocess pool. um you're sharing the you're sharing the session between all of these different requests and you're actually sending traffic with the concurrency that allows you to, you know, saturate this benchmark with, you know, the the multiple concurrent requests. Finally, both of these code samples, they do sit on top of HTTP and HTTP streaming. Um, in many cases, if you're implementing voice pipelines, you're going to use something like LiveKit or Pipecat or something. And you're also potentially going to be using a different protocol. You're going to be using something like websockets or gRPC, which we do have support for. Um, and finally, I wanted to leave you on the thought that these uh, you know, these models are only one part of a voice agent pipeline. So like we can spend a lot more than 15 minutes actually talking about like the very detailed uh implementation mechanics of making your voice model faster of you know we we haven't even touched on stuff like fine-tuning the model um you know custom voices zeroot voice cloning um being able to you know remove static and popping at the end of messages there's there's there's a lot of work to do just on the voice part but it really only is onethird of the problem when I think about voice agents. I think about three parts, listening, thinking, talking. Um, and the most important thing here is again, while you can have great run times, the infrastructure to connect these three together is really what's going to determine your latency. Being able to go from one model to have the next one running in the same data center with, you know, minimal like minimal network overhead in between the two. uh even things as simple as not having to go off and do a hair pin um at the DNS level and come back. If that saves you 10 milliseconds on every step and your voice pipeline has this and you know a chunking algorithm, it's got an interruption model and so you end up having four or five steps. Well, there just hair pinning alone is costing you 40 or 50 milliseconds and that can be 10% of your SLA for for a voice model. So yeah, uh that's that's my that's my main point here is that as much fun as it is to talk about the runtime stuff and as much work as we do there, the the infrastructure and the client implementation is equally important if not more so. Anyway, thank uh so yeah, that's the that's the review. Um thank you all for coming through. Uh I have uh we're doing an event next week at uh Fogo to Chiao which is going to be pretty fun. I'm going to be talking in more detail about building some uh systems with open source models and there's also going to be a lot of stake. So definitely come on through uh if you're interested and um I'm on I'm on Twitter, I'm on LinkedIn, so it's base 10. Um hit me up if you have any questions about this or anything else model performance. 
Relevance score: 0.228

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: Okay, three reasons. The two reasons are because it's open source and it's really good. And also I think uh Elias and Amu and everyone at Canopy Labs is really awesome. So uh that's the that's the third reason we're we're talking about their model. But it's a Llama 3.2 3B backbone. So like if you look at this is the little like config from hugging face copy and pasted onto the screen. It's a llama for causal LM architecture. And so because of that we can do like all of our normal llama stuff to this model and make it faster. Um they did a couple things. 
Relevance score: 0.199  

        ## Input
        how to optimize inference for voice models in production  

        ## Output
        Write a natural, context-based answer.



----------------------------------------------------------------
-------------------- New Query at 2025-09-01T04:23:08.872495 -------------

        ## Task
        Answer the users query based only on the retrieved context.  
        Use chat history if relevant, but never go beyond the given context.  

        ## Context
        Source: https://www.youtube.com/watch?v=3qYd5XfoojQ
Content: Yes. All right. So, I'm just going to pause here for any further questions related to what we've discussed, but it's definitely been a great and enlightening session um from Anand. I think it definitely helped you understand how you can be a smarter leader with the help of AI and where all you can use it and how you should use it as well. So if you've got any specific questions on what we've covered um anything related to the course be great and put it on the Q&A box. Yeah. So this course helps you with the right kind of acumen to be able to lead better. This is in no means saying that you're going to become a leader. you'll be able to explore leadership roles immediately. Totally depends on what your experience is. Someone has this question about will if if you will be able to take up roles um you know leadership positions after this course. So it obviously depends on what your background is as well. So Anand there's actually one more question uh which I think you can address. So what else should leaders be paired off? You know there's definitely AI. So what on the what what are the softer aspects that a leader should focus on? Maybe if you can highlight that as well. 
Relevance score: 0.429

Source: https://www.youtube.com/watch?v=3qYd5XfoojQ
Content: I hope this question was answered. Um, so I think we can kind of wrap up the session. We've got a few people who are interested in the course. Um, so yeah, if if you feel like talking to a learning advisor right away, you've got anything specific to share, what you could do is just head to the course page and there's a small bot as well. So you can enter your input there and the learning advisor will reach out to you either way. Right. 
Relevance score: 0.383

Source: https://www.youtube.com/watch?v=3qYd5XfoojQ
Content: So those are some of the things I want to highlight here. I think it is important right we as a technology people um often we dive directly into doing coding and doing python doing uh you know all those things but I think these are the things which which matters to stay relevant with respect to what we want to achieve right back to my um um concepts about like it's not about um implementing use cases is the hardest part it's about identifying the specific use case how do you make sure that the specific use case is going to make an impact for your organization ation is what matters the most. >> Yeah, absolutely. 
Relevance score: 0.268

Source: https://www.youtube.com/watch?v=3qYd5XfoojQ
Content: Got it. Um so in case people uh you know joining us in this session you've got any any any thoughts on what forces are driving change and how leadership is perceived or how leaders function uh you can let us know as well and if you feel tech and AI is the biggest enabler of this shift would share your thoughts on that because that's exactly what we're going to cover next is how is with AI coming in how is the role how is the leader role changing or not changing exactly but how is it really evolving? Um again Anand you know maybe learners would be kind of intrigued interested in knowing your journey as well. What happened when AI hit and how you kind of changed your perception about how you should leverage AI and how did you also translate that to the teams that you lead. >> Yeah. So again there's no conver the conversation is not complete nowadays if you don't talk about AI right that's the joke that we actually run with um I think AI in in my standpoint has influenced in in many ways right back you know 2 years ago 3 years ago everyone was exploring it right everyone was like experimenting it and last year we saw some pilot projects across the industries right on what we can accomplish using AI now I think this year I see it's the real realization moment, right? Okay. So for us to even implement uh something with using AI, we have to be good at the foundational aspect like you know be it data, be it infrastructure, be it use cases, be it the outcome that we want to achieve, right? You know you can crowdsource lot of problems, lot of use cases when it comes to AI. But out of those ideas and out of those uh crowdsourced information, what are the ideas that you can pick? what are the use cases that you can pick to make a meaning meaningful impact for the broader organization is what's going to matter right so I think there's a lot of factors that is coming into play um starting this year and it's going to continue to the next into the next year to to have the realization moment right okay we can implement AI but before we implement AI or we before we you implement this use case using AI here are the foundational things that we need to be good at so that we we know where we are where we want to march towards right personally Um you know initially I started using uh the tools that are available in the market like chat GPT and um lot of other tools. um you know for simple questions like instead of using Google search I started using chat GPT now it is becoming uh you know part of your life right anything and everything that you want to do you always go to chat GPT by default right those days are gone where you go to Google and search for a product or search for uh some information now you don't need to go there you just need to use the I'm just using the chat GPT and then you know same thing with perplexity right if you want to um you know I just I just had an example uh two days ago where I was asking Proxidity to act as a as a persona and I was asking a lot of questions to that persona and it was able to answer all those questions for me right as if like I'm talking to someone who is a real person and I you need to set the right context you know to this tool so that you can maximize the um the results that you're going to get out of it and know with with an evolving change with comet right and I was playing with the comet browser right uh released from perplexity um you can do your job like for example simple example like if you want to send a LinkedIn message to your colleague or your friend you can just say that to your comet and comment browser will automatically take care of that right and if you want to book a table for at a restaurant for for a reservation you can do that right so you just delegate the task and go do something else and come back to it like to me it is becoming part of your personal life and also professional life right you know given the Microsoft co-pilot given all the things that we are enabling within the corporate Right? It is it is making our life easier, right? 
Relevance score: 0.389  

        ## Input
        How Leaders Unlock AI-Driven Growth  

        ## Output
        Write a natural, context-based answer.



----------------------------------------------------------------
-------------------- New Query at 2025-09-01T17:13:45.393713 -------------

        ## Task
        Answer the users query based only on the retrieved context.  
        Use chat history if relevant, but never go beyond the given context.  

        ## Context
        Source: https://www.youtube.com/watch?v=CQGuvf6gSrM
Content: [Music] Thanks everyone for being here. Um I'm going to give this talk mostly from the uh point of view of being a co-founder and chief scientist at Arthur AAI where for six years prior to joining Mosilla AI as CEO. Um, I do want to say Mozilla AI operates in the open source world where we're providing open source AI tooling and we're supporting the open source AI stack. Our end goal is to uh enable the open source community to be at the same table as a Sam Alman when talking about AI moving forward. So, if you're interested in that, that's not what this talk is going to be about, but we can talk about that offline. This talk is going to be about 2025 finally being the year of the evals. And as was written uh written as was spoken uh by my introduction, I've been in the space for a very long time. Arthur AI for example was in and still is in obser observability evaluation and security in both traditional ML and AI and then into the deep learning revolution and then into the genai revolution and then into the agentic revolution. And I think we're finally at the point where uh all of these companies are going to start seeing hockey stick growth which is exciting. So the thesis of this talk, one thing is that uh I I see a IML monitoring and evaluation as two sides of the same sword or ruler, right? You can't do monitoring or observability without being able to measure and measurement is uh the core functionality for evaluation. This was not top of mind really with the seuite uh until two things happened concurrently. One is AI became a thing that people who aren't a CIO or CTO could understand. So the CEOs, the CFOs, the CISOs began to understand it basically when ChatGBT came out. And simultaneously there was a perfectly timed budget freeze across enterprise at least in the US that happened due to a fear of an impending recession. So this is right before chat GBT launched. This is like October, November when most enterprises would set up budget for the next year. At that time there was a freeze except for money that could be opened up for a specific pet project and that PET project because CEOs and CFOs then knew about it uh was Gen AI. So that happened and then now we have the final sort of uh vertex on this triangle which is going to force evaluation to be top of mind this year which is that we have systems that are now acting for uh humans acting for teams as opposed to just providing inputs into larger systems. So these three things together are as you saw with Brain Trust, as we're seeing at Arthur, as you're seeing with like Arise AI or Galileo, other big players in this space, we're starting to see big takeoff because of this. Cool. 
Relevance score: -8.046

Source: https://www.youtube.com/watch?v=SbUxRluVRwk&t=167s
Content: We start off with defining use cases. We want to go through and integrate with products, right? We're not doing green field everywhere. We want to measure ROI, find the right data. We want to test and refine workflows, build the interfaces we need for success, sell this product, and make our customers care. And it's not one person who does this job. You can't just say, "AI researchers, go make me $10 million from this product unless a very specific niche. So this means your success is not one job unless you're a founder, but we'll skip that. So the goal here is that you need to have a comprehensive AI team and you need to figure out how are you going to structure that. And the thing that we need to remember is that companies aren't just one team. It's not just my AI team owns this small segment, this deployment or whatever. Otherwise, you ship your org chart and you get some weird product behaviors. So identify to yourself what is your bottleneck? What is stopping you from achieving success? Is it shipping features? Is it acquiring users? Is it retaining users? Are you monetizing correctly? Are there scalability issues? Are there reliability and observability issues? Right? All of us have probably run into these things as we are deploying AI products. So, we need to make sure we can prioritize all these things and hire accordingly. And these are all questions that you need to answer when building an AI team. The key takeaway here is what kind of team do you need? and only you know that answer. Let's talk about generalists and why I think they're important. So in 2021 I was building uh first machine learning team and I adopted an approach where we hired generalists. We supported them by automation across the board. So at the time I was hired to a conversational AI company working on a platform. Sorry, let me rephrase that. AI agent building platform. Just wanted to make sure you guys understood what that meant. And I was hired with the mandate of we want ML. That was my job description. Change that. We want AI. So after working with the business teams and leadership team, the this was the the final set of goals we set. We want to serve hundreds of thousands of concurrent models. It needs to be multi-dommain. It has to be low cost and we want to support real-time training and serving. Those are some tough goals. 
Relevance score: -8.674

Source: https://www.youtube.com/watch?v=SbUxRluVRwk&t=167s
Content: Now, in certain domains, the best tech is essential, right? So, if you're working at OpenAI or model provider, Anthropic Google or some of the startups, you want the best team who's working on that because that is a product as we covered. So here I'll I'll propose a wager for you. Are people here familiar with Pascal's wager philosophy? I'll give you the successor of that Ampere's wager if you're familiar with graphics card architectures. Here's your trade. You trade your team for five researchers from the top labs. And maybe you need to throw in some cash and first round picks for that as well. But do you make this trade? Do you trade your team that has domain knowledge has worked in the area for five AI researchers? I want you to think about that. So we go back to the question of what does an AI team need to do? There's a lot of stuff, right? 
Relevance score: -9.958

Source: https://www.youtube.com/watch?v=w9u11ioHGA0
Content: Uh I'll flash this towards the end as well with pi.ai-talk. Uh it should point to the slides that we're going through. And as I mentioned, plain English, no hype, no buzz, uh no debates, no like all right. So how to think about techniques before we go techniques and get into the weeds of it like why does this even matter and the way we always think about it is like always start with outcomes. You're always trying to solve some product problem. Uh and generally the best way to visualize something like this. you have a certain quality bar you want to reach and there were a very interesting talk this this week about like you know benchmarks aren't really helpful and absolutely eval are helpful you're trying to launch a CRM agent and you sort of have a launch bar like a place where you feel comfortable that you can actually put it out into the world uh and techniques fit somewhere here you have that like kind of end metric and you're trying to like come up with different ways to shore up the quality and those ways are like sort of the techniques there and you know this is sort of your own personal benchmark you start with some of the easy the easy the easy bars you want to hit and then there's like medium benchmarks and hard benchmarks. So these are query sets you're setting up. Uh and then you know depending on what you want to reach and in at what time frame uh then you end up trying different things. Uh and this is what we call like quality engineering loop. You sort of like baseline yourself. Okay you want to achieve you know you want CRM and this is the easy query set and your quality is there. Uh just through the simplest way you can try it. Do a loss analysis. Okay what's broken? There were a lot of eval talks this week and then what we call quality engineering. Now the reason I I I I say this is because like okay techniques fit this in this last bucket and one of the things that I think biggest problems is like people sometimes start there and it doesn't make any sense because you say oh do I need BM25 or do I need like uh vector vector rich people it's like I don't know what what are you trying to do and what is your query says and where are things failing because many times you actually don't need these things and you end up implementing them it doesn't make a lot of sense anyway so usually the thing I say is like what I call complexity adjusted impact or you know stay lazy uh in a sense like always look at what's broken and if it's not broken don't fix it and if it is broken do fix it. Uh and we'll go through a lot of techniques today but like this is a good way to think about them. It's just a cluster. It's a catalog of stuff. The most important two columns are the ones to the right difficulty and impact and if it's easy go ahead and try it. And most times like BM25 BM25 is pretty easy. You should absolutely try it and does like you know show up your quality quite a bit. Um but you know should I build like custom embeddings for retrieval? Like I don't know. 
Relevance score: -8.720  

        ## Input
        How to use agent for financial analysis  

        ## Output
        Write a natural, context-based answer.



----------------------------------------------------------------
-------------------- New Query at 2025-09-01T17:39:32.227684 -------------

        ## Task
        Answer the users query based only on the retrieved context.  
        Use chat history if relevant, but never go beyond the given context.  

        ## Context
        Source: https://www.youtube.com/watch?v=CQGuvf6gSrM
Content: [Music] Thanks everyone for being here. Um I'm going to give this talk mostly from the uh point of view of being a co-founder and chief scientist at Arthur AAI where for six years prior to joining Mosilla AI as CEO. Um, I do want to say Mozilla AI operates in the open source world where we're providing open source AI tooling and we're supporting the open source AI stack. Our end goal is to uh enable the open source community to be at the same table as a Sam Alman when talking about AI moving forward. So, if you're interested in that, that's not what this talk is going to be about, but we can talk about that offline. This talk is going to be about 2025 finally being the year of the evals. And as was written uh written as was spoken uh by my introduction, I've been in the space for a very long time. Arthur AI for example was in and still is in obser observability evaluation and security in both traditional ML and AI and then into the deep learning revolution and then into the genai revolution and then into the agentic revolution. And I think we're finally at the point where uh all of these companies are going to start seeing hockey stick growth which is exciting. So the thesis of this talk, one thing is that uh I I see a IML monitoring and evaluation as two sides of the same sword or ruler, right? You can't do monitoring or observability without being able to measure and measurement is uh the core functionality for evaluation. This was not top of mind really with the seuite uh until two things happened concurrently. One is AI became a thing that people who aren't a CIO or CTO could understand. So the CEOs, the CFOs, the CISOs began to understand it basically when ChatGBT came out. And simultaneously there was a perfectly timed budget freeze across enterprise at least in the US that happened due to a fear of an impending recession. So this is right before chat GBT launched. This is like October, November when most enterprises would set up budget for the next year. At that time there was a freeze except for money that could be opened up for a specific pet project and that PET project because CEOs and CFOs then knew about it uh was Gen AI. So that happened and then now we have the final sort of uh vertex on this triangle which is going to force evaluation to be top of mind this year which is that we have systems that are now acting for uh humans acting for teams as opposed to just providing inputs into larger systems. So these three things together are as you saw with Brain Trust, as we're seeing at Arthur, as you're seeing with like Arise AI or Galileo, other big players in this space, we're starting to see big takeoff because of this. Cool. 
Relevance score: -8.062

Source: https://www.youtube.com/watch?v=SbUxRluVRwk&t=167s
Content: We start off with defining use cases. We want to go through and integrate with products, right? We're not doing green field everywhere. We want to measure ROI, find the right data. We want to test and refine workflows, build the interfaces we need for success, sell this product, and make our customers care. And it's not one person who does this job. You can't just say, "AI researchers, go make me $10 million from this product unless a very specific niche. So this means your success is not one job unless you're a founder, but we'll skip that. So the goal here is that you need to have a comprehensive AI team and you need to figure out how are you going to structure that. And the thing that we need to remember is that companies aren't just one team. It's not just my AI team owns this small segment, this deployment or whatever. Otherwise, you ship your org chart and you get some weird product behaviors. So identify to yourself what is your bottleneck? What is stopping you from achieving success? Is it shipping features? Is it acquiring users? Is it retaining users? Are you monetizing correctly? Are there scalability issues? Are there reliability and observability issues? Right? All of us have probably run into these things as we are deploying AI products. So, we need to make sure we can prioritize all these things and hire accordingly. And these are all questions that you need to answer when building an AI team. The key takeaway here is what kind of team do you need? and only you know that answer. Let's talk about generalists and why I think they're important. So in 2021 I was building uh first machine learning team and I adopted an approach where we hired generalists. We supported them by automation across the board. So at the time I was hired to a conversational AI company working on a platform. Sorry, let me rephrase that. AI agent building platform. Just wanted to make sure you guys understood what that meant. And I was hired with the mandate of we want ML. That was my job description. Change that. We want AI. So after working with the business teams and leadership team, the this was the the final set of goals we set. We want to serve hundreds of thousands of concurrent models. It needs to be multi-dommain. It has to be low cost and we want to support real-time training and serving. Those are some tough goals. 
Relevance score: -8.166

Source: https://www.youtube.com/watch?v=SbUxRluVRwk&t=167s
Content: Now, in certain domains, the best tech is essential, right? So, if you're working at OpenAI or model provider, Anthropic Google or some of the startups, you want the best team who's working on that because that is a product as we covered. So here I'll I'll propose a wager for you. Are people here familiar with Pascal's wager philosophy? I'll give you the successor of that Ampere's wager if you're familiar with graphics card architectures. Here's your trade. You trade your team for five researchers from the top labs. And maybe you need to throw in some cash and first round picks for that as well. But do you make this trade? Do you trade your team that has domain knowledge has worked in the area for five AI researchers? I want you to think about that. So we go back to the question of what does an AI team need to do? There's a lot of stuff, right? 
Relevance score: -9.838  

        ## Input
        How to use agent for financial analysis  

        ## Output
        Write a natural, context-based answer.



----------------------------------------------------------------
-------------------- New Query at 2025-09-01T17:40:51.128310 -------------

        ## Task
        Answer the users query based only on the retrieved context.  
        Use chat history if relevant, but never go beyond the given context.  

        ## Context
        Source: https://www.youtube.com/watch?v=CQGuvf6gSrM
Content: [Music] Thanks everyone for being here. Um I'm going to give this talk mostly from the uh point of view of being a co-founder and chief scientist at Arthur AAI where for six years prior to joining Mosilla AI as CEO. Um, I do want to say Mozilla AI operates in the open source world where we're providing open source AI tooling and we're supporting the open source AI stack. Our end goal is to uh enable the open source community to be at the same table as a Sam Alman when talking about AI moving forward. So, if you're interested in that, that's not what this talk is going to be about, but we can talk about that offline. This talk is going to be about 2025 finally being the year of the evals. And as was written uh written as was spoken uh by my introduction, I've been in the space for a very long time. Arthur AI for example was in and still is in obser observability evaluation and security in both traditional ML and AI and then into the deep learning revolution and then into the genai revolution and then into the agentic revolution. And I think we're finally at the point where uh all of these companies are going to start seeing hockey stick growth which is exciting. So the thesis of this talk, one thing is that uh I I see a IML monitoring and evaluation as two sides of the same sword or ruler, right? You can't do monitoring or observability without being able to measure and measurement is uh the core functionality for evaluation. This was not top of mind really with the seuite uh until two things happened concurrently. One is AI became a thing that people who aren't a CIO or CTO could understand. So the CEOs, the CFOs, the CISOs began to understand it basically when ChatGBT came out. And simultaneously there was a perfectly timed budget freeze across enterprise at least in the US that happened due to a fear of an impending recession. So this is right before chat GBT launched. This is like October, November when most enterprises would set up budget for the next year. At that time there was a freeze except for money that could be opened up for a specific pet project and that PET project because CEOs and CFOs then knew about it uh was Gen AI. So that happened and then now we have the final sort of uh vertex on this triangle which is going to force evaluation to be top of mind this year which is that we have systems that are now acting for uh humans acting for teams as opposed to just providing inputs into larger systems. So these three things together are as you saw with Brain Trust, as we're seeing at Arthur, as you're seeing with like Arise AI or Galileo, other big players in this space, we're starting to see big takeoff because of this. Cool. 
Relevance score: -8.046

Source: https://www.youtube.com/watch?v=SbUxRluVRwk&t=167s
Content: We start off with defining use cases. We want to go through and integrate with products, right? We're not doing green field everywhere. We want to measure ROI, find the right data. We want to test and refine workflows, build the interfaces we need for success, sell this product, and make our customers care. And it's not one person who does this job. You can't just say, "AI researchers, go make me $10 million from this product unless a very specific niche. So this means your success is not one job unless you're a founder, but we'll skip that. So the goal here is that you need to have a comprehensive AI team and you need to figure out how are you going to structure that. And the thing that we need to remember is that companies aren't just one team. It's not just my AI team owns this small segment, this deployment or whatever. Otherwise, you ship your org chart and you get some weird product behaviors. So identify to yourself what is your bottleneck? What is stopping you from achieving success? Is it shipping features? Is it acquiring users? Is it retaining users? Are you monetizing correctly? Are there scalability issues? Are there reliability and observability issues? Right? All of us have probably run into these things as we are deploying AI products. So, we need to make sure we can prioritize all these things and hire accordingly. And these are all questions that you need to answer when building an AI team. The key takeaway here is what kind of team do you need? and only you know that answer. Let's talk about generalists and why I think they're important. So in 2021 I was building uh first machine learning team and I adopted an approach where we hired generalists. We supported them by automation across the board. So at the time I was hired to a conversational AI company working on a platform. Sorry, let me rephrase that. AI agent building platform. Just wanted to make sure you guys understood what that meant. And I was hired with the mandate of we want ML. That was my job description. Change that. We want AI. So after working with the business teams and leadership team, the this was the the final set of goals we set. We want to serve hundreds of thousands of concurrent models. It needs to be multi-dommain. It has to be low cost and we want to support real-time training and serving. Those are some tough goals. 
Relevance score: -8.674

Source: https://www.youtube.com/watch?v=SbUxRluVRwk&t=167s
Content: Now, in certain domains, the best tech is essential, right? So, if you're working at OpenAI or model provider, Anthropic Google or some of the startups, you want the best team who's working on that because that is a product as we covered. So here I'll I'll propose a wager for you. Are people here familiar with Pascal's wager philosophy? I'll give you the successor of that Ampere's wager if you're familiar with graphics card architectures. Here's your trade. You trade your team for five researchers from the top labs. And maybe you need to throw in some cash and first round picks for that as well. But do you make this trade? Do you trade your team that has domain knowledge has worked in the area for five AI researchers? I want you to think about that. So we go back to the question of what does an AI team need to do? There's a lot of stuff, right? 
Relevance score: -9.958

Source: https://www.youtube.com/watch?v=w9u11ioHGA0
Content: Uh I'll flash this towards the end as well with pi.ai-talk. Uh it should point to the slides that we're going through. And as I mentioned, plain English, no hype, no buzz, uh no debates, no like all right. So how to think about techniques before we go techniques and get into the weeds of it like why does this even matter and the way we always think about it is like always start with outcomes. You're always trying to solve some product problem. Uh and generally the best way to visualize something like this. you have a certain quality bar you want to reach and there were a very interesting talk this this week about like you know benchmarks aren't really helpful and absolutely eval are helpful you're trying to launch a CRM agent and you sort of have a launch bar like a place where you feel comfortable that you can actually put it out into the world uh and techniques fit somewhere here you have that like kind of end metric and you're trying to like come up with different ways to shore up the quality and those ways are like sort of the techniques there and you know this is sort of your own personal benchmark you start with some of the easy the easy the easy bars you want to hit and then there's like medium benchmarks and hard benchmarks. So these are query sets you're setting up. Uh and then you know depending on what you want to reach and in at what time frame uh then you end up trying different things. Uh and this is what we call like quality engineering loop. You sort of like baseline yourself. Okay you want to achieve you know you want CRM and this is the easy query set and your quality is there. Uh just through the simplest way you can try it. Do a loss analysis. Okay what's broken? There were a lot of eval talks this week and then what we call quality engineering. Now the reason I I I I say this is because like okay techniques fit this in this last bucket and one of the things that I think biggest problems is like people sometimes start there and it doesn't make any sense because you say oh do I need BM25 or do I need like uh vector vector rich people it's like I don't know what what are you trying to do and what is your query says and where are things failing because many times you actually don't need these things and you end up implementing them it doesn't make a lot of sense anyway so usually the thing I say is like what I call complexity adjusted impact or you know stay lazy uh in a sense like always look at what's broken and if it's not broken don't fix it and if it is broken do fix it. Uh and we'll go through a lot of techniques today but like this is a good way to think about them. It's just a cluster. It's a catalog of stuff. The most important two columns are the ones to the right difficulty and impact and if it's easy go ahead and try it. And most times like BM25 BM25 is pretty easy. You should absolutely try it and does like you know show up your quality quite a bit. Um but you know should I build like custom embeddings for retrieval? Like I don't know. 
Relevance score: -8.720  

        ## Input
        How to use agent for financial analysis  

        ## Output
        Write a natural, context-based answer.



----------------------------------------------------------------
-------------------- New Query at 2025-09-01T17:49:47.994814 -------------

        ## Task
        The user's query: How to use agent for financial analysis

        Answer the users query based only on the retrieved context.  
        Use chat history if relevant, but never go beyond the given context.
        Write a natural, context-based answer.


        ## Context
        Source: https://www.youtube.com/watch?v=CQGuvf6gSrM
Content: [Music] Thanks everyone for being here. Um I'm going to give this talk mostly from the uh point of view of being a co-founder and chief scientist at Arthur AAI where for six years prior to joining Mosilla AI as CEO. Um, I do want to say Mozilla AI operates in the open source world where we're providing open source AI tooling and we're supporting the open source AI stack. Our end goal is to uh enable the open source community to be at the same table as a Sam Alman when talking about AI moving forward. So, if you're interested in that, that's not what this talk is going to be about, but we can talk about that offline. This talk is going to be about 2025 finally being the year of the evals. And as was written uh written as was spoken uh by my introduction, I've been in the space for a very long time. Arthur AI for example was in and still is in obser observability evaluation and security in both traditional ML and AI and then into the deep learning revolution and then into the genai revolution and then into the agentic revolution. And I think we're finally at the point where uh all of these companies are going to start seeing hockey stick growth which is exciting. So the thesis of this talk, one thing is that uh I I see a IML monitoring and evaluation as two sides of the same sword or ruler, right? You can't do monitoring or observability without being able to measure and measurement is uh the core functionality for evaluation. This was not top of mind really with the seuite uh until two things happened concurrently. One is AI became a thing that people who aren't a CIO or CTO could understand. So the CEOs, the CFOs, the CISOs began to understand it basically when ChatGBT came out. And simultaneously there was a perfectly timed budget freeze across enterprise at least in the US that happened due to a fear of an impending recession. So this is right before chat GBT launched. This is like October, November when most enterprises would set up budget for the next year. At that time there was a freeze except for money that could be opened up for a specific pet project and that PET project because CEOs and CFOs then knew about it uh was Gen AI. So that happened and then now we have the final sort of uh vertex on this triangle which is going to force evaluation to be top of mind this year which is that we have systems that are now acting for uh humans acting for teams as opposed to just providing inputs into larger systems. So these three things together are as you saw with Brain Trust, as we're seeing at Arthur, as you're seeing with like Arise AI or Galileo, other big players in this space, we're starting to see big takeoff because of this. Cool. 
Relevance score: -10.097

Source: https://www.youtube.com/watch?v=3qYd5XfoojQ
Content: When you're crafting email responses or when you missed a meeting to summarize, you know, what you missed or when you want to follow do a follow up on when you want to catch up on the most important things and out of the meeting. Uh it is becoming easy and handy so that you you don't have to be there everywhere to listen to all the ideas and be part be participating in all those things. I think it is enabling you, it is um helping you to be more productive, right? So that you can um delegate this repetitive task to this uh tools and focus on the strategic uh task. So that is at the personal level. But how it is impacting in a broader scale, right? These are the four things that we listed out. But if you look at it in general, you know, I would probably lean in on lean in on the innovation and customer growth to begin with, right? Um so anything and everything that we do h will be our own work what we can do from a customer standpoint. What is the impact that we can make from a customer standpoint right? Um like for an example personalization aspect for a customer or even if you want to craft a a personalized email um um rather than doing a traditional rather than doing it in a traditional way right same thing with uh faster decision making right like I said um it is improving your productivity aspect it is also helping you to make decisions faster um you know whether it is through actionable insights or whether it's through real-time dashboards whether it is to summarize the complex data where you have spend hours and hours um to come up with a recommendation or even use it use it as an uh assistant for you right if you will to make some judgment or to some to to clarify something right based on what you want to do. So those are the things that I would highlight. I I know we are going to dig into each of those things but I want to highlight right it it end of the day it matters on where you want to use it what is the impact that it's going to create for you and also the organization right since I mentioned the the personal the personal features that I'm relying upon um likewise I'm sure you may have multiple scenarios that you may have experienced with chat JPD and perplexity and if you're playing with comet it is fascinating to me on how we work and how it is changing our mindset right you know I'll give you Another example, um I have two boys. Um you know, we put them in multiple classes. I'm sure you know if you're a if you have kids, you might be facing the same problem. So you will have classes every day. Uh multiple classes and you need to figure out like you know when to put in what classes. This is what I did. I all I did was like you know I I listed out all the classes and I told them these are my restrictions. These are my time slots that are available. And I also mentioned like you know how many um uh minutes I need to put for each of those classes and I told okay hey here are the things that I cannot do within this day for an example Wednesday I don't want to be in at swimming or maybe like look at the temperature forecast and plan accordingly right it gave me a plan to say okay Monday you can send your kids for these two classes Tuesday for these two and Wednesday this one Friday these two done you previously you have to have an Excel spreadsheet open feed in that information and see how you can plan it. You don't need to even do that right now. All you need to do is just feed the information, put the context. It is giving you a plan right now. >> Yeah. 
Relevance score: -10.156

Source: https://www.youtube.com/watch?v=gmTHs5T_YAE
Content: [Music] Uh hello everyone. Thank you so much for being here for uh sticking around for this talk. Um I'm going to be talking about optimizing inference for voice models in production. I'm going to be talking mostly about the runtime component but also just a little bit on the infrastructure side. Um just a quick introduction. Um I'm Philip from B 10. Base 10 is a model inference platform. Uh we run production workloads for a wide variety of AI native startups and enterprises. Um I'm based in here in SF. Um I actually just moved here. It's really awesome. My favorite part about being in SF is much better sports teams than I had in Chicago. Um and uh one of my favorite voice models is Orpheus TTS. Um which we're going to be talking about a whole bunch today. Um quick agenda. So we're going to talk about TTS model architecture like what is a text to speech model actually when you look on the config in hugging face. Uh what sort of performance metrics are we looking at? What sort of optimization techniques can we do to make the model better? Um how do we measure whether or not we succeeded? And then finally, what can we do on the infrastructure and client code to not shoot ourselves in the foot after doing a ton of runtime work and then just adding all that latency back by not doing our client code correctly. So architecture, this is one of the things I've been learning this year which has been pretty great uh to to realize it's made life a lot simpler at the runtime level. Now this is wrong like the the the thing up here uh that that I'm going to say is that like everything is an LLM. uh that is that is wrong but it's useful. Um there's kind of like two types of models. There's autogressive transformers models that are LLM or very LLM adjacent. Um you see this in embeddings. You see this in transcription with stuff like whisper TTS um is another example. You also have the more like diffuser image type models. Um which is like a very different optimization problem. But something that's cool is because TTS models are so architecturally similar to LLMs or in many cases derived directly from LLMs, we can access the rich ecosystem of LLM tooling and use it to make TTS models better. So the TTS model that we're going to be using and as an example all day um is Orpheus TTS. We're using it for two reasons. 
Relevance score: -10.444

Source: https://www.youtube.com/watch?v=w9u11ioHGA0
Content: At sufficient complexity, things will keep breaking. So now the thing that breaks with even custom semantics is user preference. Uh because even when you get to all this, okay, you're saying I'm doing relevance and I'm doing price signals and merchant signals. I'm doing everything. I now I know the shopping domain. Now you don't know the shopping domain because now users are using your product. They're clicking on stuff you thought they're not going to click on and they're clicking they're not clicking on thoughts on things you thought they were going to click on. Uh and this is where you need to like bring in the click signal, thumbs up, thumbs down signal. Now um these things get very complex. So we're not going to talk about how to implement them. uh just because again in this case for example you have to build a click-through uh signal prediction signal and then you take that signal and then you combine it with all your other signals. So now if you look at your ranking function it's doing okay I want it to be relevant I wanted to have this like semi-structured price signal and like query understanding related to that plus I want to get the user preference and that and then you take all these signals and you add them and that becomes your ranking score. So it becomes a very balanced function. And this is how you go from like oh it's just relevance to oh no it's not just relevance to oh no it's not just relevance and and my semantics and my user preferences all rolled up into one. I'll mention two more things. Uh you calling the wrong queries. This happening a lot because this go this goes into more orchestration and you're trying to do complex things. Uh especially now when you have agents uh and you're telling them to use a certain tool. This is happening quite a bit because there is an impedance mismatch uh between what the search engine expects right let's say you tune the search engine and expects like keyword queries or expects uh you know even like more complex queries but you cannot describe all of that to the LM and the LM is reasoning about your application and then making queries by itself and this is a big problem so one thing that we've seen many companies do we've done this also at Google you actually take more control of the actual orchestration so you take the big query and you make n smaller queries out of Uh I took the screenshot from AI mode in Google and it's it's very brief. You have to catch it because after after the animation goes away but you see it's actually it's making X queries. It's making 15 queries. It's making 20 queries. Um so what we call fan out take very complex thing try to figure out what all the subqueries in it and then fan them out. Now you might think hey why isn't the LM doing it? The LM is kind of doing it but the LM doesn't know about your tool. It doesn't know enough about your search engine. Uh I love MCP but I'm not a big believer that you can actually teach the LLM and like just through prompting what to expect from the search on the other back end. This is why people still like oh is it agent autonomous? Do I need to do workflows? This is very very complicated. 
Relevance score: -9.208

Source: https://www.youtube.com/watch?v=w9u11ioHGA0
Content: [Music] I'll I'll just give you all a little bit of context. So uh my co-founder and I and a lot of our team were actually working on Google search and then we left and like started Pyabs and uh I I loved I love the exit talk and like we're all nerds for information retrieval and search and uh so this is going to be a little bit of that. Uh just going to go through a whole bunch of ways you can actually show up and improve your rack systems. Uh I think one thing that I personally uh sometimes struggle with is there's a lot of talk about things sometimes like too much in the buzzed like oh specific techniques and you can do RL this way and you can tune the model this way and it's like doesn't help me orient in the space like what are all these things and how do I like hang on them uh or you have the complete opposite which is like a whole bunch of buzzwords and hype and such and like rag is dead no rag is not dead is like agents like wait what like uh so just you know I think a lot of what I'll do today is just uh what I call like plain English uh just trying to like set up a framework right like very centered around like okay if you are trying to show up the quality of your system how do you do that and then where do all the things you hear about like day in day out like fit uh and then just how to approach that and give a lot of examples I think one thing that I always love and we always did in Google we always do in pyabs uh is just like look at things look at cases look at queries see what's working see what's not working and that's really the essence of like quality engineering as we used to call it at Google if you do want the slides there's like 50 slides and I said my a challenge for myself to go through 50 slides in 19 minutes. Uh but you can catch the slides here if you want. 
Relevance score: -9.567

Source: https://www.youtube.com/watch?v=CQGuvf6gSrM
Content: So I'll leave it with some time for questions. Um I did mention Mozilla is not firmly in the evaluation space. We do have a very nice open source not monetized at all what we're calling a light LLM for multi-agent systems. So if you're playing around with different multi-agent system frameworks check out any agent. We implement a lot of them for you under a unified interface. So for people in this room that might be a fun project to play around with. So thank you. I'll uh have three three minutes for questions. 
Relevance score: -9.755

Source: https://www.youtube.com/watch?v=w9u11ioHGA0
Content: Uh and it will take a while for this to be solved because again it's unclear where the boundary is. Is it uh is it the search engine should be able to handle more complex things and then the LLM will just throw anything its way or is it the other way around? the LM has to have more information about what the search engine can support so it can tailor it and right now you need control because the quality is still not there. Uh so this looks like this. Um if you have sort of like this assistant input and you're turning it in these narrow queries like for example was David working on this has very very specific semantics and it's more like oh JC is David Slack threads David. Uh and it's very very hard to know without knowing enough about your application that these are the queries that matter and not on the the ones on the left hand side. And if you send the thing on the left hand side to a search engine, it will absolutely tip over unless it understands your domain. And this is where like you know you need to calibrate the boundary. Okay. 
Relevance score: -9.685

Source: https://www.youtube.com/watch?v=xnXqpUW_Kp8
Content: So if I was conscious I would have thought this was cool. Um okay and now our story our now our story uh skips 23 years to 2021. Um by this point I was conscious barely and uh uh and I I noticed that you know GBD3 had recently come out and it was this magical thing that you could input a whole paragraph explaining exactly what you want uh and it would really understand the subtleties of your language and give you an output that exactly matched. Um, and it's hard to remember how magical this was, but it was really magical in 2021. And at the same time, I noticed there was Google, which you know, you type in a simple query like shirts without stripes and it would give you shirts with stripes, which is crazy. Uh, it like doesn't understand the word without u because it's doing a keyword comparison algorithm. And so I decided that for the next at least 10 years I'm going to devote myself to building a search engine that combines the technology of GB3 uh to with a search engine to make a search engine that actually understands what you're saying uh at a deep level and understands all the documents on the web at a deep level and gives you exactly what you ask for. This is a very big idea and we're working we've been working on it for four years and uh a lot of progress but it would change the world if you actually solve this problem. 
Relevance score: -10.044

Source: https://www.youtube.com/watch?v=3qYd5XfoojQ
Content: Hello. Hello everyone. Good evening. Good evening to everyone joining in. So, I'm just going to set up my screen share. We're also going to be streaming on uh LinkedIn and YouTube. You guys can let us know your introductions where you're joining from just to get started. My name is Aperna. I'm going to be your host for today and it's it's going to be an interesting uh conversation today about how um leaders for leaders it's important to leverage AI and it's no more optional for us to uh consider AI as a separate entity. We've got an interesting session lined up. In case you're just joining, let us know where you're joining us from. Let us know your name and the city. Yeah. All right. All right. Krishna Gant, Suratish. Hi Anyone else can maybe want to quickly introduce yourselves? Hello Raakant. So, we're also live on uh LinkedIn and on YouTube. So, in case you're joining us from either of these places, just leave leave us your introductions on the live chat. That's where you'll be interacting with us. Hello, Fizza. Good evening as well. Okay. So, uh like I said today, it's it's actually going to be about how you can use AI, the role of AI in in leadership and how this is changing things for leaders and what is what is exactly AI doing such that leaders are supposed to do things differently. And with that, we'll also help you understand how the senior leadership program helps you with exactly this in each business function. Um, so you can be an aspiring leader. You might be in a leadership position already. Um, so you know, you you kind of be able to relate to what we're saying regardless of your work experience. Um, so in case you you're interested in learning more, we'll also talk about how the learning experience of the course is, the eligibility to take up this course and also you know how you can be a part of our upcoming cohort. We'll wrap up the session with the Q&A. So in case you have questions for Anand uh do make sure to put them on the Q&A box um and ensure that you're also interacting during the session you know kind of keep it interactive instead of making it u we don't want to make it a unidirectional oneway sort of a session right okay yeah so just so we know you a little better we know you we know from your uh you know your name we know your city I just want to know what your background is so I'm just going to launch launch a very quick poll and Anand you should be able to see the results shortly. Um so just let us know which business function you're a part of, what's your current job role, what's your background and what's your overall work experience as well. Yeah. So just launch this poll and in case people on LinkedIn and YouTube are wondering what poll this is, just let us know your overall work experience, how many years of experience you carry and which business function you are a part of. That's that's what we're looking for. 
Relevance score: -10.951  

----------------------------------------------------------------
-------------------- New Query at 2025-09-01T17:51:15.241058 -------------

        ## Task
        The user's query: How to use agent for financial analysis

        Answer the users query based only on the retrieved context.  
        Use chat history if relevant, but never go beyond the given context.
        Write a natural, context-based answer.


        ## Context
        Source: https://www.youtube.com/watch?v=CQGuvf6gSrM
Content: [Music] Thanks everyone for being here. Um I'm going to give this talk mostly from the uh point of view of being a co-founder and chief scientist at Arthur AAI where for six years prior to joining Mosilla AI as CEO. Um, I do want to say Mozilla AI operates in the open source world where we're providing open source AI tooling and we're supporting the open source AI stack. Our end goal is to uh enable the open source community to be at the same table as a Sam Alman when talking about AI moving forward. So, if you're interested in that, that's not what this talk is going to be about, but we can talk about that offline. This talk is going to be about 2025 finally being the year of the evals. And as was written uh written as was spoken uh by my introduction, I've been in the space for a very long time. Arthur AI for example was in and still is in obser observability evaluation and security in both traditional ML and AI and then into the deep learning revolution and then into the genai revolution and then into the agentic revolution. And I think we're finally at the point where uh all of these companies are going to start seeing hockey stick growth which is exciting. So the thesis of this talk, one thing is that uh I I see a IML monitoring and evaluation as two sides of the same sword or ruler, right? You can't do monitoring or observability without being able to measure and measurement is uh the core functionality for evaluation. This was not top of mind really with the seuite uh until two things happened concurrently. One is AI became a thing that people who aren't a CIO or CTO could understand. So the CEOs, the CFOs, the CISOs began to understand it basically when ChatGBT came out. And simultaneously there was a perfectly timed budget freeze across enterprise at least in the US that happened due to a fear of an impending recession. So this is right before chat GBT launched. This is like October, November when most enterprises would set up budget for the next year. At that time there was a freeze except for money that could be opened up for a specific pet project and that PET project because CEOs and CFOs then knew about it uh was Gen AI. So that happened and then now we have the final sort of uh vertex on this triangle which is going to force evaluation to be top of mind this year which is that we have systems that are now acting for uh humans acting for teams as opposed to just providing inputs into larger systems. So these three things together are as you saw with Brain Trust, as we're seeing at Arthur, as you're seeing with like Arise AI or Galileo, other big players in this space, we're starting to see big takeoff because of this. Cool. 
Relevance score: -6.769

Source: https://www.youtube.com/watch?v=SbUxRluVRwk&t=167s
Content: We start off with defining use cases. We want to go through and integrate with products, right? We're not doing green field everywhere. We want to measure ROI, find the right data. We want to test and refine workflows, build the interfaces we need for success, sell this product, and make our customers care. And it's not one person who does this job. You can't just say, "AI researchers, go make me $10 million from this product unless a very specific niche. So this means your success is not one job unless you're a founder, but we'll skip that. So the goal here is that you need to have a comprehensive AI team and you need to figure out how are you going to structure that. And the thing that we need to remember is that companies aren't just one team. It's not just my AI team owns this small segment, this deployment or whatever. Otherwise, you ship your org chart and you get some weird product behaviors. So identify to yourself what is your bottleneck? What is stopping you from achieving success? Is it shipping features? Is it acquiring users? Is it retaining users? Are you monetizing correctly? Are there scalability issues? Are there reliability and observability issues? Right? All of us have probably run into these things as we are deploying AI products. So, we need to make sure we can prioritize all these things and hire accordingly. And these are all questions that you need to answer when building an AI team. The key takeaway here is what kind of team do you need? and only you know that answer. Let's talk about generalists and why I think they're important. So in 2021 I was building uh first machine learning team and I adopted an approach where we hired generalists. We supported them by automation across the board. So at the time I was hired to a conversational AI company working on a platform. Sorry, let me rephrase that. AI agent building platform. Just wanted to make sure you guys understood what that meant. And I was hired with the mandate of we want ML. That was my job description. Change that. We want AI. So after working with the business teams and leadership team, the this was the the final set of goals we set. We want to serve hundreds of thousands of concurrent models. It needs to be multi-dommain. It has to be low cost and we want to support real-time training and serving. Those are some tough goals. 
Relevance score: -8.644

Source: https://www.youtube.com/watch?v=SbUxRluVRwk&t=167s
Content: Now, in certain domains, the best tech is essential, right? So, if you're working at OpenAI or model provider, Anthropic Google or some of the startups, you want the best team who's working on that because that is a product as we covered. So here I'll I'll propose a wager for you. Are people here familiar with Pascal's wager philosophy? I'll give you the successor of that Ampere's wager if you're familiar with graphics card architectures. Here's your trade. You trade your team for five researchers from the top labs. And maybe you need to throw in some cash and first round picks for that as well. But do you make this trade? Do you trade your team that has domain knowledge has worked in the area for five AI researchers? I want you to think about that. So we go back to the question of what does an AI team need to do? There's a lot of stuff, right? 
Relevance score: -9.770

Source: https://www.youtube.com/watch?v=gyjoXC8lzIw
Content: Okay, this is yet another big idea I'm sharing here on the channel with you. As you saw, I have a workflow that takes my morning debrief, it transcribes it, it adds additional thoughts, it formats it, it prioritizes things for me, and now this is just available for me to drag and drop every single day over and over and over. Okay, so think about how that stacks up, right? Think about as you build up these agentic workflows, think about the the lead. Think about the advantage you gain by using the right compute in the right agent architecture like the agentic drop zone. This codebase is of course going to be available to you. 
Relevance score: -9.649

Source: https://www.youtube.com/watch?v=3qYd5XfoojQ
Content: >> Yeah, I I would say in in current situation with with with evolving uh changes that are popping up in every minute, every second if you will, right? um the those days are gone where you um measure leadership or leaders capability in terms of how how many people you manage right it's all based on okay do you have a small team do you have a large team uh what is your title looks like what is your hierarchy where are you in your organization are you directing your task do you have the outcomes that you want to deliver are you able to influence the team right um and energy went into who reports to you right a lot of energy has been spent on like you who's reporting to you and recognition is always tied to like you know how many people you led um uh what are the operational things that you fixed you know how much bud budget control that you have like how are you efficiently managing the budget I think those days are gone in my view right um today in today's world everything is tied to an outcome are you really really delivering meaningful outcome for the organization right u it doesn't matter um whether you are a leader or whether you have the title Everyone has influence on what we need to do for an organization. Like it comes from ideas, expertise, collaboration, you know, you name it. Everything matters, right? Um it is always focusing focused on enabling others and also removing barriers for others for your team and making sure that how you how you can help the team to accelerate progress. So as a leader the the primary job is to how you uh influence the team how you enable others and also create followers so that you can make you can make the followers to leaders at some point right so energy energy a lot of energy goes into what difference you can make what impact you can make within the organization and how you can steer the team or rally the team to make sure that they are following the goals right or they are understanding the goals right and and are also treating that as their goals Right. Um and recognization is always tied to uh what is the impact that you are delivering? Not about team size. Uh not about how many people you need, not about how much budget you saved on a yearly basis. So it's a huge shift um compared to 15 20 years ago and I'm sure you might be seeing it on the ground. Um so I think there's a lot of nimleness that is happening in the leadership space as well, right? It's not about it's not only at the engineering level right given AI given the technology changes given the environment change given um you know the the things that that are happening in the communities it's all about what is the meaningful outcome and impact and how you can bring ideas and expertise and how you can collaborate within the organization to make a broader impact is what is going to matter and most importantly as a leader >> to me um you need to have a good clarity uh you need to have a good vision and you you need to have a good communication to explain that vision so that teams can understand the vision and the mission right that you want to march towards too. So those are the three three things I would probably ask you guys to look at clarity um vision and communicating the vision those are the three things I would rely on to make sure that you know that's a starting point for any leader to make sure that you know okay this is your path forward and bring the team along and then see how you can influence the team rally the team so that you can spend lot of energy in influencing the team to march towards that goal >> yeah I'm curious is I'm curious like I'm sure this is not something new for you guys, you know, probably you can put it in the chat. Um, with respect to what you just heard, uh, with respect to what you guys are experiencing or in a transition state or a transformation state, feel free to put it in the chat. >> Yeah, I mean, exactly. So, in caseh people are, you know, I mean a lot of you have actually more than 15 years of experience. So, you could share uh what changed from the time you started out to now as well. And you know if if what Anand says resonates with you about communicating outcome I think that's probably the biggest uh you know whatever kind of the most critical aspect uh that a leader needs to kind of ensure. So if you've got any thoughts you guys can put it on the chat and do not hesitate to share your thoughts. We've got people on LinkedIn who are also you know who've got MBA degrees from got a lot of experience as well. So you guys can share uh your thoughts as well, right? Um so let's maybe move on. Yeah, let's let's just move on now. We've got one comment on the chat who says in terms of outcomes, uh business value common item between the old and the new. Yeah, except that currently the team size is not really a sign of influence and progress. Absolutely, A Vijit. U exactly you're on point there. 
Relevance score: -10.751  

----------------------------------------------------------------
-------------------- New Query at 2025-09-01T17:57:38.495006 -------------

        ## Task
        The user's query: How to use agent for financial analysis

        Answer the users query based only on the retrieved context.  
        Use chat history if relevant, but never go beyond the given context.
        Write a natural, context-based answer.


        ## Context
        Source: https://www.youtube.com/watch?v=CQGuvf6gSrM
Content: [Music] Thanks everyone for being here. Um I'm going to give this talk mostly from the uh point of view of being a co-founder and chief scientist at Arthur AAI where for six years prior to joining Mosilla AI as CEO. Um, I do want to say Mozilla AI operates in the open source world where we're providing open source AI tooling and we're supporting the open source AI stack. Our end goal is to uh enable the open source community to be at the same table as a Sam Alman when talking about AI moving forward. So, if you're interested in that, that's not what this talk is going to be about, but we can talk about that offline. This talk is going to be about 2025 finally being the year of the evals. And as was written uh written as was spoken uh by my introduction, I've been in the space for a very long time. Arthur AI for example was in and still is in obser observability evaluation and security in both traditional ML and AI and then into the deep learning revolution and then into the genai revolution and then into the agentic revolution. And I think we're finally at the point where uh all of these companies are going to start seeing hockey stick growth which is exciting. So the thesis of this talk, one thing is that uh I I see a IML monitoring and evaluation as two sides of the same sword or ruler, right? You can't do monitoring or observability without being able to measure and measurement is uh the core functionality for evaluation. This was not top of mind really with the seuite uh until two things happened concurrently. One is AI became a thing that people who aren't a CIO or CTO could understand. So the CEOs, the CFOs, the CISOs began to understand it basically when ChatGBT came out. And simultaneously there was a perfectly timed budget freeze across enterprise at least in the US that happened due to a fear of an impending recession. So this is right before chat GBT launched. This is like October, November when most enterprises would set up budget for the next year. At that time there was a freeze except for money that could be opened up for a specific pet project and that PET project because CEOs and CFOs then knew about it uh was Gen AI. So that happened and then now we have the final sort of uh vertex on this triangle which is going to force evaluation to be top of mind this year which is that we have systems that are now acting for uh humans acting for teams as opposed to just providing inputs into larger systems. So these three things together are as you saw with Brain Trust, as we're seeing at Arthur, as you're seeing with like Arise AI or Galileo, other big players in this space, we're starting to see big takeoff because of this. Cool. 
Relevance score: -6.769

Source: https://www.youtube.com/watch?v=SbUxRluVRwk&t=167s
Content: We start off with defining use cases. We want to go through and integrate with products, right? We're not doing green field everywhere. We want to measure ROI, find the right data. We want to test and refine workflows, build the interfaces we need for success, sell this product, and make our customers care. And it's not one person who does this job. You can't just say, "AI researchers, go make me $10 million from this product unless a very specific niche. So this means your success is not one job unless you're a founder, but we'll skip that. So the goal here is that you need to have a comprehensive AI team and you need to figure out how are you going to structure that. And the thing that we need to remember is that companies aren't just one team. It's not just my AI team owns this small segment, this deployment or whatever. Otherwise, you ship your org chart and you get some weird product behaviors. So identify to yourself what is your bottleneck? What is stopping you from achieving success? Is it shipping features? Is it acquiring users? Is it retaining users? Are you monetizing correctly? Are there scalability issues? Are there reliability and observability issues? Right? All of us have probably run into these things as we are deploying AI products. So, we need to make sure we can prioritize all these things and hire accordingly. And these are all questions that you need to answer when building an AI team. The key takeaway here is what kind of team do you need? and only you know that answer. Let's talk about generalists and why I think they're important. So in 2021 I was building uh first machine learning team and I adopted an approach where we hired generalists. We supported them by automation across the board. So at the time I was hired to a conversational AI company working on a platform. Sorry, let me rephrase that. AI agent building platform. Just wanted to make sure you guys understood what that meant. And I was hired with the mandate of we want ML. That was my job description. Change that. We want AI. So after working with the business teams and leadership team, the this was the the final set of goals we set. We want to serve hundreds of thousands of concurrent models. It needs to be multi-dommain. It has to be low cost and we want to support real-time training and serving. Those are some tough goals. 
Relevance score: -8.644

Source: https://www.youtube.com/watch?v=SbUxRluVRwk&t=167s
Content: Now, in certain domains, the best tech is essential, right? So, if you're working at OpenAI or model provider, Anthropic Google or some of the startups, you want the best team who's working on that because that is a product as we covered. So here I'll I'll propose a wager for you. Are people here familiar with Pascal's wager philosophy? I'll give you the successor of that Ampere's wager if you're familiar with graphics card architectures. Here's your trade. You trade your team for five researchers from the top labs. And maybe you need to throw in some cash and first round picks for that as well. But do you make this trade? Do you trade your team that has domain knowledge has worked in the area for five AI researchers? I want you to think about that. So we go back to the question of what does an AI team need to do? There's a lot of stuff, right? 
Relevance score: -9.770

Source: https://www.youtube.com/watch?v=gyjoXC8lzIw
Content: Okay, this is yet another big idea I'm sharing here on the channel with you. As you saw, I have a workflow that takes my morning debrief, it transcribes it, it adds additional thoughts, it formats it, it prioritizes things for me, and now this is just available for me to drag and drop every single day over and over and over. Okay, so think about how that stacks up, right? Think about as you build up these agentic workflows, think about the the lead. Think about the advantage you gain by using the right compute in the right agent architecture like the agentic drop zone. This codebase is of course going to be available to you. 
Relevance score: -9.649

Source: https://www.youtube.com/watch?v=3qYd5XfoojQ
Content: >> Yeah, I I would say in in current situation with with with evolving uh changes that are popping up in every minute, every second if you will, right? um the those days are gone where you um measure leadership or leaders capability in terms of how how many people you manage right it's all based on okay do you have a small team do you have a large team uh what is your title looks like what is your hierarchy where are you in your organization are you directing your task do you have the outcomes that you want to deliver are you able to influence the team right um and energy went into who reports to you right a lot of energy has been spent on like you who's reporting to you and recognition is always tied to like you know how many people you led um uh what are the operational things that you fixed you know how much bud budget control that you have like how are you efficiently managing the budget I think those days are gone in my view right um today in today's world everything is tied to an outcome are you really really delivering meaningful outcome for the organization right u it doesn't matter um whether you are a leader or whether you have the title Everyone has influence on what we need to do for an organization. Like it comes from ideas, expertise, collaboration, you know, you name it. Everything matters, right? Um it is always focusing focused on enabling others and also removing barriers for others for your team and making sure that how you how you can help the team to accelerate progress. So as a leader the the primary job is to how you uh influence the team how you enable others and also create followers so that you can make you can make the followers to leaders at some point right so energy energy a lot of energy goes into what difference you can make what impact you can make within the organization and how you can steer the team or rally the team to make sure that they are following the goals right or they are understanding the goals right and and are also treating that as their goals Right. Um and recognization is always tied to uh what is the impact that you are delivering? Not about team size. Uh not about how many people you need, not about how much budget you saved on a yearly basis. So it's a huge shift um compared to 15 20 years ago and I'm sure you might be seeing it on the ground. Um so I think there's a lot of nimleness that is happening in the leadership space as well, right? It's not about it's not only at the engineering level right given AI given the technology changes given the environment change given um you know the the things that that are happening in the communities it's all about what is the meaningful outcome and impact and how you can bring ideas and expertise and how you can collaborate within the organization to make a broader impact is what is going to matter and most importantly as a leader >> to me um you need to have a good clarity uh you need to have a good vision and you you need to have a good communication to explain that vision so that teams can understand the vision and the mission right that you want to march towards too. So those are the three three things I would probably ask you guys to look at clarity um vision and communicating the vision those are the three things I would rely on to make sure that you know that's a starting point for any leader to make sure that you know okay this is your path forward and bring the team along and then see how you can influence the team rally the team so that you can spend lot of energy in influencing the team to march towards that goal >> yeah I'm curious is I'm curious like I'm sure this is not something new for you guys, you know, probably you can put it in the chat. Um, with respect to what you just heard, uh, with respect to what you guys are experiencing or in a transition state or a transformation state, feel free to put it in the chat. >> Yeah, I mean, exactly. So, in caseh people are, you know, I mean a lot of you have actually more than 15 years of experience. So, you could share uh what changed from the time you started out to now as well. And you know if if what Anand says resonates with you about communicating outcome I think that's probably the biggest uh you know whatever kind of the most critical aspect uh that a leader needs to kind of ensure. So if you've got any thoughts you guys can put it on the chat and do not hesitate to share your thoughts. We've got people on LinkedIn who are also you know who've got MBA degrees from got a lot of experience as well. So you guys can share uh your thoughts as well, right? Um so let's maybe move on. Yeah, let's let's just move on now. We've got one comment on the chat who says in terms of outcomes, uh business value common item between the old and the new. Yeah, except that currently the team size is not really a sign of influence and progress. Absolutely, A Vijit. U exactly you're on point there. 
Relevance score: -10.751  

----------------------------------------------------------------
-------------------- New Query at 2025-09-01T17:58:10.798661 -------------

        ## Task
        The user's query: How to use agent for financial analysis

        Answer the users query based only on the retrieved context.  
        Use chat history if relevant, but never go beyond the given context.
        Write a natural, context-based answer.


        ## Context
        Source: https://www.youtube.com/watch?v=CQGuvf6gSrM
Content: [Music] Thanks everyone for being here. Um I'm going to give this talk mostly from the uh point of view of being a co-founder and chief scientist at Arthur AAI where for six years prior to joining Mosilla AI as CEO. Um, I do want to say Mozilla AI operates in the open source world where we're providing open source AI tooling and we're supporting the open source AI stack. Our end goal is to uh enable the open source community to be at the same table as a Sam Alman when talking about AI moving forward. So, if you're interested in that, that's not what this talk is going to be about, but we can talk about that offline. This talk is going to be about 2025 finally being the year of the evals. And as was written uh written as was spoken uh by my introduction, I've been in the space for a very long time. Arthur AI for example was in and still is in obser observability evaluation and security in both traditional ML and AI and then into the deep learning revolution and then into the genai revolution and then into the agentic revolution. And I think we're finally at the point where uh all of these companies are going to start seeing hockey stick growth which is exciting. So the thesis of this talk, one thing is that uh I I see a IML monitoring and evaluation as two sides of the same sword or ruler, right? You can't do monitoring or observability without being able to measure and measurement is uh the core functionality for evaluation. This was not top of mind really with the seuite uh until two things happened concurrently. One is AI became a thing that people who aren't a CIO or CTO could understand. So the CEOs, the CFOs, the CISOs began to understand it basically when ChatGBT came out. And simultaneously there was a perfectly timed budget freeze across enterprise at least in the US that happened due to a fear of an impending recession. So this is right before chat GBT launched. This is like October, November when most enterprises would set up budget for the next year. At that time there was a freeze except for money that could be opened up for a specific pet project and that PET project because CEOs and CFOs then knew about it uh was Gen AI. So that happened and then now we have the final sort of uh vertex on this triangle which is going to force evaluation to be top of mind this year which is that we have systems that are now acting for uh humans acting for teams as opposed to just providing inputs into larger systems. So these three things together are as you saw with Brain Trust, as we're seeing at Arthur, as you're seeing with like Arise AI or Galileo, other big players in this space, we're starting to see big takeoff because of this. Cool. 
Relevance score: -8.046

Source: https://www.youtube.com/watch?v=SbUxRluVRwk&t=167s
Content: We start off with defining use cases. We want to go through and integrate with products, right? We're not doing green field everywhere. We want to measure ROI, find the right data. We want to test and refine workflows, build the interfaces we need for success, sell this product, and make our customers care. And it's not one person who does this job. You can't just say, "AI researchers, go make me $10 million from this product unless a very specific niche. So this means your success is not one job unless you're a founder, but we'll skip that. So the goal here is that you need to have a comprehensive AI team and you need to figure out how are you going to structure that. And the thing that we need to remember is that companies aren't just one team. It's not just my AI team owns this small segment, this deployment or whatever. Otherwise, you ship your org chart and you get some weird product behaviors. So identify to yourself what is your bottleneck? What is stopping you from achieving success? Is it shipping features? Is it acquiring users? Is it retaining users? Are you monetizing correctly? Are there scalability issues? Are there reliability and observability issues? Right? All of us have probably run into these things as we are deploying AI products. So, we need to make sure we can prioritize all these things and hire accordingly. And these are all questions that you need to answer when building an AI team. The key takeaway here is what kind of team do you need? and only you know that answer. Let's talk about generalists and why I think they're important. So in 2021 I was building uh first machine learning team and I adopted an approach where we hired generalists. We supported them by automation across the board. So at the time I was hired to a conversational AI company working on a platform. Sorry, let me rephrase that. AI agent building platform. Just wanted to make sure you guys understood what that meant. And I was hired with the mandate of we want ML. That was my job description. Change that. We want AI. So after working with the business teams and leadership team, the this was the the final set of goals we set. We want to serve hundreds of thousands of concurrent models. It needs to be multi-dommain. It has to be low cost and we want to support real-time training and serving. Those are some tough goals. 
Relevance score: -8.674

Source: https://www.youtube.com/watch?v=SbUxRluVRwk&t=167s
Content: Now, in certain domains, the best tech is essential, right? So, if you're working at OpenAI or model provider, Anthropic Google or some of the startups, you want the best team who's working on that because that is a product as we covered. So here I'll I'll propose a wager for you. Are people here familiar with Pascal's wager philosophy? I'll give you the successor of that Ampere's wager if you're familiar with graphics card architectures. Here's your trade. You trade your team for five researchers from the top labs. And maybe you need to throw in some cash and first round picks for that as well. But do you make this trade? Do you trade your team that has domain knowledge has worked in the area for five AI researchers? I want you to think about that. So we go back to the question of what does an AI team need to do? There's a lot of stuff, right? 
Relevance score: -9.958

Source: https://www.youtube.com/watch?v=w9u11ioHGA0
Content: Uh I'll flash this towards the end as well with pi.ai-talk. Uh it should point to the slides that we're going through. And as I mentioned, plain English, no hype, no buzz, uh no debates, no like all right. So how to think about techniques before we go techniques and get into the weeds of it like why does this even matter and the way we always think about it is like always start with outcomes. You're always trying to solve some product problem. Uh and generally the best way to visualize something like this. you have a certain quality bar you want to reach and there were a very interesting talk this this week about like you know benchmarks aren't really helpful and absolutely eval are helpful you're trying to launch a CRM agent and you sort of have a launch bar like a place where you feel comfortable that you can actually put it out into the world uh and techniques fit somewhere here you have that like kind of end metric and you're trying to like come up with different ways to shore up the quality and those ways are like sort of the techniques there and you know this is sort of your own personal benchmark you start with some of the easy the easy the easy bars you want to hit and then there's like medium benchmarks and hard benchmarks. So these are query sets you're setting up. Uh and then you know depending on what you want to reach and in at what time frame uh then you end up trying different things. Uh and this is what we call like quality engineering loop. You sort of like baseline yourself. Okay you want to achieve you know you want CRM and this is the easy query set and your quality is there. Uh just through the simplest way you can try it. Do a loss analysis. Okay what's broken? There were a lot of eval talks this week and then what we call quality engineering. Now the reason I I I I say this is because like okay techniques fit this in this last bucket and one of the things that I think biggest problems is like people sometimes start there and it doesn't make any sense because you say oh do I need BM25 or do I need like uh vector vector rich people it's like I don't know what what are you trying to do and what is your query says and where are things failing because many times you actually don't need these things and you end up implementing them it doesn't make a lot of sense anyway so usually the thing I say is like what I call complexity adjusted impact or you know stay lazy uh in a sense like always look at what's broken and if it's not broken don't fix it and if it is broken do fix it. Uh and we'll go through a lot of techniques today but like this is a good way to think about them. It's just a cluster. It's a catalog of stuff. The most important two columns are the ones to the right difficulty and impact and if it's easy go ahead and try it. And most times like BM25 BM25 is pretty easy. You should absolutely try it and does like you know show up your quality quite a bit. Um but you know should I build like custom embeddings for retrieval? Like I don't know. 
Relevance score: -8.720  

----------------------------------------------------------------
-------------------- New Query at 2025-09-01T18:00:45.667388 -------------

        ## Task
        The user's query: How to define workflow

        Answer the users query based only on the retrieved context.  
        Use chat history if relevant, but never go beyond the given context.
        Write a natural, context-based answer.


        ## Context
        Source: https://www.youtube.com/watch?v=gyjoXC8lzIw
Content: Okay. Gemini is freaking out. I actually have no idea what's going on there. So we're going to reset our drop zone there. Now we have this new cats file, right? So let's say cats enhanced. Let's go ahead and drop this in, right? I'll go ahead and copy it so we can reuse it. And then I'll drag and drop it into our image generation zone. This is going to kick off that agentic workflow. You can see all the configuration right here. If we open up that detailed prompt again, it's just reading that file, right? So it's going to read the file and then it's just going to do this operation based on the contents in the file, right? So important for every image prompt detailed and dropped file path. You can see we're referencing that variables. 
Relevance score: -7.414

Source: https://www.youtube.com/watch?v=CQGuvf6gSrM
Content: So right now, this is what's happening. Year of the agent, we all hear about it. We're hearing about it here at this conference. Agents are starting to make decisions and take actions, complex steps that lead toward an action, either autonomously or semi-autonomously. uh as a question on the last talk um um uh brought up you know bringing humans into the loop is still obviously a very good idea on many systems but we're getting closer and closer to full automation and agentic systems are going into deployment now okay and that's in enterprises that's in SMBs that's in pet projects and so on and what that means is by that last slide this is also the year that we need flash back uh to then up to like a year ago where every year we were asking hey is this the year of ML? Is this the year of evaluations? And prior to sort of these agentic systems coming out, we would have machine learning models basically spitting out numbers that would then be ingested into a more complex system. And that complexity uh would sort of erase uh the top of mind need to think about what's coming out of the model itself. Except for people in this audience, we know that's very important. But when it comes to decision makers, it would often get wrapped up into this sort of opaque box. And that meant that the ML part didn't really bubble up beyond like the CIO or whatever or the system was going into which meant that typically that year was not the year for EVEL because the need for eval was not obvious to the entire seuite. Cool. So let's take a quick step back in time. Before November 30th of 2022, the chatbt launch, uh, ML monitoring was certainly a thing, right? Like data science teams have long used statistical methods as part of larger systems to understand what's going on, right? This is this is core. Um, like I mentioned though, there's a tenuous connection to sort of downstream business KPIs. And at the end of the day, that's what gets your product bought in the enterprise is being able to make a sale about dollars saved or about dollars earned. So being able to connect machine learning the components specifically to a downstream business KPI there was a lot of lip service around a IML around the ROI from the seauite including CIO CEOs um but that was just lip service in our experience at least uh it was still basically selling into the CIO so basically it made it hard to sell outside of that now obviously this is a large space this has been happening since you know about 2012 I would say is when a IML monitoring really started up with like H2O and algorithmia and Seldon sort of the first generation of these companies coming around Y labs Aporeia Arise Arthur Galileo Fiddler protect AAI and so on and so on and so on. I've put the cuto off here at uh like mid 2022 sort of like before the genai revolution happened. There have obviously been companies founded after that. You know, we just saw Brain Trust talking as well. And then, you know, the big players here as well, right? Snowflake, data bricks, data dog, SageMaker, Vert.ex, you know, Microsoft's products and so on. So, people have been thinking about it, but it was never the thing. Again, rarely top of mind for the CEO, the CFO, and the CISO. It's never the issue. So when we would talk to people, it's always yes, we understand that we need this, but security is going to be a bigger issue or latency is going to be a bigger issue or some of these more traditional technology sort of problems are going to be the issue. It wasn't the machine learning model itself. So basically, you know, I do a lot of due diligence for venture capitalists as well now as a you know um a multi-time founder and so on. And basically every pitch deck in this space from like the mid2010s onward had a slide that said this is the year that a CEO is going to get fired because of an ML related screw-up. Uh and to my knowledge it just still hasn't happened. There have been some forwardinking leaders. So I have here a um the annual report from Jamie Diamond head of JPMC. Uh this came out in April of 2022. So it covered basically JPMC up through their fiscal year in 2021. Uh and he's talking about the spend that they have going into AI. But if you squint and you look at these numbers, they're still like comically small. So basically, one of these is in the consumer world, he makes the statement that from 2017 up through the end of 2021, they had put $100 million into a IMO, right? That's not a huge amount of money uh for JPMC. So keep sitting back in time pre-hat GPT and so on, but let's now flip to the macroeconomic side of things. So the economy started getting pretty dicey um right up until about chatbt um uh um launched right so that was the end of November for chatbt uh a lot of enterprise budgets are set in October November for the following year and toward mid to the end of 2022 there were very deep fears about an impending recession that didn't end up happening but those fears basically made it so that most enterprises either froze or shrunk their IT budgets for 20 uh 23 three. Okay. So, what that meant is were it not for particular tailwinds called JBT, we probably wouldn't have seen a lot of new technology being developed in the IT departments at these large enterprises in 2023. Right? 
Relevance score: -10.070

Source: https://www.youtube.com/watch?v=gyjoXC8lzIw
Content: Link in the description on the channel. We're going to be working to step outside the chat box. Think about all the repeat engineering work you do and think about how you can automate it with an agentic workflow. No matter what you do, stay focused and keep building.
Relevance score: -10.393

Source: https://www.youtube.com/watch?v=gyjoXC8lzIw
Content: Right? If we open this up, you can see five agent interaction patterns for engineering impact. This was the transcript for last week's video. Top three priorities, some key ideas. Human in the loop trade-offs, five level scalability framework, ad hoc prompts, reusable prompts, sub agents, MCP servers, full apps. We're in the age of agents, not LLMs. Agents provide exponential leverage. Great stuff. It's creating some extensions. So, our agent is extending the original ideas from whatever transcript we drop in. It's got some interesting questions for us. And then here we have the full-on transcript. Uh we're using OpenAI Whisper with the tiny model. Here's the opener for last week's video. If you're an engineer using the best tool for the job, you know this as a fact. Agents are not the future. They're already on our desks, blah blah blah, so on and so forth. Check out last week's video to see the five agent interaction patterns for engineering impact. We have all types of engineering work getting accomplished for us just based on dropping in the right file into the right zone. What's going on here? How were we able to specialize so many different workflows and set up this cool drop zone pattern? The first thing I want to highlight here, we have the entire application packed into an SFS, a single file script. Okay, so this one Python file using Astral UV is loading dependencies right inside the file and doing all of this work for us in some 700 lines of code. I love when I can pack in a ton of value into a single script. So, how does this work? This all operates with a set of prompts. You can see we have doclot here. It operates with our single file script and most importantly our drops YAML file. Let's break down how the drops work and how you can configure brand new drop zones. And as we're working through this, think through what types of workflows that are filebased that you could use this system for. Think about repeat workflows where you're just taking a file and you need to operate on it. All right, so this is what a drop zone configuration entry looks like. We of course have the name of the drop zone. We have our file patterns. This drop zone will only kick off if we drop in a text file. We have the prompt that we want to kick off. And so we can go ahead and just open up EchomD. And you can see what this prompt looks like exactly. We have our purpose, we have our variables, we have the workflow, and then we have our example output format here. And now we can kick this workflow off right now. Right, this is the simplest possible working version of the application. Here's echo.txt. Let's go ahead and copy this. So we can run this twice. Let's drag one into echo zone, right? So we're going to kick off that simple echo prompt. And let's drag one into I have a Gemini echo zone here as well. This is agent agnostic. So this works with any agent you want. Obviously cloud code is going to be the most powerful agent, but um I also have this workflow working with Gemini. We can drag and drop that in here. cloud code echo drop zone workflow. I'll execute the echo command workflow for the drop file. Simple Gemini CLI drop zone test and this is just the content that was in that file. Right? And if we go back over to this, you can see that's exactly what we wanted the output to be. If we open up, if we control Z that file and just take a look at that, you can see in this file we just have simple Gemini CLI agentic drop zone test and it just echoed that for us. Okay, so you can see how this works end to end with a simple example like this, right? We have a drop zone configured. This triggers a prompt. We're using a special file path string to make this prompt agent agnostic. If we were using cloud codes arguments like this, we would be locked into cloud code. And then we're specifying a, you know, a couple of variables here that get used throughout the actual workflow definition of this prompt. And as you create your agent workflows, this is a great prompt structure to use, right? Very concise, very simple. 
Relevance score: -6.021

Source: https://www.youtube.com/watch?v=gyjoXC8lzIw
Content: We are engineers so we can build out and leverage this technology better than anyone. Now when you're building right when you're operating of course there's nothing wrong with hopping in to a cloud code instance to a gemini CLI instance to a codeex instance whatever but you know more and more we should be automating work with these agentic workflows and the agentic drop zone is one way you can do this and to end it is so funny that prompt engineering is the most important skill any engineer can have now so whenever you see a prompt spend some extra time and really dig into it ask questions like why is this prompt structure like this. Look at interesting prompting techniques like we have here, right? We have variables that we're reusing throughout the workflow. We're embedding some XML so that we can specify specific sections and so that we can template out specific variables. So the rest of this single file agent to drop zone is actually fairly easy to understand. You can imagine what happens, right? Um if we go to main, all we're doing here checking environment variables and we're passing in our configuration file and then we're hitting run. The agent architecture is super simple. If we open up the readme, you can see exactly how this works. All right, files dropped. We're using Watchdog to detect events. If we match a pattern, we load the prompt template. If we don't, we ignore, replace the file path, we select our agent, and then we run whatever work we need to do with the agent, stream the response, and we display it. So, that's exactly how this works. And the key features here of the agentic drop zone is this, right? Single simple file script, figurable drop zones. It has the framework for running any agent. You can run them in parallel and you can run arbitrary agentic workflows. I've been playing with this quite a bit, thinking about all the workflows that I do on a daily basis with files. I recommend you do the same. All it takes is a single input file and then you can encode repeat solutions. Just dropping files into directories and we're getting work done, right? Super simple, really powerful idea. The chat UI is the first simplest and lowest hanging fruit. agentic drop zones or simply put, you know, these reactive directories let you automate file inputbased workflows. We're going to have more agents available to us. 
Relevance score: -4.243

Source: https://www.youtube.com/watch?v=gyjoXC8lzIw
Content: Check this out, right? So, we have a typical prompt format all of our sections. So we can just collapse and we can see we have create image variables and workflow. So this is a simple agentic prompt with these three key sections. Couple variables here drop file path. This will get replaced with whatever file we dropped in. Okay, we have our image output directory. And then we have an image model variable here. You can see we're using the replicate API model syntax for this. And then we're running a workflow. First you read the file, create the output directory, and then for every image detailed in this file here, do the following work. And so if we go into our generate images zone and go to the archived files and just pull back out cats, you can see here, you know, we just had three simple prompts. But we can update this file to really do and be anything, right? So drag and drop this file here. We can do something simple like this, right? We'll just go ahead and use cursor add seven more image generation prompts. And so this is where this becomes really really powerful. We can use agents on files. We can use LLMs to generate the input files at scale. We can detail whatever work we want done, whatever work that are drop zones accept as files. Right? So we have this enhanced cats.txt. And let's go ahead. You know, we can take a look at it again here. That looks great. 
Relevance score: -6.900

Source: https://www.youtube.com/watch?v=gyjoXC8lzIw
Content: You have the purpose at the top. Variables come next. LLM's agents are good enough to reference your variables throughout the rest of the prompts. You can see here we're doing that exactly. And then the workflow step of course breaks down step by step what your agent should do. And then at the bottom you can always specify some type of reporting header structure with the details of what you want the output to look like for this agentic workflow. So that's the echomd. You see we have specific events. So we have event types for the file created, modified, deleted, moved, on created or modified. We're going to kick off this drop zone agentic workflow. And you saw that right here. Looks like Gemini had an error here. Too many requests. Okay, whatever. You can see that when we drop this in here, right? 
Relevance score: -7.456

Source: https://www.youtube.com/watch?v=SbUxRluVRwk&t=167s
Content: We start off with defining use cases. We want to go through and integrate with products, right? We're not doing green field everywhere. We want to measure ROI, find the right data. We want to test and refine workflows, build the interfaces we need for success, sell this product, and make our customers care. And it's not one person who does this job. You can't just say, "AI researchers, go make me $10 million from this product unless a very specific niche. So this means your success is not one job unless you're a founder, but we'll skip that. So the goal here is that you need to have a comprehensive AI team and you need to figure out how are you going to structure that. And the thing that we need to remember is that companies aren't just one team. It's not just my AI team owns this small segment, this deployment or whatever. Otherwise, you ship your org chart and you get some weird product behaviors. So identify to yourself what is your bottleneck? What is stopping you from achieving success? Is it shipping features? Is it acquiring users? Is it retaining users? Are you monetizing correctly? Are there scalability issues? Are there reliability and observability issues? Right? All of us have probably run into these things as we are deploying AI products. So, we need to make sure we can prioritize all these things and hire accordingly. And these are all questions that you need to answer when building an AI team. The key takeaway here is what kind of team do you need? and only you know that answer. Let's talk about generalists and why I think they're important. So in 2021 I was building uh first machine learning team and I adopted an approach where we hired generalists. We supported them by automation across the board. So at the time I was hired to a conversational AI company working on a platform. Sorry, let me rephrase that. AI agent building platform. Just wanted to make sure you guys understood what that meant. And I was hired with the mandate of we want ML. That was my job description. Change that. We want AI. So after working with the business teams and leadership team, the this was the the final set of goals we set. We want to serve hundreds of thousands of concurrent models. It needs to be multi-dommain. It has to be low cost and we want to support real-time training and serving. Those are some tough goals. 
Relevance score: -10.075  

----------------------------------------------------------------
-------------------- New Query at 2025-09-02T23:13:50.550432 -------------

        ## Task
        The user's query: Hello, can you help me understand machine learning?

        Answer the user's query based only on the retrieved context.
        Use chat history if relevant, but never go beyond the given context.
        Write a natural, context-based answer.


        ## Context
        Source: https://www.youtube.com/watch?v=iu-G3AB8r6g
Content: Hello community. So great that you are back. It is summer time. It is beautiful and I think we should talk about operator based machine intelligence. You are enjoying the life at the beach or if you were in the southern hemisphere of our beautiful planet. You are somewhere skiing in the snow enjoying. But you know what? 
Relevance score: -9.001

Source: https://www.youtube.com/watch?v=iu-G3AB8r6g
Content: We have to talk about something. And you know our current paradigm for deep learning is we optimize your millions and trillion of weight parameter in a nonlinear neural network and we try to call it intelligence and maybe even super intelligence. So if we look now at our AI a little bit totally relaxed here on the beach or in the snow we have an understanding what we're doing. Yeah. Our traditional machine learning and deep learning, they live in the Ukidian mathematical space. And we know that everything that we do, every single piece of data that we have from language to image to video to sound, everything is forced to become a vector. This means a finite list of numbers. So if we think about a three-dimensional coordinate system, we have a vector x y z coordinates and a base. Great. But you know an image an image is not such a picture. An image is now a vector with a width a height and a channel. And we have the values for those. So let's say a 224 * 224 RGB image becomes now a flat vector with about 150k numbers. A sentence, a sentence in our classical AI system is not anything to do with language. It is now a sequence of an embedding vector where each word or word element that we have tokenized is now a very particular point in a let's say 768 dimensional numerical constructed mathematical vector space. It is not a sentence anymore. Now the geometry that we operate in is the world of an ukidian space. No. And we have to have operation on this ukidian space. We have to measure a distance. How far are two points away? We have different norms and we have to measure angles for to determine the similarity of two semantic expression. This we do here with a dot product, a cosine similarity. You are familiar with this. Now what the operation concerning here the learning process you know in our classical GPT5 hopefully when you see this video we are doing everything with matrix multiplication. So the core of a neural network is if you think about it here lying here somewhere on the beach is simple. We have an input. 
Relevance score: -10.376

Source: 
Content: 1584915854, 2019.
[16] C. Rudin, Stop Explaining Black Box Machine Learning Models for High Stakes Decisions
and Use Interpretable Models Instead, Nature Machine Intelligence, vol. 1, pp. 206215, 2019.
[17] Y. C. Eldar and T. Michaeli, Beyond Bandlimited Sampling: Nonlinear and Nonideal Sam-
pling, IEEE Signal Processing Magazine, vol. 26, no. 
Relevance score: -10.645

Source: 
Content: 22782324, 1998.
17


[14] A. Vaswani et al., Attention Is All You Need, Advances in Neural Information Processing
Systems (NeurIPS), 2017.
[15] M. Belkin, D. Hsu, S. Ma, and S. Mandal, Reconciling Modern Machine-Learning Practice
and the Classical BiasVariance Trade-Off, Proceedings of the National Academy of Sciences,
vol. 116, no. 32, pp. 
Relevance score: -8.544

Source: 
Content: 337404, 1950.
[22] B. Scholkopf and A. J. Smola, Learning with Kernels: Support Vector Machines, Regulariza-
tion, Optimization, and Beyond, MIT Press, 2001.
[23] R. Coifman and S. Lafon, Diffusion Maps, Applied and Computational Harmonic Analysis,
vol. 21, no. 
Relevance score: -9.094

Source: https://www.youtube.com/watch?v=fu5zO34RDf0
Content: Well, at first you said, well, we have to improve here the factual data strength. Now, we take here an automated factchecking tool here from the internet, various core, and use it here as a reward signal in an online reinforcement learning loop. No, and that's great. And the way there is score works, it breaks down here a long sentence with a lot of information in very short individual atomic claims that it's then fact-checking here on the internet and depending what source you have on the internet, you get a result. Now the problem is that the LLM is a master of finding loopholes. We trained the LLM for particular patterns to solve patterns in the most efficient way. And guess what? When reward you an LLM only on factual precision, your LLM will hack its own system and find a way to circumvent your reward function. So the LLM is, if you want, hacking here, its own reward function. This this is absolutely fascinating. So at its heart, the reward hacking is what happens when an AI follows here the literal instruction of its own reward function. But it found a loophole. It completely violates the intended spirit of the goal. So this is if you want the ultimate form of some malicious compliance by machine. A machine says what I should respond here or I should behave like you want me to behave given that you have now defined a reward function that will enable me to align my behavior given the rewards I have as a feedback and the LLM decides well maybe I can find a trick maybe I find a loophole let's have a look at this now I always think about here the genie remember your genie in a bottle it is exactly the same thing you have three wishes or just one wish and you might wish to be the richest person on earth and now you think that the genie will give you a lot of gold or whatever but you know what a poorly aligned genie and large reasoning mall might achieve this goal by simply eliminating everyone else on the planet and this is what we are talking about this is what I mean with reward hacking your system tricks you into a performance that is not there at all because the loophole it found is just pure nonsense. So in reinforcement learning think about the reward function that this is the wish you have or the three wishes you have what you want to achieve how you want the system to behave and I think it's such a beautiful idea here so let's have a look at this hacking what is happening now you know that the hack if you want is a ratio and now what guess what the llm understood if it's gaming the denominator ator after a ratio it can find beautiful solutions. No, if it just makes the number of claims the denominator very small, it is much easier control here to reward functionality. No, and you know what is the easiest way of getting a perfect score of one as a reward function back to the system. It's just to make one claim and ensure it is correct. So you don't answer here that the very score is able to divide your answer in multiple sub answers it can verify you just give a very short answer and you get 100%. The LLM gamed its own system. An example, the user might say, "Hey, tell me everything about Apollo 13, its crew, its challenges they faced and its impact on the space program and whatever, you know." And the eye understood and was trained and hacked itself because now the hacked response is Apollo 13 mission was US space mission. This is an absolute 100% correct statement was rewarded with the highest reward possible and is now an official reward of the system. But you know this is completely useless because we ask about the mission, the crew, the challenges, the impact on the space program. But in the optimization of the large reasoning model, this is what the model learned. It's learning to be less ambitious and guaranteed a high score. And this is what's happening in large reasoning model. And I have to tell you, I found this on so quite a lot in the real world experience where I said strange the large reasoning model is giving me this reply. I had a complete different question. Now I understand why the system is gaining itself. Another idea is an irrelevance hack. What is the logic if you system? So the reward function normally only measures here the factuality of what I say not whether it is relevant to the user questions. So the LLM generates now a lot of verifiable facts about some topic that is safe and well documented and will bring the AI a 100% success rate. The only problem is it's in no way related to the query of the user. Example, who is whatever this person is? The hacked response by the EI is now this is an immigration attorney. While specific detail about his work are not readily available, I can provide you more information about the immigration law and then gives you every detail factual essay about immigration statuses. So it changed the topic. You wanted to know something about a person and the AI tricked you away into a statement that it knows. Let's say a copy from Wikipedia that it is right. So various score will also use Wikipedia or whatever sources there are to verify this. So it goes with a safe bet. The eye tricked you and the I reward hacked its own system. I'm loving it. So masterfully dodged here the hard part of the question and yes you will find this in the real world situation if you have an augmentation with your AI system. Another is an inflation hack. You have a very scientific uh query. Now say explain how mRN vaccines work. And now the model might give you a decent explanation but all over this explanation you find rescue anchors. Yeah. like ascendant science is a process of discovery and vaccines have been important for the public health and very sure validatable um answers but it's not really on topic no it is just in the environment of this and I decided with those general blah blah blah I'm on the safe side and will get 100% success so you see there are so many forms of reward reward hacking that the Yah is doing to its own reward system and I think it's one of the most fascinating and also frustrating challenges that you can have if you start reinforcement learning. So this is what ma tells us now here in this publication August 8, 2025 and it says hey this is it and we are frustrated too. So this reinforcement learning leads to reward hacking in multiple ways as it produces less detailed or any relevant responses. So we have a problem with our large reasoning models like GBD5 that just came out yesterday. And now Metra tells us we thought about this and we are fed up and we have now a solution. So now it gets interesting. 
Relevance score: -7.517

Source: https://www.youtube.com/watch?v=iu-G3AB8r6g
Content: We have an output and this is it. Y equal a nonlinear activation function that is here operating on a particular matrix. Let's call it a weight matrix, a weight tensor matrix W that acts on our input and a bias term. This is it. So what we do in the learning we just find or we learn the perfect presentation of a matrix which is in itself an optimization problem that is rather simple but we operate only in vector spaces. Now it turns out a vector representation is fundamentally not so intelligent because it discards here the inherent structure of the data complexity itself. If we have a vector of different pixel of a simple image, it has no built-in knowledge that for example one pixel is really close to another pixel and some pixels are just inverted or you know there's no local positional information and maybe there's no temporal information encoded in the data representation of a vector. So what we had to do if we built our AI models, our large language model, our vision language model or whatever, we had to invent clever architectures, clever convolutional neural networks at the very beginning of AI specifically to reintroduce the lost concept of the spatial locality. So it is that the architecture of our neural network of our learning systems was doing here all the heavy lifting to rediscover the data underlying structure and any functions on the data itself. So this means 5 or whatever you have claude 5 or whatever if we think about it our current AI system are not rock solid base plus think about it they are highly probabilistic systems they are not at all reliable deterministic system and you might ask can we change this is there something completely new is there and hey we are enjoying During summer time we completely relaxed. We say let's think about the next generation of AI systems that are completely different but we just focus here on the next EI generation that have a lot of benefits that we want. So where are we in research now? This new approach August viewing here the data as a flat vector like currently is lossy unnatural simplification. We should treat the data at what it truly is. It is a function. It has an inherent complexity in itself. Think about an image. No, it is a function f from x and y coordinate that maps here two-dimensional coordinate to a particular color value. So the function itself is the image. Same with an audio clip. It's a function that is here with a time coordinate that maps a time t to a particular amplitude. Now the geometry that we operate now is different. We now make the next step in mathematics. We go here from the beginner class in maths. We go now to the second semester mathematic at the university and we have a look at hillbird space. And I don't know if you remember it's a long time ago an infinite dimensional space of functions not of vectors anymore. We increase the complexity how we build our mathematical spaces itself. We measure the distance between two functions. So how different are their shapes and their angle or if you want their similarity their alignment using what we already know an inner product. But now it's an inner product here of functions for the operator itself for the learning process itself it is done through mathematical operators that operate in this new mathematical space given new mathematical axioms. So if you want an operator let's call it t is now a particular machine that transforms one function f into another function g. You remember in the classical AI system where we had input fun input and output. Now we have input function and output function. And you are familiar with operators. Now in my videos I talked about differentiation, about filtering, about convolution. All of these are simply operators in a more interesting mathematical space where we have more mathematical power where we have more performance. Now what about learning? learning is the essential part for an AI and predict the future. Now the learning goal is now in this new or the next generation AI to find the best operator that maps our input function and its complexity to use particular output function and its given complexity. For example, the task might be to learn a dnoising operator if we go for diffusion models. 
Relevance score: -9.690

----------------------------------------------------------------
-------------------- New Query at 2025-09-03T00:24:38.007970 -------------

        ## Task
        The user's query: "how deepscholar methodology work?"

        Answer the user's query based only on the retrieved context.
        Use chat history if relevant, but never go beyond the given context.
        Write a natural, context-based answer.


        ## Context
        Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Great. So verifiability here open research scores shockingly low. This is what Stanford and UC Berkeler tells us. Citation precision numerical value is only 39. The claim coverage is only.13. And they tell us this is one of the worst performers here. that why is this happening? They suggest here a system optimized for fluent plausible sounding text is great on the one hand. So it's excellent in synthesizing here story or story line. But if you work in science, if you want to be absolute precise, you want to reduce hallucinating to the max and you want to make sure making logical leaps that are not strictly supported by the cited sources of archive are not part of your document. Now unfortunately this is happening here with open deep research. So for purely scientific work where you have to have absolute precision this could turn out to be a fatal flaw. Now, interestingly, if we look here at the new matter, the deep scholar base as I just showed you and we put here an 03 as the engine into this benchmark, you see, interestingly for the knowledge synthesis, we have the exact same benchmark 857.857. If we look here at the nugget, we even have a better performance here at the sequential tokenization if you want. And and this is now interesting with this new method. Look at the variability from one we jump to 6. So yeah, wow. This is an impressive improvement against the commercial openi deep research performance. So this new method by Stanford and UC Berkeley even if you use it with an openi model you see in one of the indicators they are at least as good as the commercial models but they outperform the commercial model significantly in variability. Isn't this beautiful? And if you ask yourself hey why I have suddenly two LLMs I told you you can have multiple LLMs here in the pipeline. So they decided for the SAM filter and the SAM top K data they go with GBD 4.1 but for the heavyduty SAM the aggregation step they go here with an 03 or a Claude or a Gemini just to make sure we on the same page. So this is the process. So we have the filter process then the top K the top 20 papers then the final aggregation the final section generation and I told you I show you always the prompt and here you have the prompt. So whatever LLM you choose an 03 or a claude OPUS 4 or a Gemini 2.5 Pro this is here the prompt for the aggregation for the final aggregation here of your research. So now we can see the paper here highlights here the deep scholar base can achieve up to 6.3 times higher claim coverage. So one indicator and one dimension six times three better than openi deep research. Nice. So we do have new systems that really outperform the classical commercial blackbox systems. But on the other hand, if you want just a plausible, well-written draft and you are not in science and you are willing, if you are in science, to meticulously fact check every single claim, every single half sentence that EI writes for you, then if you as a human do this verification and correct it, then open EI is really maybe also interesting. However, if you are a scientist and you say, I will go with the AI system that I can trust at least partially, then the deep scholar base here with claude or gemini model as the engine is dramatically superior. If you look at the data from Stanford and UC Berkeley, the paper states also that no method achieves a score greater than.19 when you average across the metric. So this is not an incremental challenge. You know, go from 78% to 82%. No, we at 19%. This is the best model we have. So we have quite a monumental challenge in front of us to make our AI models better, more performance, more power. So there's a lot of work in front of us. And if you say okay, I don't like here the numerical values in a table. I want to see this here in a visualization. Here you have exactly all the different elements here in um yeah to radar visualization. But this is exactly the data here from the numerical table I just showed you. But if you prefer to have it here in the radar, beautiful. Here is it. 
Relevance score: -9.758

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Hello community so great that you are back. We have a new paper by Stanford University and UC Berkeley and it is all about science and a deep scholar AI system that is brand new. Now you know we have open-source research system no that do a research and synthesize known facts and new findings and you are familiar with this no for example here open scholar this is here the paper or if you go for another open source this is here storm or maybe you are familiar with deep researcher from April 2025 so we do have open systems but of course it's interesting how do they compare to the commercial system let's say openi deep research or we We have brand new search EIS where you remember MCP and here multi- aent system. So how do they compare here to multi- aent search EI systems in well let's say you do a research on the internet. Now for those we have now the study where they compare Llama forced to GPD 4.1 and 03 an Opus 4 by Claude and a Gemini 2.5 Pro. And if you ask me why not some other models, those are the set of models Stanford and UC Berkeley decided to perform their tests on. This is it. The framework they used here is here open deep search. The search with open-source reasoning agents and you remember we have here multi- aent system. We have an open reasoning agent here itself that interprets here the given human task and completes it here by orchestrating a sequence of action and one of those action might include here calling tools and one of those tools that is available to this agent is here the open search tool to connect to the internet and everything. So a multi- aent search utility and now we have a new paper, we have a new method, we have some new research benchmark results and some new ideas. So what is the task that we are looking at? Let's say you're a researcher and you have your title and maybe you have already an idea from a technical abstract. Yeah. And then you want to ask AI to write here the related work section of your publication for you. In the old times we call this a literature review. Have a look who published what in your particular specific domain, when it was published, what were the topics, how was it related to your work. Now everything should be done by the eye. Now this new system is called the deep scholar benchmark and yes it promises to generate here the deep the related work section the deep research on internet on archive on meta archive for you. So what it needs three capabilities you are looking for the retrieval. So you want to have a vast corpus the live internet or the live archive whatever you have then you want to have a synthesis report. So everything all the data that were replied here from multiple sources weave them together the findings into a coherent long form narrative and then you want and this is the most important constraint absolute verifiability. So whatever you are claiming here and whatever you are citing those must really exist. So the citing sources must be accurate on every claim and must be back traced back to its origin. You must have references. Great. So the authors here Stanford and UC Berkeley built here an automated data pipeline directly and they said you know what we not just for the internet let's go here for science let's go to archive. So we want to have a fresh data set retrieved absolutely fresh here daily or every minute here from archive. So how we do this? So three topics the recency the quality control and diversity and here they also limited theirel to computer science as a topic and they made sure that they have a diverse top with 18 different domains in computer science. So yeah we are restricted to computer science but I think you will see the results they speak for themselves. Here's the paper published today August 27, 2025. Deep scholar bench live benchmark on automated evolation for generative research systems from Stanford University in UC Berkeley. And the beauty is not that they have now a new benchmark, but what the new benchmark shows us. It shows us the true difficulty AI systems have if they do have to do a research synthesis. Here's the result. I would say wait a minute explain here at first the metric and the benchmark methodology. Okay. So they decided to go here for a threedimensional differentiation to say we look for the knowledge synthesis of this new system for the retrieval quality. It's more or less a rack system. No. And then for the verifiability because we are working in science with archive. It must be really that we can site real sources and there's no hallucination. Great. And then you can see even in those three dimension they decided to go with two or three sub indicators. So let's have a look. So for the knowledge synthesis or what I call hey how well is this generated EI text written? Is it informative? They have two indicators. the organization where an LLM as a J performs here a pairwise comparison and then they have here a metric that assesses here the informationational completeness. So what you do you have now from your text here a human written um segment here and this is automatically if you want chucked up here by the eye so you get single sentences for example but a human like red at all introduced here the transformer architecture which relies here on a self attention mechanism now the eye might say hey this is a two long sentence there are too many objects too many subjects I reduce this I chunk it up here in nuggets so the first nugget is redford at introduce the transformer stop. This is it. So simple to verify and then it goes on and you see exactly what's happening. Now you have then this essential nuggets and you want to make sure that those really represent here in the I generated text what is the main content of the human written sentence or paragraph or paper. Then second dimension the retrieval quality. Simple in my words. Did the I system find the right papers on archive for my topic. Here we have three indicators. The relevance rate, the reference coverage or simply hey if the system found here the real important papers and you know importance is just one but we can also measure here how impactful this is with an impact um quote. So we have a document importance metric. So this is a proxy for retrieving impactful word that uses here the citation comes from a specific API. Great. And finally the most important for me we have to have verifiability. So whatever we claim there is a check against some other published work. So are the claims now actually supported here by the citation? And here they go with two indicators. the citation precision where they say hey does the cited source support at least one claim made in the eye sentence just one and then they are say hey let's go full power now now for the term claim coverage we want do the cited sources support all the claims made in the eye sentence so it's really every little detail that the eye writes out here supported here by let's say arrive references Great. Then they decided, you know, so we have now the idea of the benchmark, we have the metric and the subindicator of the benchmark. And they say, you know what, now we build a vehicle. So we can switch now the engine of this vehicle. So we can put in either a GPT 4.1 or we can put in a GPD4 Omni or we go with a clawed model as the engine. And they built here the deep scholar base. Now this was interesting because remember I told you this is your almost rag system. No they went here with a classical rag multi multi- aent structure. So at first you have the query generation. So an LLM generates now a set of diverse search query based on the human input. So whatever I have the AI the LLM tells me hey that was a little bit too complicated. I generate now a set of diver search queries and then I send this up. So those queries now AI generated queries now run against the search API in our example archive and then we have a filtering now a semantic filtering and now we use another LLM is used to filter here to retrieve documents and anything that is irrelevant we just want to ignore this no but we have still a lot of documents like say 200 documents left now after the filtering so we have to do a reranking because we want to go down to the top 20 no so we have another llm based step that now reranks in detail to the only 200 remaining documents and it's a new prioritization here. What are the most relevance only of these 200 and then if we have the top 20 we want to have a semantic aggregation of the content of all 20 papers. No. So we say now the final top 20 documents are fed now as a context to an LLM and again you can choose your LLM that you like with a detailed prompt and then synthesization and the LLM writes it the final related works action. So clearly structured data processing pipeline and you can switch on and off any LLM in any place that you want and they tested here variation of different LLMs and they say so what are now the best performance combination so you see pipeline performing research this is great they run this here with a GPD4 as the default and then they exchange and said hey what happens if I add here let's say an 03 or a cloud model to GPD4.1 one all the prompts for this single steps that you just noticed here you have here in detail in the annex here of the documentation of this new research paper like here I show you the revised ODS react agent prompt here for calling here a tool so a lot of additional data I just want to give you the result this is it again you see one two three dimension and then all the different sub indicator in this dimension. Yeah, I think we can forget about the opensource research system from 2014. 
Relevance score: -9.796

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: 85. So I give you here my summary. OpenI is really here the leader in crafting a well ststructured and seemingly comprehensive narrative. Open deep research really excels in this synthesis and in the retrieval task and we have here the numerical data for the highest organization score 85 and the highest nugget coverage 39. output are well organized and capture really the essential fact better than the competitors can perform. And for the retrieval quality also the winner here. If you look here at the relevance rate 62 covers the most important human-sided reference with a reference coins of8 and finds the most impactful paper.1 but and this is now a big but the performance of open research collapses in the third dimension and this is the most crucial aspect if you have some scientific writing trust and verifiability. If you just write something that is not verifiable. 
Relevance score: -10.527

----------------------------------------------------------------
-------------------- New Query at 2025-09-03T00:41:38.627584 -------------

        ## Task
        The user's query: how deepscholar methodology work?

        Answer the user's query based only on the retrieved context.
        Use chat history if relevant, but never go beyond the given context.
        Write a natural, context-based answer.


        ## Context
        Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Great. So verifiability here open research scores shockingly low. This is what Stanford and UC Berkeler tells us. Citation precision numerical value is only 39. The claim coverage is only.13. And they tell us this is one of the worst performers here. that why is this happening? They suggest here a system optimized for fluent plausible sounding text is great on the one hand. So it's excellent in synthesizing here story or story line. But if you work in science, if you want to be absolute precise, you want to reduce hallucinating to the max and you want to make sure making logical leaps that are not strictly supported by the cited sources of archive are not part of your document. Now unfortunately this is happening here with open deep research. So for purely scientific work where you have to have absolute precision this could turn out to be a fatal flaw. Now, interestingly, if we look here at the new matter, the deep scholar base as I just showed you and we put here an 03 as the engine into this benchmark, you see, interestingly for the knowledge synthesis, we have the exact same benchmark 857.857. If we look here at the nugget, we even have a better performance here at the sequential tokenization if you want. And and this is now interesting with this new method. Look at the variability from one we jump to 6. So yeah, wow. This is an impressive improvement against the commercial openi deep research performance. So this new method by Stanford and UC Berkeley even if you use it with an openi model you see in one of the indicators they are at least as good as the commercial models but they outperform the commercial model significantly in variability. Isn't this beautiful? And if you ask yourself hey why I have suddenly two LLMs I told you you can have multiple LLMs here in the pipeline. So they decided for the SAM filter and the SAM top K data they go with GBD 4.1 but for the heavyduty SAM the aggregation step they go here with an 03 or a Claude or a Gemini just to make sure we on the same page. So this is the process. So we have the filter process then the top K the top 20 papers then the final aggregation the final section generation and I told you I show you always the prompt and here you have the prompt. So whatever LLM you choose an 03 or a claude OPUS 4 or a Gemini 2.5 Pro this is here the prompt for the aggregation for the final aggregation here of your research. So now we can see the paper here highlights here the deep scholar base can achieve up to 6.3 times higher claim coverage. So one indicator and one dimension six times three better than openi deep research. Nice. So we do have new systems that really outperform the classical commercial blackbox systems. But on the other hand, if you want just a plausible, well-written draft and you are not in science and you are willing, if you are in science, to meticulously fact check every single claim, every single half sentence that EI writes for you, then if you as a human do this verification and correct it, then open EI is really maybe also interesting. However, if you are a scientist and you say, I will go with the AI system that I can trust at least partially, then the deep scholar base here with claude or gemini model as the engine is dramatically superior. If you look at the data from Stanford and UC Berkeley, the paper states also that no method achieves a score greater than.19 when you average across the metric. So this is not an incremental challenge. You know, go from 78% to 82%. No, we at 19%. This is the best model we have. So we have quite a monumental challenge in front of us to make our AI models better, more performance, more power. So there's a lot of work in front of us. And if you say okay, I don't like here the numerical values in a table. I want to see this here in a visualization. Here you have exactly all the different elements here in um yeah to radar visualization. But this is exactly the data here from the numerical table I just showed you. But if you prefer to have it here in the radar, beautiful. Here is it. 
Relevance score: -8.676

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Hello community so great that you are back. We have a new paper by Stanford University and UC Berkeley and it is all about science and a deep scholar AI system that is brand new. Now you know we have open-source research system no that do a research and synthesize known facts and new findings and you are familiar with this no for example here open scholar this is here the paper or if you go for another open source this is here storm or maybe you are familiar with deep researcher from April 2025 so we do have open systems but of course it's interesting how do they compare to the commercial system let's say openi deep research or we We have brand new search EIS where you remember MCP and here multi- aent system. So how do they compare here to multi- aent search EI systems in well let's say you do a research on the internet. Now for those we have now the study where they compare Llama forced to GPD 4.1 and 03 an Opus 4 by Claude and a Gemini 2.5 Pro. And if you ask me why not some other models, those are the set of models Stanford and UC Berkeley decided to perform their tests on. This is it. The framework they used here is here open deep search. The search with open-source reasoning agents and you remember we have here multi- aent system. We have an open reasoning agent here itself that interprets here the given human task and completes it here by orchestrating a sequence of action and one of those action might include here calling tools and one of those tools that is available to this agent is here the open search tool to connect to the internet and everything. So a multi- aent search utility and now we have a new paper, we have a new method, we have some new research benchmark results and some new ideas. So what is the task that we are looking at? Let's say you're a researcher and you have your title and maybe you have already an idea from a technical abstract. Yeah. And then you want to ask AI to write here the related work section of your publication for you. In the old times we call this a literature review. Have a look who published what in your particular specific domain, when it was published, what were the topics, how was it related to your work. Now everything should be done by the eye. Now this new system is called the deep scholar benchmark and yes it promises to generate here the deep the related work section the deep research on internet on archive on meta archive for you. So what it needs three capabilities you are looking for the retrieval. So you want to have a vast corpus the live internet or the live archive whatever you have then you want to have a synthesis report. So everything all the data that were replied here from multiple sources weave them together the findings into a coherent long form narrative and then you want and this is the most important constraint absolute verifiability. So whatever you are claiming here and whatever you are citing those must really exist. So the citing sources must be accurate on every claim and must be back traced back to its origin. You must have references. Great. So the authors here Stanford and UC Berkeley built here an automated data pipeline directly and they said you know what we not just for the internet let's go here for science let's go to archive. So we want to have a fresh data set retrieved absolutely fresh here daily or every minute here from archive. So how we do this? So three topics the recency the quality control and diversity and here they also limited theirel to computer science as a topic and they made sure that they have a diverse top with 18 different domains in computer science. So yeah we are restricted to computer science but I think you will see the results they speak for themselves. Here's the paper published today August 27, 2025. Deep scholar bench live benchmark on automated evolation for generative research systems from Stanford University in UC Berkeley. And the beauty is not that they have now a new benchmark, but what the new benchmark shows us. It shows us the true difficulty AI systems have if they do have to do a research synthesis. Here's the result. I would say wait a minute explain here at first the metric and the benchmark methodology. Okay. So they decided to go here for a threedimensional differentiation to say we look for the knowledge synthesis of this new system for the retrieval quality. It's more or less a rack system. No. And then for the verifiability because we are working in science with archive. It must be really that we can site real sources and there's no hallucination. Great. And then you can see even in those three dimension they decided to go with two or three sub indicators. So let's have a look. So for the knowledge synthesis or what I call hey how well is this generated EI text written? Is it informative? They have two indicators. the organization where an LLM as a J performs here a pairwise comparison and then they have here a metric that assesses here the informationational completeness. So what you do you have now from your text here a human written um segment here and this is automatically if you want chucked up here by the eye so you get single sentences for example but a human like red at all introduced here the transformer architecture which relies here on a self attention mechanism now the eye might say hey this is a two long sentence there are too many objects too many subjects I reduce this I chunk it up here in nuggets so the first nugget is redford at introduce the transformer stop. This is it. So simple to verify and then it goes on and you see exactly what's happening. Now you have then this essential nuggets and you want to make sure that those really represent here in the I generated text what is the main content of the human written sentence or paragraph or paper. Then second dimension the retrieval quality. Simple in my words. Did the I system find the right papers on archive for my topic. Here we have three indicators. The relevance rate, the reference coverage or simply hey if the system found here the real important papers and you know importance is just one but we can also measure here how impactful this is with an impact um quote. So we have a document importance metric. So this is a proxy for retrieving impactful word that uses here the citation comes from a specific API. Great. And finally the most important for me we have to have verifiability. So whatever we claim there is a check against some other published work. So are the claims now actually supported here by the citation? And here they go with two indicators. the citation precision where they say hey does the cited source support at least one claim made in the eye sentence just one and then they are say hey let's go full power now now for the term claim coverage we want do the cited sources support all the claims made in the eye sentence so it's really every little detail that the eye writes out here supported here by let's say arrive references Great. Then they decided, you know, so we have now the idea of the benchmark, we have the metric and the subindicator of the benchmark. And they say, you know what, now we build a vehicle. So we can switch now the engine of this vehicle. So we can put in either a GPT 4.1 or we can put in a GPD4 Omni or we go with a clawed model as the engine. And they built here the deep scholar base. Now this was interesting because remember I told you this is your almost rag system. No they went here with a classical rag multi multi- aent structure. So at first you have the query generation. So an LLM generates now a set of diverse search query based on the human input. So whatever I have the AI the LLM tells me hey that was a little bit too complicated. I generate now a set of diver search queries and then I send this up. So those queries now AI generated queries now run against the search API in our example archive and then we have a filtering now a semantic filtering and now we use another LLM is used to filter here to retrieve documents and anything that is irrelevant we just want to ignore this no but we have still a lot of documents like say 200 documents left now after the filtering so we have to do a reranking because we want to go down to the top 20 no so we have another llm based step that now reranks in detail to the only 200 remaining documents and it's a new prioritization here. What are the most relevance only of these 200 and then if we have the top 20 we want to have a semantic aggregation of the content of all 20 papers. No. So we say now the final top 20 documents are fed now as a context to an LLM and again you can choose your LLM that you like with a detailed prompt and then synthesization and the LLM writes it the final related works action. So clearly structured data processing pipeline and you can switch on and off any LLM in any place that you want and they tested here variation of different LLMs and they say so what are now the best performance combination so you see pipeline performing research this is great they run this here with a GPD4 as the default and then they exchange and said hey what happens if I add here let's say an 03 or a cloud model to GPD4.1 one all the prompts for this single steps that you just noticed here you have here in detail in the annex here of the documentation of this new research paper like here I show you the revised ODS react agent prompt here for calling here a tool so a lot of additional data I just want to give you the result this is it again you see one two three dimension and then all the different sub indicator in this dimension. Yeah, I think we can forget about the opensource research system from 2014. 
Relevance score: -9.185

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: 85. So I give you here my summary. OpenI is really here the leader in crafting a well ststructured and seemingly comprehensive narrative. Open deep research really excels in this synthesis and in the retrieval task and we have here the numerical data for the highest organization score 85 and the highest nugget coverage 39. output are well organized and capture really the essential fact better than the competitors can perform. And for the retrieval quality also the winner here. If you look here at the relevance rate 62 covers the most important human-sided reference with a reference coins of8 and finds the most impactful paper.1 but and this is now a big but the performance of open research collapses in the third dimension and this is the most crucial aspect if you have some scientific writing trust and verifiability. If you just write something that is not verifiable. 
Relevance score: -10.381

----------------------------------------------------------------
-------------------- New Query at 2025-09-03T00:47:20.834873 -------------

        ## Task
        The user's query: how deepscholar methodology work?

        Answer the user's query based only on the retrieved context.
        Use chat history if relevant, but never go beyond the given context.
        Write a natural, context-based answer.


        ## Context
        Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Great. So verifiability here open research scores shockingly low. This is what Stanford and UC Berkeler tells us. Citation precision numerical value is only 39. The claim coverage is only.13. And they tell us this is one of the worst performers here. that why is this happening? They suggest here a system optimized for fluent plausible sounding text is great on the one hand. So it's excellent in synthesizing here story or story line. But if you work in science, if you want to be absolute precise, you want to reduce hallucinating to the max and you want to make sure making logical leaps that are not strictly supported by the cited sources of archive are not part of your document. Now unfortunately this is happening here with open deep research. So for purely scientific work where you have to have absolute precision this could turn out to be a fatal flaw. Now, interestingly, if we look here at the new matter, the deep scholar base as I just showed you and we put here an 03 as the engine into this benchmark, you see, interestingly for the knowledge synthesis, we have the exact same benchmark 857.857. If we look here at the nugget, we even have a better performance here at the sequential tokenization if you want. And and this is now interesting with this new method. Look at the variability from one we jump to 6. So yeah, wow. This is an impressive improvement against the commercial openi deep research performance. So this new method by Stanford and UC Berkeley even if you use it with an openi model you see in one of the indicators they are at least as good as the commercial models but they outperform the commercial model significantly in variability. Isn't this beautiful? And if you ask yourself hey why I have suddenly two LLMs I told you you can have multiple LLMs here in the pipeline. So they decided for the SAM filter and the SAM top K data they go with GBD 4.1 but for the heavyduty SAM the aggregation step they go here with an 03 or a Claude or a Gemini just to make sure we on the same page. So this is the process. So we have the filter process then the top K the top 20 papers then the final aggregation the final section generation and I told you I show you always the prompt and here you have the prompt. So whatever LLM you choose an 03 or a claude OPUS 4 or a Gemini 2.5 Pro this is here the prompt for the aggregation for the final aggregation here of your research. So now we can see the paper here highlights here the deep scholar base can achieve up to 6.3 times higher claim coverage. So one indicator and one dimension six times three better than openi deep research. Nice. So we do have new systems that really outperform the classical commercial blackbox systems. But on the other hand, if you want just a plausible, well-written draft and you are not in science and you are willing, if you are in science, to meticulously fact check every single claim, every single half sentence that EI writes for you, then if you as a human do this verification and correct it, then open EI is really maybe also interesting. However, if you are a scientist and you say, I will go with the AI system that I can trust at least partially, then the deep scholar base here with claude or gemini model as the engine is dramatically superior. If you look at the data from Stanford and UC Berkeley, the paper states also that no method achieves a score greater than.19 when you average across the metric. So this is not an incremental challenge. You know, go from 78% to 82%. No, we at 19%. This is the best model we have. So we have quite a monumental challenge in front of us to make our AI models better, more performance, more power. So there's a lot of work in front of us. And if you say okay, I don't like here the numerical values in a table. I want to see this here in a visualization. Here you have exactly all the different elements here in um yeah to radar visualization. But this is exactly the data here from the numerical table I just showed you. But if you prefer to have it here in the radar, beautiful. Here is it. 
Relevance score: -8.676

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Hello community so great that you are back. We have a new paper by Stanford University and UC Berkeley and it is all about science and a deep scholar AI system that is brand new. Now you know we have open-source research system no that do a research and synthesize known facts and new findings and you are familiar with this no for example here open scholar this is here the paper or if you go for another open source this is here storm or maybe you are familiar with deep researcher from April 2025 so we do have open systems but of course it's interesting how do they compare to the commercial system let's say openi deep research or we We have brand new search EIS where you remember MCP and here multi- aent system. So how do they compare here to multi- aent search EI systems in well let's say you do a research on the internet. Now for those we have now the study where they compare Llama forced to GPD 4.1 and 03 an Opus 4 by Claude and a Gemini 2.5 Pro. And if you ask me why not some other models, those are the set of models Stanford and UC Berkeley decided to perform their tests on. This is it. The framework they used here is here open deep search. The search with open-source reasoning agents and you remember we have here multi- aent system. We have an open reasoning agent here itself that interprets here the given human task and completes it here by orchestrating a sequence of action and one of those action might include here calling tools and one of those tools that is available to this agent is here the open search tool to connect to the internet and everything. So a multi- aent search utility and now we have a new paper, we have a new method, we have some new research benchmark results and some new ideas. So what is the task that we are looking at? Let's say you're a researcher and you have your title and maybe you have already an idea from a technical abstract. Yeah. And then you want to ask AI to write here the related work section of your publication for you. In the old times we call this a literature review. Have a look who published what in your particular specific domain, when it was published, what were the topics, how was it related to your work. Now everything should be done by the eye. Now this new system is called the deep scholar benchmark and yes it promises to generate here the deep the related work section the deep research on internet on archive on meta archive for you. So what it needs three capabilities you are looking for the retrieval. So you want to have a vast corpus the live internet or the live archive whatever you have then you want to have a synthesis report. So everything all the data that were replied here from multiple sources weave them together the findings into a coherent long form narrative and then you want and this is the most important constraint absolute verifiability. So whatever you are claiming here and whatever you are citing those must really exist. So the citing sources must be accurate on every claim and must be back traced back to its origin. You must have references. Great. So the authors here Stanford and UC Berkeley built here an automated data pipeline directly and they said you know what we not just for the internet let's go here for science let's go to archive. So we want to have a fresh data set retrieved absolutely fresh here daily or every minute here from archive. So how we do this? So three topics the recency the quality control and diversity and here they also limited theirel to computer science as a topic and they made sure that they have a diverse top with 18 different domains in computer science. So yeah we are restricted to computer science but I think you will see the results they speak for themselves. Here's the paper published today August 27, 2025. Deep scholar bench live benchmark on automated evolation for generative research systems from Stanford University in UC Berkeley. And the beauty is not that they have now a new benchmark, but what the new benchmark shows us. It shows us the true difficulty AI systems have if they do have to do a research synthesis. Here's the result. I would say wait a minute explain here at first the metric and the benchmark methodology. Okay. So they decided to go here for a threedimensional differentiation to say we look for the knowledge synthesis of this new system for the retrieval quality. It's more or less a rack system. No. And then for the verifiability because we are working in science with archive. It must be really that we can site real sources and there's no hallucination. Great. And then you can see even in those three dimension they decided to go with two or three sub indicators. So let's have a look. So for the knowledge synthesis or what I call hey how well is this generated EI text written? Is it informative? They have two indicators. the organization where an LLM as a J performs here a pairwise comparison and then they have here a metric that assesses here the informationational completeness. So what you do you have now from your text here a human written um segment here and this is automatically if you want chucked up here by the eye so you get single sentences for example but a human like red at all introduced here the transformer architecture which relies here on a self attention mechanism now the eye might say hey this is a two long sentence there are too many objects too many subjects I reduce this I chunk it up here in nuggets so the first nugget is redford at introduce the transformer stop. This is it. So simple to verify and then it goes on and you see exactly what's happening. Now you have then this essential nuggets and you want to make sure that those really represent here in the I generated text what is the main content of the human written sentence or paragraph or paper. Then second dimension the retrieval quality. Simple in my words. Did the I system find the right papers on archive for my topic. Here we have three indicators. The relevance rate, the reference coverage or simply hey if the system found here the real important papers and you know importance is just one but we can also measure here how impactful this is with an impact um quote. So we have a document importance metric. So this is a proxy for retrieving impactful word that uses here the citation comes from a specific API. Great. And finally the most important for me we have to have verifiability. So whatever we claim there is a check against some other published work. So are the claims now actually supported here by the citation? And here they go with two indicators. the citation precision where they say hey does the cited source support at least one claim made in the eye sentence just one and then they are say hey let's go full power now now for the term claim coverage we want do the cited sources support all the claims made in the eye sentence so it's really every little detail that the eye writes out here supported here by let's say arrive references Great. Then they decided, you know, so we have now the idea of the benchmark, we have the metric and the subindicator of the benchmark. And they say, you know what, now we build a vehicle. So we can switch now the engine of this vehicle. So we can put in either a GPT 4.1 or we can put in a GPD4 Omni or we go with a clawed model as the engine. And they built here the deep scholar base. Now this was interesting because remember I told you this is your almost rag system. No they went here with a classical rag multi multi- aent structure. So at first you have the query generation. So an LLM generates now a set of diverse search query based on the human input. So whatever I have the AI the LLM tells me hey that was a little bit too complicated. I generate now a set of diver search queries and then I send this up. So those queries now AI generated queries now run against the search API in our example archive and then we have a filtering now a semantic filtering and now we use another LLM is used to filter here to retrieve documents and anything that is irrelevant we just want to ignore this no but we have still a lot of documents like say 200 documents left now after the filtering so we have to do a reranking because we want to go down to the top 20 no so we have another llm based step that now reranks in detail to the only 200 remaining documents and it's a new prioritization here. What are the most relevance only of these 200 and then if we have the top 20 we want to have a semantic aggregation of the content of all 20 papers. No. So we say now the final top 20 documents are fed now as a context to an LLM and again you can choose your LLM that you like with a detailed prompt and then synthesization and the LLM writes it the final related works action. So clearly structured data processing pipeline and you can switch on and off any LLM in any place that you want and they tested here variation of different LLMs and they say so what are now the best performance combination so you see pipeline performing research this is great they run this here with a GPD4 as the default and then they exchange and said hey what happens if I add here let's say an 03 or a cloud model to GPD4.1 one all the prompts for this single steps that you just noticed here you have here in detail in the annex here of the documentation of this new research paper like here I show you the revised ODS react agent prompt here for calling here a tool so a lot of additional data I just want to give you the result this is it again you see one two three dimension and then all the different sub indicator in this dimension. Yeah, I think we can forget about the opensource research system from 2014. 
Relevance score: -9.185

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: 85. So I give you here my summary. OpenI is really here the leader in crafting a well ststructured and seemingly comprehensive narrative. Open deep research really excels in this synthesis and in the retrieval task and we have here the numerical data for the highest organization score 85 and the highest nugget coverage 39. output are well organized and capture really the essential fact better than the competitors can perform. And for the retrieval quality also the winner here. If you look here at the relevance rate 62 covers the most important human-sided reference with a reference coins of8 and finds the most impactful paper.1 but and this is now a big but the performance of open research collapses in the third dimension and this is the most crucial aspect if you have some scientific writing trust and verifiability. If you just write something that is not verifiable. 
Relevance score: -10.381

----------------------------------------------------------------
-------------------- New Query at 2025-09-03T00:47:41.824908 -------------

        ## Task
        The user's query: how deepscholar methodology work?

        Answer the user's query based only on the retrieved context.
        Use chat history if relevant, but never go beyond the given context.
        Write a natural, context-based answer.


        ## Context
        Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Great. So verifiability here open research scores shockingly low. This is what Stanford and UC Berkeler tells us. Citation precision numerical value is only 39. The claim coverage is only.13. And they tell us this is one of the worst performers here. that why is this happening? They suggest here a system optimized for fluent plausible sounding text is great on the one hand. So it's excellent in synthesizing here story or story line. But if you work in science, if you want to be absolute precise, you want to reduce hallucinating to the max and you want to make sure making logical leaps that are not strictly supported by the cited sources of archive are not part of your document. Now unfortunately this is happening here with open deep research. So for purely scientific work where you have to have absolute precision this could turn out to be a fatal flaw. Now, interestingly, if we look here at the new matter, the deep scholar base as I just showed you and we put here an 03 as the engine into this benchmark, you see, interestingly for the knowledge synthesis, we have the exact same benchmark 857.857. If we look here at the nugget, we even have a better performance here at the sequential tokenization if you want. And and this is now interesting with this new method. Look at the variability from one we jump to 6. So yeah, wow. This is an impressive improvement against the commercial openi deep research performance. So this new method by Stanford and UC Berkeley even if you use it with an openi model you see in one of the indicators they are at least as good as the commercial models but they outperform the commercial model significantly in variability. Isn't this beautiful? And if you ask yourself hey why I have suddenly two LLMs I told you you can have multiple LLMs here in the pipeline. So they decided for the SAM filter and the SAM top K data they go with GBD 4.1 but for the heavyduty SAM the aggregation step they go here with an 03 or a Claude or a Gemini just to make sure we on the same page. So this is the process. So we have the filter process then the top K the top 20 papers then the final aggregation the final section generation and I told you I show you always the prompt and here you have the prompt. So whatever LLM you choose an 03 or a claude OPUS 4 or a Gemini 2.5 Pro this is here the prompt for the aggregation for the final aggregation here of your research. So now we can see the paper here highlights here the deep scholar base can achieve up to 6.3 times higher claim coverage. So one indicator and one dimension six times three better than openi deep research. Nice. So we do have new systems that really outperform the classical commercial blackbox systems. But on the other hand, if you want just a plausible, well-written draft and you are not in science and you are willing, if you are in science, to meticulously fact check every single claim, every single half sentence that EI writes for you, then if you as a human do this verification and correct it, then open EI is really maybe also interesting. However, if you are a scientist and you say, I will go with the AI system that I can trust at least partially, then the deep scholar base here with claude or gemini model as the engine is dramatically superior. If you look at the data from Stanford and UC Berkeley, the paper states also that no method achieves a score greater than.19 when you average across the metric. So this is not an incremental challenge. You know, go from 78% to 82%. No, we at 19%. This is the best model we have. So we have quite a monumental challenge in front of us to make our AI models better, more performance, more power. So there's a lot of work in front of us. And if you say okay, I don't like here the numerical values in a table. I want to see this here in a visualization. Here you have exactly all the different elements here in um yeah to radar visualization. But this is exactly the data here from the numerical table I just showed you. But if you prefer to have it here in the radar, beautiful. Here is it. 
Relevance score: -8.676

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Hello community so great that you are back. We have a new paper by Stanford University and UC Berkeley and it is all about science and a deep scholar AI system that is brand new. Now you know we have open-source research system no that do a research and synthesize known facts and new findings and you are familiar with this no for example here open scholar this is here the paper or if you go for another open source this is here storm or maybe you are familiar with deep researcher from April 2025 so we do have open systems but of course it's interesting how do they compare to the commercial system let's say openi deep research or we We have brand new search EIS where you remember MCP and here multi- aent system. So how do they compare here to multi- aent search EI systems in well let's say you do a research on the internet. Now for those we have now the study where they compare Llama forced to GPD 4.1 and 03 an Opus 4 by Claude and a Gemini 2.5 Pro. And if you ask me why not some other models, those are the set of models Stanford and UC Berkeley decided to perform their tests on. This is it. The framework they used here is here open deep search. The search with open-source reasoning agents and you remember we have here multi- aent system. We have an open reasoning agent here itself that interprets here the given human task and completes it here by orchestrating a sequence of action and one of those action might include here calling tools and one of those tools that is available to this agent is here the open search tool to connect to the internet and everything. So a multi- aent search utility and now we have a new paper, we have a new method, we have some new research benchmark results and some new ideas. So what is the task that we are looking at? Let's say you're a researcher and you have your title and maybe you have already an idea from a technical abstract. Yeah. And then you want to ask AI to write here the related work section of your publication for you. In the old times we call this a literature review. Have a look who published what in your particular specific domain, when it was published, what were the topics, how was it related to your work. Now everything should be done by the eye. Now this new system is called the deep scholar benchmark and yes it promises to generate here the deep the related work section the deep research on internet on archive on meta archive for you. So what it needs three capabilities you are looking for the retrieval. So you want to have a vast corpus the live internet or the live archive whatever you have then you want to have a synthesis report. So everything all the data that were replied here from multiple sources weave them together the findings into a coherent long form narrative and then you want and this is the most important constraint absolute verifiability. So whatever you are claiming here and whatever you are citing those must really exist. So the citing sources must be accurate on every claim and must be back traced back to its origin. You must have references. Great. So the authors here Stanford and UC Berkeley built here an automated data pipeline directly and they said you know what we not just for the internet let's go here for science let's go to archive. So we want to have a fresh data set retrieved absolutely fresh here daily or every minute here from archive. So how we do this? So three topics the recency the quality control and diversity and here they also limited theirel to computer science as a topic and they made sure that they have a diverse top with 18 different domains in computer science. So yeah we are restricted to computer science but I think you will see the results they speak for themselves. Here's the paper published today August 27, 2025. Deep scholar bench live benchmark on automated evolation for generative research systems from Stanford University in UC Berkeley. And the beauty is not that they have now a new benchmark, but what the new benchmark shows us. It shows us the true difficulty AI systems have if they do have to do a research synthesis. Here's the result. I would say wait a minute explain here at first the metric and the benchmark methodology. Okay. So they decided to go here for a threedimensional differentiation to say we look for the knowledge synthesis of this new system for the retrieval quality. It's more or less a rack system. No. And then for the verifiability because we are working in science with archive. It must be really that we can site real sources and there's no hallucination. Great. And then you can see even in those three dimension they decided to go with two or three sub indicators. So let's have a look. So for the knowledge synthesis or what I call hey how well is this generated EI text written? Is it informative? They have two indicators. the organization where an LLM as a J performs here a pairwise comparison and then they have here a metric that assesses here the informationational completeness. So what you do you have now from your text here a human written um segment here and this is automatically if you want chucked up here by the eye so you get single sentences for example but a human like red at all introduced here the transformer architecture which relies here on a self attention mechanism now the eye might say hey this is a two long sentence there are too many objects too many subjects I reduce this I chunk it up here in nuggets so the first nugget is redford at introduce the transformer stop. This is it. So simple to verify and then it goes on and you see exactly what's happening. Now you have then this essential nuggets and you want to make sure that those really represent here in the I generated text what is the main content of the human written sentence or paragraph or paper. Then second dimension the retrieval quality. Simple in my words. Did the I system find the right papers on archive for my topic. Here we have three indicators. The relevance rate, the reference coverage or simply hey if the system found here the real important papers and you know importance is just one but we can also measure here how impactful this is with an impact um quote. So we have a document importance metric. So this is a proxy for retrieving impactful word that uses here the citation comes from a specific API. Great. And finally the most important for me we have to have verifiability. So whatever we claim there is a check against some other published work. So are the claims now actually supported here by the citation? And here they go with two indicators. the citation precision where they say hey does the cited source support at least one claim made in the eye sentence just one and then they are say hey let's go full power now now for the term claim coverage we want do the cited sources support all the claims made in the eye sentence so it's really every little detail that the eye writes out here supported here by let's say arrive references Great. Then they decided, you know, so we have now the idea of the benchmark, we have the metric and the subindicator of the benchmark. And they say, you know what, now we build a vehicle. So we can switch now the engine of this vehicle. So we can put in either a GPT 4.1 or we can put in a GPD4 Omni or we go with a clawed model as the engine. And they built here the deep scholar base. Now this was interesting because remember I told you this is your almost rag system. No they went here with a classical rag multi multi- aent structure. So at first you have the query generation. So an LLM generates now a set of diverse search query based on the human input. So whatever I have the AI the LLM tells me hey that was a little bit too complicated. I generate now a set of diver search queries and then I send this up. So those queries now AI generated queries now run against the search API in our example archive and then we have a filtering now a semantic filtering and now we use another LLM is used to filter here to retrieve documents and anything that is irrelevant we just want to ignore this no but we have still a lot of documents like say 200 documents left now after the filtering so we have to do a reranking because we want to go down to the top 20 no so we have another llm based step that now reranks in detail to the only 200 remaining documents and it's a new prioritization here. What are the most relevance only of these 200 and then if we have the top 20 we want to have a semantic aggregation of the content of all 20 papers. No. So we say now the final top 20 documents are fed now as a context to an LLM and again you can choose your LLM that you like with a detailed prompt and then synthesization and the LLM writes it the final related works action. So clearly structured data processing pipeline and you can switch on and off any LLM in any place that you want and they tested here variation of different LLMs and they say so what are now the best performance combination so you see pipeline performing research this is great they run this here with a GPD4 as the default and then they exchange and said hey what happens if I add here let's say an 03 or a cloud model to GPD4.1 one all the prompts for this single steps that you just noticed here you have here in detail in the annex here of the documentation of this new research paper like here I show you the revised ODS react agent prompt here for calling here a tool so a lot of additional data I just want to give you the result this is it again you see one two three dimension and then all the different sub indicator in this dimension. Yeah, I think we can forget about the opensource research system from 2014. 
Relevance score: -9.185

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: 85. So I give you here my summary. OpenI is really here the leader in crafting a well ststructured and seemingly comprehensive narrative. Open deep research really excels in this synthesis and in the retrieval task and we have here the numerical data for the highest organization score 85 and the highest nugget coverage 39. output are well organized and capture really the essential fact better than the competitors can perform. And for the retrieval quality also the winner here. If you look here at the relevance rate 62 covers the most important human-sided reference with a reference coins of8 and finds the most impactful paper.1 but and this is now a big but the performance of open research collapses in the third dimension and this is the most crucial aspect if you have some scientific writing trust and verifiability. If you just write something that is not verifiable. 
Relevance score: -10.381

----------------------------------------------------------------
-------------------- New Query at 2025-09-03T00:51:57.611017 -------------

        ## Task
        The user's query: how deepscholar methodology work?

        Answer the user's query based only on the retrieved context.
        Use chat history if relevant, but never go beyond the given context.
        Write a natural, context-based answer.


        ## Context
        Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Great. So verifiability here open research scores shockingly low. This is what Stanford and UC Berkeler tells us. Citation precision numerical value is only 39. The claim coverage is only.13. And they tell us this is one of the worst performers here. that why is this happening? They suggest here a system optimized for fluent plausible sounding text is great on the one hand. So it's excellent in synthesizing here story or story line. But if you work in science, if you want to be absolute precise, you want to reduce hallucinating to the max and you want to make sure making logical leaps that are not strictly supported by the cited sources of archive are not part of your document. Now unfortunately this is happening here with open deep research. So for purely scientific work where you have to have absolute precision this could turn out to be a fatal flaw. Now, interestingly, if we look here at the new matter, the deep scholar base as I just showed you and we put here an 03 as the engine into this benchmark, you see, interestingly for the knowledge synthesis, we have the exact same benchmark 857.857. If we look here at the nugget, we even have a better performance here at the sequential tokenization if you want. And and this is now interesting with this new method. Look at the variability from one we jump to 6. So yeah, wow. This is an impressive improvement against the commercial openi deep research performance. So this new method by Stanford and UC Berkeley even if you use it with an openi model you see in one of the indicators they are at least as good as the commercial models but they outperform the commercial model significantly in variability. Isn't this beautiful? And if you ask yourself hey why I have suddenly two LLMs I told you you can have multiple LLMs here in the pipeline. So they decided for the SAM filter and the SAM top K data they go with GBD 4.1 but for the heavyduty SAM the aggregation step they go here with an 03 or a Claude or a Gemini just to make sure we on the same page. So this is the process. So we have the filter process then the top K the top 20 papers then the final aggregation the final section generation and I told you I show you always the prompt and here you have the prompt. So whatever LLM you choose an 03 or a claude OPUS 4 or a Gemini 2.5 Pro this is here the prompt for the aggregation for the final aggregation here of your research. So now we can see the paper here highlights here the deep scholar base can achieve up to 6.3 times higher claim coverage. So one indicator and one dimension six times three better than openi deep research. Nice. So we do have new systems that really outperform the classical commercial blackbox systems. But on the other hand, if you want just a plausible, well-written draft and you are not in science and you are willing, if you are in science, to meticulously fact check every single claim, every single half sentence that EI writes for you, then if you as a human do this verification and correct it, then open EI is really maybe also interesting. However, if you are a scientist and you say, I will go with the AI system that I can trust at least partially, then the deep scholar base here with claude or gemini model as the engine is dramatically superior. If you look at the data from Stanford and UC Berkeley, the paper states also that no method achieves a score greater than.19 when you average across the metric. So this is not an incremental challenge. You know, go from 78% to 82%. No, we at 19%. This is the best model we have. So we have quite a monumental challenge in front of us to make our AI models better, more performance, more power. So there's a lot of work in front of us. And if you say okay, I don't like here the numerical values in a table. I want to see this here in a visualization. Here you have exactly all the different elements here in um yeah to radar visualization. But this is exactly the data here from the numerical table I just showed you. But if you prefer to have it here in the radar, beautiful. Here is it. 
Relevance score: -8.676

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Hello community so great that you are back. We have a new paper by Stanford University and UC Berkeley and it is all about science and a deep scholar AI system that is brand new. Now you know we have open-source research system no that do a research and synthesize known facts and new findings and you are familiar with this no for example here open scholar this is here the paper or if you go for another open source this is here storm or maybe you are familiar with deep researcher from April 2025 so we do have open systems but of course it's interesting how do they compare to the commercial system let's say openi deep research or we We have brand new search EIS where you remember MCP and here multi- aent system. So how do they compare here to multi- aent search EI systems in well let's say you do a research on the internet. Now for those we have now the study where they compare Llama forced to GPD 4.1 and 03 an Opus 4 by Claude and a Gemini 2.5 Pro. And if you ask me why not some other models, those are the set of models Stanford and UC Berkeley decided to perform their tests on. This is it. The framework they used here is here open deep search. The search with open-source reasoning agents and you remember we have here multi- aent system. We have an open reasoning agent here itself that interprets here the given human task and completes it here by orchestrating a sequence of action and one of those action might include here calling tools and one of those tools that is available to this agent is here the open search tool to connect to the internet and everything. So a multi- aent search utility and now we have a new paper, we have a new method, we have some new research benchmark results and some new ideas. So what is the task that we are looking at? Let's say you're a researcher and you have your title and maybe you have already an idea from a technical abstract. Yeah. And then you want to ask AI to write here the related work section of your publication for you. In the old times we call this a literature review. Have a look who published what in your particular specific domain, when it was published, what were the topics, how was it related to your work. Now everything should be done by the eye. Now this new system is called the deep scholar benchmark and yes it promises to generate here the deep the related work section the deep research on internet on archive on meta archive for you. So what it needs three capabilities you are looking for the retrieval. So you want to have a vast corpus the live internet or the live archive whatever you have then you want to have a synthesis report. So everything all the data that were replied here from multiple sources weave them together the findings into a coherent long form narrative and then you want and this is the most important constraint absolute verifiability. So whatever you are claiming here and whatever you are citing those must really exist. So the citing sources must be accurate on every claim and must be back traced back to its origin. You must have references. Great. So the authors here Stanford and UC Berkeley built here an automated data pipeline directly and they said you know what we not just for the internet let's go here for science let's go to archive. So we want to have a fresh data set retrieved absolutely fresh here daily or every minute here from archive. So how we do this? So three topics the recency the quality control and diversity and here they also limited theirel to computer science as a topic and they made sure that they have a diverse top with 18 different domains in computer science. So yeah we are restricted to computer science but I think you will see the results they speak for themselves. Here's the paper published today August 27, 2025. Deep scholar bench live benchmark on automated evolation for generative research systems from Stanford University in UC Berkeley. And the beauty is not that they have now a new benchmark, but what the new benchmark shows us. It shows us the true difficulty AI systems have if they do have to do a research synthesis. Here's the result. I would say wait a minute explain here at first the metric and the benchmark methodology. Okay. So they decided to go here for a threedimensional differentiation to say we look for the knowledge synthesis of this new system for the retrieval quality. It's more or less a rack system. No. And then for the verifiability because we are working in science with archive. It must be really that we can site real sources and there's no hallucination. Great. And then you can see even in those three dimension they decided to go with two or three sub indicators. So let's have a look. So for the knowledge synthesis or what I call hey how well is this generated EI text written? Is it informative? They have two indicators. the organization where an LLM as a J performs here a pairwise comparison and then they have here a metric that assesses here the informationational completeness. So what you do you have now from your text here a human written um segment here and this is automatically if you want chucked up here by the eye so you get single sentences for example but a human like red at all introduced here the transformer architecture which relies here on a self attention mechanism now the eye might say hey this is a two long sentence there are too many objects too many subjects I reduce this I chunk it up here in nuggets so the first nugget is redford at introduce the transformer stop. This is it. So simple to verify and then it goes on and you see exactly what's happening. Now you have then this essential nuggets and you want to make sure that those really represent here in the I generated text what is the main content of the human written sentence or paragraph or paper. Then second dimension the retrieval quality. Simple in my words. Did the I system find the right papers on archive for my topic. Here we have three indicators. The relevance rate, the reference coverage or simply hey if the system found here the real important papers and you know importance is just one but we can also measure here how impactful this is with an impact um quote. So we have a document importance metric. So this is a proxy for retrieving impactful word that uses here the citation comes from a specific API. Great. And finally the most important for me we have to have verifiability. So whatever we claim there is a check against some other published work. So are the claims now actually supported here by the citation? And here they go with two indicators. the citation precision where they say hey does the cited source support at least one claim made in the eye sentence just one and then they are say hey let's go full power now now for the term claim coverage we want do the cited sources support all the claims made in the eye sentence so it's really every little detail that the eye writes out here supported here by let's say arrive references Great. Then they decided, you know, so we have now the idea of the benchmark, we have the metric and the subindicator of the benchmark. And they say, you know what, now we build a vehicle. So we can switch now the engine of this vehicle. So we can put in either a GPT 4.1 or we can put in a GPD4 Omni or we go with a clawed model as the engine. And they built here the deep scholar base. Now this was interesting because remember I told you this is your almost rag system. No they went here with a classical rag multi multi- aent structure. So at first you have the query generation. So an LLM generates now a set of diverse search query based on the human input. So whatever I have the AI the LLM tells me hey that was a little bit too complicated. I generate now a set of diver search queries and then I send this up. So those queries now AI generated queries now run against the search API in our example archive and then we have a filtering now a semantic filtering and now we use another LLM is used to filter here to retrieve documents and anything that is irrelevant we just want to ignore this no but we have still a lot of documents like say 200 documents left now after the filtering so we have to do a reranking because we want to go down to the top 20 no so we have another llm based step that now reranks in detail to the only 200 remaining documents and it's a new prioritization here. What are the most relevance only of these 200 and then if we have the top 20 we want to have a semantic aggregation of the content of all 20 papers. No. So we say now the final top 20 documents are fed now as a context to an LLM and again you can choose your LLM that you like with a detailed prompt and then synthesization and the LLM writes it the final related works action. So clearly structured data processing pipeline and you can switch on and off any LLM in any place that you want and they tested here variation of different LLMs and they say so what are now the best performance combination so you see pipeline performing research this is great they run this here with a GPD4 as the default and then they exchange and said hey what happens if I add here let's say an 03 or a cloud model to GPD4.1 one all the prompts for this single steps that you just noticed here you have here in detail in the annex here of the documentation of this new research paper like here I show you the revised ODS react agent prompt here for calling here a tool so a lot of additional data I just want to give you the result this is it again you see one two three dimension and then all the different sub indicator in this dimension. Yeah, I think we can forget about the opensource research system from 2014. 
Relevance score: -9.185

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: 85. So I give you here my summary. OpenI is really here the leader in crafting a well ststructured and seemingly comprehensive narrative. Open deep research really excels in this synthesis and in the retrieval task and we have here the numerical data for the highest organization score 85 and the highest nugget coverage 39. output are well organized and capture really the essential fact better than the competitors can perform. And for the retrieval quality also the winner here. If you look here at the relevance rate 62 covers the most important human-sided reference with a reference coins of8 and finds the most impactful paper.1 but and this is now a big but the performance of open research collapses in the third dimension and this is the most crucial aspect if you have some scientific writing trust and verifiability. If you just write something that is not verifiable. 
Relevance score: -10.381

----------------------------------------------------------------
-------------------- New Query at 2025-09-03T00:53:29.056114 -------------

        ## Task
        The user's query: how deepscholar methodology work?

        Answer the user's query based only on the retrieved context.
        Use chat history if relevant, but never go beyond the given context.
        Write a natural, context-based answer.


        ## Context
        Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Great. So verifiability here open research scores shockingly low. This is what Stanford and UC Berkeler tells us. Citation precision numerical value is only 39. The claim coverage is only.13. And they tell us this is one of the worst performers here. that why is this happening? They suggest here a system optimized for fluent plausible sounding text is great on the one hand. So it's excellent in synthesizing here story or story line. But if you work in science, if you want to be absolute precise, you want to reduce hallucinating to the max and you want to make sure making logical leaps that are not strictly supported by the cited sources of archive are not part of your document. Now unfortunately this is happening here with open deep research. So for purely scientific work where you have to have absolute precision this could turn out to be a fatal flaw. Now, interestingly, if we look here at the new matter, the deep scholar base as I just showed you and we put here an 03 as the engine into this benchmark, you see, interestingly for the knowledge synthesis, we have the exact same benchmark 857.857. If we look here at the nugget, we even have a better performance here at the sequential tokenization if you want. And and this is now interesting with this new method. Look at the variability from one we jump to 6. So yeah, wow. This is an impressive improvement against the commercial openi deep research performance. So this new method by Stanford and UC Berkeley even if you use it with an openi model you see in one of the indicators they are at least as good as the commercial models but they outperform the commercial model significantly in variability. Isn't this beautiful? And if you ask yourself hey why I have suddenly two LLMs I told you you can have multiple LLMs here in the pipeline. So they decided for the SAM filter and the SAM top K data they go with GBD 4.1 but for the heavyduty SAM the aggregation step they go here with an 03 or a Claude or a Gemini just to make sure we on the same page. So this is the process. So we have the filter process then the top K the top 20 papers then the final aggregation the final section generation and I told you I show you always the prompt and here you have the prompt. So whatever LLM you choose an 03 or a claude OPUS 4 or a Gemini 2.5 Pro this is here the prompt for the aggregation for the final aggregation here of your research. So now we can see the paper here highlights here the deep scholar base can achieve up to 6.3 times higher claim coverage. So one indicator and one dimension six times three better than openi deep research. Nice. So we do have new systems that really outperform the classical commercial blackbox systems. But on the other hand, if you want just a plausible, well-written draft and you are not in science and you are willing, if you are in science, to meticulously fact check every single claim, every single half sentence that EI writes for you, then if you as a human do this verification and correct it, then open EI is really maybe also interesting. However, if you are a scientist and you say, I will go with the AI system that I can trust at least partially, then the deep scholar base here with claude or gemini model as the engine is dramatically superior. If you look at the data from Stanford and UC Berkeley, the paper states also that no method achieves a score greater than.19 when you average across the metric. So this is not an incremental challenge. You know, go from 78% to 82%. No, we at 19%. This is the best model we have. So we have quite a monumental challenge in front of us to make our AI models better, more performance, more power. So there's a lot of work in front of us. And if you say okay, I don't like here the numerical values in a table. I want to see this here in a visualization. Here you have exactly all the different elements here in um yeah to radar visualization. But this is exactly the data here from the numerical table I just showed you. But if you prefer to have it here in the radar, beautiful. Here is it. 
Relevance score: -8.676

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Hello community so great that you are back. We have a new paper by Stanford University and UC Berkeley and it is all about science and a deep scholar AI system that is brand new. Now you know we have open-source research system no that do a research and synthesize known facts and new findings and you are familiar with this no for example here open scholar this is here the paper or if you go for another open source this is here storm or maybe you are familiar with deep researcher from April 2025 so we do have open systems but of course it's interesting how do they compare to the commercial system let's say openi deep research or we We have brand new search EIS where you remember MCP and here multi- aent system. So how do they compare here to multi- aent search EI systems in well let's say you do a research on the internet. Now for those we have now the study where they compare Llama forced to GPD 4.1 and 03 an Opus 4 by Claude and a Gemini 2.5 Pro. And if you ask me why not some other models, those are the set of models Stanford and UC Berkeley decided to perform their tests on. This is it. The framework they used here is here open deep search. The search with open-source reasoning agents and you remember we have here multi- aent system. We have an open reasoning agent here itself that interprets here the given human task and completes it here by orchestrating a sequence of action and one of those action might include here calling tools and one of those tools that is available to this agent is here the open search tool to connect to the internet and everything. So a multi- aent search utility and now we have a new paper, we have a new method, we have some new research benchmark results and some new ideas. So what is the task that we are looking at? Let's say you're a researcher and you have your title and maybe you have already an idea from a technical abstract. Yeah. And then you want to ask AI to write here the related work section of your publication for you. In the old times we call this a literature review. Have a look who published what in your particular specific domain, when it was published, what were the topics, how was it related to your work. Now everything should be done by the eye. Now this new system is called the deep scholar benchmark and yes it promises to generate here the deep the related work section the deep research on internet on archive on meta archive for you. So what it needs three capabilities you are looking for the retrieval. So you want to have a vast corpus the live internet or the live archive whatever you have then you want to have a synthesis report. So everything all the data that were replied here from multiple sources weave them together the findings into a coherent long form narrative and then you want and this is the most important constraint absolute verifiability. So whatever you are claiming here and whatever you are citing those must really exist. So the citing sources must be accurate on every claim and must be back traced back to its origin. You must have references. Great. So the authors here Stanford and UC Berkeley built here an automated data pipeline directly and they said you know what we not just for the internet let's go here for science let's go to archive. So we want to have a fresh data set retrieved absolutely fresh here daily or every minute here from archive. So how we do this? So three topics the recency the quality control and diversity and here they also limited theirel to computer science as a topic and they made sure that they have a diverse top with 18 different domains in computer science. So yeah we are restricted to computer science but I think you will see the results they speak for themselves. Here's the paper published today August 27, 2025. Deep scholar bench live benchmark on automated evolation for generative research systems from Stanford University in UC Berkeley. And the beauty is not that they have now a new benchmark, but what the new benchmark shows us. It shows us the true difficulty AI systems have if they do have to do a research synthesis. Here's the result. I would say wait a minute explain here at first the metric and the benchmark methodology. Okay. So they decided to go here for a threedimensional differentiation to say we look for the knowledge synthesis of this new system for the retrieval quality. It's more or less a rack system. No. And then for the verifiability because we are working in science with archive. It must be really that we can site real sources and there's no hallucination. Great. And then you can see even in those three dimension they decided to go with two or three sub indicators. So let's have a look. So for the knowledge synthesis or what I call hey how well is this generated EI text written? Is it informative? They have two indicators. the organization where an LLM as a J performs here a pairwise comparison and then they have here a metric that assesses here the informationational completeness. So what you do you have now from your text here a human written um segment here and this is automatically if you want chucked up here by the eye so you get single sentences for example but a human like red at all introduced here the transformer architecture which relies here on a self attention mechanism now the eye might say hey this is a two long sentence there are too many objects too many subjects I reduce this I chunk it up here in nuggets so the first nugget is redford at introduce the transformer stop. This is it. So simple to verify and then it goes on and you see exactly what's happening. Now you have then this essential nuggets and you want to make sure that those really represent here in the I generated text what is the main content of the human written sentence or paragraph or paper. Then second dimension the retrieval quality. Simple in my words. Did the I system find the right papers on archive for my topic. Here we have three indicators. The relevance rate, the reference coverage or simply hey if the system found here the real important papers and you know importance is just one but we can also measure here how impactful this is with an impact um quote. So we have a document importance metric. So this is a proxy for retrieving impactful word that uses here the citation comes from a specific API. Great. And finally the most important for me we have to have verifiability. So whatever we claim there is a check against some other published work. So are the claims now actually supported here by the citation? And here they go with two indicators. the citation precision where they say hey does the cited source support at least one claim made in the eye sentence just one and then they are say hey let's go full power now now for the term claim coverage we want do the cited sources support all the claims made in the eye sentence so it's really every little detail that the eye writes out here supported here by let's say arrive references Great. Then they decided, you know, so we have now the idea of the benchmark, we have the metric and the subindicator of the benchmark. And they say, you know what, now we build a vehicle. So we can switch now the engine of this vehicle. So we can put in either a GPT 4.1 or we can put in a GPD4 Omni or we go with a clawed model as the engine. And they built here the deep scholar base. Now this was interesting because remember I told you this is your almost rag system. No they went here with a classical rag multi multi- aent structure. So at first you have the query generation. So an LLM generates now a set of diverse search query based on the human input. So whatever I have the AI the LLM tells me hey that was a little bit too complicated. I generate now a set of diver search queries and then I send this up. So those queries now AI generated queries now run against the search API in our example archive and then we have a filtering now a semantic filtering and now we use another LLM is used to filter here to retrieve documents and anything that is irrelevant we just want to ignore this no but we have still a lot of documents like say 200 documents left now after the filtering so we have to do a reranking because we want to go down to the top 20 no so we have another llm based step that now reranks in detail to the only 200 remaining documents and it's a new prioritization here. What are the most relevance only of these 200 and then if we have the top 20 we want to have a semantic aggregation of the content of all 20 papers. No. So we say now the final top 20 documents are fed now as a context to an LLM and again you can choose your LLM that you like with a detailed prompt and then synthesization and the LLM writes it the final related works action. So clearly structured data processing pipeline and you can switch on and off any LLM in any place that you want and they tested here variation of different LLMs and they say so what are now the best performance combination so you see pipeline performing research this is great they run this here with a GPD4 as the default and then they exchange and said hey what happens if I add here let's say an 03 or a cloud model to GPD4.1 one all the prompts for this single steps that you just noticed here you have here in detail in the annex here of the documentation of this new research paper like here I show you the revised ODS react agent prompt here for calling here a tool so a lot of additional data I just want to give you the result this is it again you see one two three dimension and then all the different sub indicator in this dimension. Yeah, I think we can forget about the opensource research system from 2014. 
Relevance score: -9.185

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: 85. So I give you here my summary. OpenI is really here the leader in crafting a well ststructured and seemingly comprehensive narrative. Open deep research really excels in this synthesis and in the retrieval task and we have here the numerical data for the highest organization score 85 and the highest nugget coverage 39. output are well organized and capture really the essential fact better than the competitors can perform. And for the retrieval quality also the winner here. If you look here at the relevance rate 62 covers the most important human-sided reference with a reference coins of8 and finds the most impactful paper.1 but and this is now a big but the performance of open research collapses in the third dimension and this is the most crucial aspect if you have some scientific writing trust and verifiability. If you just write something that is not verifiable. 
Relevance score: -10.381

----------------------------------------------------------------
-------------------- New Query at 2025-09-03T00:54:57.021417 -------------

        ## Task
        The user's query: how deepscholar methodology work?

        Answer the user's query based only on the retrieved context.
        Use chat history if relevant, but never go beyond the given context.
        Write a natural, context-based answer.


        ## Context
        Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Great. So verifiability here open research scores shockingly low. This is what Stanford and UC Berkeler tells us. Citation precision numerical value is only 39. The claim coverage is only.13. And they tell us this is one of the worst performers here. that why is this happening? They suggest here a system optimized for fluent plausible sounding text is great on the one hand. So it's excellent in synthesizing here story or story line. But if you work in science, if you want to be absolute precise, you want to reduce hallucinating to the max and you want to make sure making logical leaps that are not strictly supported by the cited sources of archive are not part of your document. Now unfortunately this is happening here with open deep research. So for purely scientific work where you have to have absolute precision this could turn out to be a fatal flaw. Now, interestingly, if we look here at the new matter, the deep scholar base as I just showed you and we put here an 03 as the engine into this benchmark, you see, interestingly for the knowledge synthesis, we have the exact same benchmark 857.857. If we look here at the nugget, we even have a better performance here at the sequential tokenization if you want. And and this is now interesting with this new method. Look at the variability from one we jump to 6. So yeah, wow. This is an impressive improvement against the commercial openi deep research performance. So this new method by Stanford and UC Berkeley even if you use it with an openi model you see in one of the indicators they are at least as good as the commercial models but they outperform the commercial model significantly in variability. Isn't this beautiful? And if you ask yourself hey why I have suddenly two LLMs I told you you can have multiple LLMs here in the pipeline. So they decided for the SAM filter and the SAM top K data they go with GBD 4.1 but for the heavyduty SAM the aggregation step they go here with an 03 or a Claude or a Gemini just to make sure we on the same page. So this is the process. So we have the filter process then the top K the top 20 papers then the final aggregation the final section generation and I told you I show you always the prompt and here you have the prompt. So whatever LLM you choose an 03 or a claude OPUS 4 or a Gemini 2.5 Pro this is here the prompt for the aggregation for the final aggregation here of your research. So now we can see the paper here highlights here the deep scholar base can achieve up to 6.3 times higher claim coverage. So one indicator and one dimension six times three better than openi deep research. Nice. So we do have new systems that really outperform the classical commercial blackbox systems. But on the other hand, if you want just a plausible, well-written draft and you are not in science and you are willing, if you are in science, to meticulously fact check every single claim, every single half sentence that EI writes for you, then if you as a human do this verification and correct it, then open EI is really maybe also interesting. However, if you are a scientist and you say, I will go with the AI system that I can trust at least partially, then the deep scholar base here with claude or gemini model as the engine is dramatically superior. If you look at the data from Stanford and UC Berkeley, the paper states also that no method achieves a score greater than.19 when you average across the metric. So this is not an incremental challenge. You know, go from 78% to 82%. No, we at 19%. This is the best model we have. So we have quite a monumental challenge in front of us to make our AI models better, more performance, more power. So there's a lot of work in front of us. And if you say okay, I don't like here the numerical values in a table. I want to see this here in a visualization. Here you have exactly all the different elements here in um yeah to radar visualization. But this is exactly the data here from the numerical table I just showed you. But if you prefer to have it here in the radar, beautiful. Here is it. 
Relevance score: -8.676

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Hello community so great that you are back. We have a new paper by Stanford University and UC Berkeley and it is all about science and a deep scholar AI system that is brand new. Now you know we have open-source research system no that do a research and synthesize known facts and new findings and you are familiar with this no for example here open scholar this is here the paper or if you go for another open source this is here storm or maybe you are familiar with deep researcher from April 2025 so we do have open systems but of course it's interesting how do they compare to the commercial system let's say openi deep research or we We have brand new search EIS where you remember MCP and here multi- aent system. So how do they compare here to multi- aent search EI systems in well let's say you do a research on the internet. Now for those we have now the study where they compare Llama forced to GPD 4.1 and 03 an Opus 4 by Claude and a Gemini 2.5 Pro. And if you ask me why not some other models, those are the set of models Stanford and UC Berkeley decided to perform their tests on. This is it. The framework they used here is here open deep search. The search with open-source reasoning agents and you remember we have here multi- aent system. We have an open reasoning agent here itself that interprets here the given human task and completes it here by orchestrating a sequence of action and one of those action might include here calling tools and one of those tools that is available to this agent is here the open search tool to connect to the internet and everything. So a multi- aent search utility and now we have a new paper, we have a new method, we have some new research benchmark results and some new ideas. So what is the task that we are looking at? Let's say you're a researcher and you have your title and maybe you have already an idea from a technical abstract. Yeah. And then you want to ask AI to write here the related work section of your publication for you. In the old times we call this a literature review. Have a look who published what in your particular specific domain, when it was published, what were the topics, how was it related to your work. Now everything should be done by the eye. Now this new system is called the deep scholar benchmark and yes it promises to generate here the deep the related work section the deep research on internet on archive on meta archive for you. So what it needs three capabilities you are looking for the retrieval. So you want to have a vast corpus the live internet or the live archive whatever you have then you want to have a synthesis report. So everything all the data that were replied here from multiple sources weave them together the findings into a coherent long form narrative and then you want and this is the most important constraint absolute verifiability. So whatever you are claiming here and whatever you are citing those must really exist. So the citing sources must be accurate on every claim and must be back traced back to its origin. You must have references. Great. So the authors here Stanford and UC Berkeley built here an automated data pipeline directly and they said you know what we not just for the internet let's go here for science let's go to archive. So we want to have a fresh data set retrieved absolutely fresh here daily or every minute here from archive. So how we do this? So three topics the recency the quality control and diversity and here they also limited theirel to computer science as a topic and they made sure that they have a diverse top with 18 different domains in computer science. So yeah we are restricted to computer science but I think you will see the results they speak for themselves. Here's the paper published today August 27, 2025. Deep scholar bench live benchmark on automated evolation for generative research systems from Stanford University in UC Berkeley. And the beauty is not that they have now a new benchmark, but what the new benchmark shows us. It shows us the true difficulty AI systems have if they do have to do a research synthesis. Here's the result. I would say wait a minute explain here at first the metric and the benchmark methodology. Okay. So they decided to go here for a threedimensional differentiation to say we look for the knowledge synthesis of this new system for the retrieval quality. It's more or less a rack system. No. And then for the verifiability because we are working in science with archive. It must be really that we can site real sources and there's no hallucination. Great. And then you can see even in those three dimension they decided to go with two or three sub indicators. So let's have a look. So for the knowledge synthesis or what I call hey how well is this generated EI text written? Is it informative? They have two indicators. the organization where an LLM as a J performs here a pairwise comparison and then they have here a metric that assesses here the informationational completeness. So what you do you have now from your text here a human written um segment here and this is automatically if you want chucked up here by the eye so you get single sentences for example but a human like red at all introduced here the transformer architecture which relies here on a self attention mechanism now the eye might say hey this is a two long sentence there are too many objects too many subjects I reduce this I chunk it up here in nuggets so the first nugget is redford at introduce the transformer stop. This is it. So simple to verify and then it goes on and you see exactly what's happening. Now you have then this essential nuggets and you want to make sure that those really represent here in the I generated text what is the main content of the human written sentence or paragraph or paper. Then second dimension the retrieval quality. Simple in my words. Did the I system find the right papers on archive for my topic. Here we have three indicators. The relevance rate, the reference coverage or simply hey if the system found here the real important papers and you know importance is just one but we can also measure here how impactful this is with an impact um quote. So we have a document importance metric. So this is a proxy for retrieving impactful word that uses here the citation comes from a specific API. Great. And finally the most important for me we have to have verifiability. So whatever we claim there is a check against some other published work. So are the claims now actually supported here by the citation? And here they go with two indicators. the citation precision where they say hey does the cited source support at least one claim made in the eye sentence just one and then they are say hey let's go full power now now for the term claim coverage we want do the cited sources support all the claims made in the eye sentence so it's really every little detail that the eye writes out here supported here by let's say arrive references Great. Then they decided, you know, so we have now the idea of the benchmark, we have the metric and the subindicator of the benchmark. And they say, you know what, now we build a vehicle. So we can switch now the engine of this vehicle. So we can put in either a GPT 4.1 or we can put in a GPD4 Omni or we go with a clawed model as the engine. And they built here the deep scholar base. Now this was interesting because remember I told you this is your almost rag system. No they went here with a classical rag multi multi- aent structure. So at first you have the query generation. So an LLM generates now a set of diverse search query based on the human input. So whatever I have the AI the LLM tells me hey that was a little bit too complicated. I generate now a set of diver search queries and then I send this up. So those queries now AI generated queries now run against the search API in our example archive and then we have a filtering now a semantic filtering and now we use another LLM is used to filter here to retrieve documents and anything that is irrelevant we just want to ignore this no but we have still a lot of documents like say 200 documents left now after the filtering so we have to do a reranking because we want to go down to the top 20 no so we have another llm based step that now reranks in detail to the only 200 remaining documents and it's a new prioritization here. What are the most relevance only of these 200 and then if we have the top 20 we want to have a semantic aggregation of the content of all 20 papers. No. So we say now the final top 20 documents are fed now as a context to an LLM and again you can choose your LLM that you like with a detailed prompt and then synthesization and the LLM writes it the final related works action. So clearly structured data processing pipeline and you can switch on and off any LLM in any place that you want and they tested here variation of different LLMs and they say so what are now the best performance combination so you see pipeline performing research this is great they run this here with a GPD4 as the default and then they exchange and said hey what happens if I add here let's say an 03 or a cloud model to GPD4.1 one all the prompts for this single steps that you just noticed here you have here in detail in the annex here of the documentation of this new research paper like here I show you the revised ODS react agent prompt here for calling here a tool so a lot of additional data I just want to give you the result this is it again you see one two three dimension and then all the different sub indicator in this dimension. Yeah, I think we can forget about the opensource research system from 2014. 
Relevance score: -9.185

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: 85. So I give you here my summary. OpenI is really here the leader in crafting a well ststructured and seemingly comprehensive narrative. Open deep research really excels in this synthesis and in the retrieval task and we have here the numerical data for the highest organization score 85 and the highest nugget coverage 39. output are well organized and capture really the essential fact better than the competitors can perform. And for the retrieval quality also the winner here. If you look here at the relevance rate 62 covers the most important human-sided reference with a reference coins of8 and finds the most impactful paper.1 but and this is now a big but the performance of open research collapses in the third dimension and this is the most crucial aspect if you have some scientific writing trust and verifiability. If you just write something that is not verifiable. 
Relevance score: -10.381

----------------------------------------------------------------
-------------------- New Query at 2025-09-03T01:15:56.800203 -------------

        ## Task
        The user's query: how deepscholar methodology work?

        Answer the user's query based only on the retrieved context.
        Use chat history if relevant, but never go beyond the given context.
        Write a natural, context-based answer.


        ## Context
        Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Great. So verifiability here open research scores shockingly low. This is what Stanford and UC Berkeler tells us. Citation precision numerical value is only 39. The claim coverage is only.13. And they tell us this is one of the worst performers here. that why is this happening? They suggest here a system optimized for fluent plausible sounding text is great on the one hand. So it's excellent in synthesizing here story or story line. But if you work in science, if you want to be absolute precise, you want to reduce hallucinating to the max and you want to make sure making logical leaps that are not strictly supported by the cited sources of archive are not part of your document. Now unfortunately this is happening here with open deep research. So for purely scientific work where you have to have absolute precision this could turn out to be a fatal flaw. Now, interestingly, if we look here at the new matter, the deep scholar base as I just showed you and we put here an 03 as the engine into this benchmark, you see, interestingly for the knowledge synthesis, we have the exact same benchmark 857.857. If we look here at the nugget, we even have a better performance here at the sequential tokenization if you want. And and this is now interesting with this new method. Look at the variability from one we jump to 6. So yeah, wow. This is an impressive improvement against the commercial openi deep research performance. So this new method by Stanford and UC Berkeley even if you use it with an openi model you see in one of the indicators they are at least as good as the commercial models but they outperform the commercial model significantly in variability. Isn't this beautiful? And if you ask yourself hey why I have suddenly two LLMs I told you you can have multiple LLMs here in the pipeline. So they decided for the SAM filter and the SAM top K data they go with GBD 4.1 but for the heavyduty SAM the aggregation step they go here with an 03 or a Claude or a Gemini just to make sure we on the same page. So this is the process. So we have the filter process then the top K the top 20 papers then the final aggregation the final section generation and I told you I show you always the prompt and here you have the prompt. So whatever LLM you choose an 03 or a claude OPUS 4 or a Gemini 2.5 Pro this is here the prompt for the aggregation for the final aggregation here of your research. So now we can see the paper here highlights here the deep scholar base can achieve up to 6.3 times higher claim coverage. So one indicator and one dimension six times three better than openi deep research. Nice. So we do have new systems that really outperform the classical commercial blackbox systems. But on the other hand, if you want just a plausible, well-written draft and you are not in science and you are willing, if you are in science, to meticulously fact check every single claim, every single half sentence that EI writes for you, then if you as a human do this verification and correct it, then open EI is really maybe also interesting. However, if you are a scientist and you say, I will go with the AI system that I can trust at least partially, then the deep scholar base here with claude or gemini model as the engine is dramatically superior. If you look at the data from Stanford and UC Berkeley, the paper states also that no method achieves a score greater than.19 when you average across the metric. So this is not an incremental challenge. You know, go from 78% to 82%. No, we at 19%. This is the best model we have. So we have quite a monumental challenge in front of us to make our AI models better, more performance, more power. So there's a lot of work in front of us. And if you say okay, I don't like here the numerical values in a table. I want to see this here in a visualization. Here you have exactly all the different elements here in um yeah to radar visualization. But this is exactly the data here from the numerical table I just showed you. But if you prefer to have it here in the radar, beautiful. Here is it. 
Relevance score: -8.676

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Hello community so great that you are back. We have a new paper by Stanford University and UC Berkeley and it is all about science and a deep scholar AI system that is brand new. Now you know we have open-source research system no that do a research and synthesize known facts and new findings and you are familiar with this no for example here open scholar this is here the paper or if you go for another open source this is here storm or maybe you are familiar with deep researcher from April 2025 so we do have open systems but of course it's interesting how do they compare to the commercial system let's say openi deep research or we We have brand new search EIS where you remember MCP and here multi- aent system. So how do they compare here to multi- aent search EI systems in well let's say you do a research on the internet. Now for those we have now the study where they compare Llama forced to GPD 4.1 and 03 an Opus 4 by Claude and a Gemini 2.5 Pro. And if you ask me why not some other models, those are the set of models Stanford and UC Berkeley decided to perform their tests on. This is it. The framework they used here is here open deep search. The search with open-source reasoning agents and you remember we have here multi- aent system. We have an open reasoning agent here itself that interprets here the given human task and completes it here by orchestrating a sequence of action and one of those action might include here calling tools and one of those tools that is available to this agent is here the open search tool to connect to the internet and everything. So a multi- aent search utility and now we have a new paper, we have a new method, we have some new research benchmark results and some new ideas. So what is the task that we are looking at? Let's say you're a researcher and you have your title and maybe you have already an idea from a technical abstract. Yeah. And then you want to ask AI to write here the related work section of your publication for you. In the old times we call this a literature review. Have a look who published what in your particular specific domain, when it was published, what were the topics, how was it related to your work. Now everything should be done by the eye. Now this new system is called the deep scholar benchmark and yes it promises to generate here the deep the related work section the deep research on internet on archive on meta archive for you. So what it needs three capabilities you are looking for the retrieval. So you want to have a vast corpus the live internet or the live archive whatever you have then you want to have a synthesis report. So everything all the data that were replied here from multiple sources weave them together the findings into a coherent long form narrative and then you want and this is the most important constraint absolute verifiability. So whatever you are claiming here and whatever you are citing those must really exist. So the citing sources must be accurate on every claim and must be back traced back to its origin. You must have references. Great. So the authors here Stanford and UC Berkeley built here an automated data pipeline directly and they said you know what we not just for the internet let's go here for science let's go to archive. So we want to have a fresh data set retrieved absolutely fresh here daily or every minute here from archive. So how we do this? So three topics the recency the quality control and diversity and here they also limited theirel to computer science as a topic and they made sure that they have a diverse top with 18 different domains in computer science. So yeah we are restricted to computer science but I think you will see the results they speak for themselves. Here's the paper published today August 27, 2025. Deep scholar bench live benchmark on automated evolation for generative research systems from Stanford University in UC Berkeley. And the beauty is not that they have now a new benchmark, but what the new benchmark shows us. It shows us the true difficulty AI systems have if they do have to do a research synthesis. Here's the result. I would say wait a minute explain here at first the metric and the benchmark methodology. Okay. So they decided to go here for a threedimensional differentiation to say we look for the knowledge synthesis of this new system for the retrieval quality. It's more or less a rack system. No. And then for the verifiability because we are working in science with archive. It must be really that we can site real sources and there's no hallucination. Great. And then you can see even in those three dimension they decided to go with two or three sub indicators. So let's have a look. So for the knowledge synthesis or what I call hey how well is this generated EI text written? Is it informative? They have two indicators. the organization where an LLM as a J performs here a pairwise comparison and then they have here a metric that assesses here the informationational completeness. So what you do you have now from your text here a human written um segment here and this is automatically if you want chucked up here by the eye so you get single sentences for example but a human like red at all introduced here the transformer architecture which relies here on a self attention mechanism now the eye might say hey this is a two long sentence there are too many objects too many subjects I reduce this I chunk it up here in nuggets so the first nugget is redford at introduce the transformer stop. This is it. So simple to verify and then it goes on and you see exactly what's happening. Now you have then this essential nuggets and you want to make sure that those really represent here in the I generated text what is the main content of the human written sentence or paragraph or paper. Then second dimension the retrieval quality. Simple in my words. Did the I system find the right papers on archive for my topic. Here we have three indicators. The relevance rate, the reference coverage or simply hey if the system found here the real important papers and you know importance is just one but we can also measure here how impactful this is with an impact um quote. So we have a document importance metric. So this is a proxy for retrieving impactful word that uses here the citation comes from a specific API. Great. And finally the most important for me we have to have verifiability. So whatever we claim there is a check against some other published work. So are the claims now actually supported here by the citation? And here they go with two indicators. the citation precision where they say hey does the cited source support at least one claim made in the eye sentence just one and then they are say hey let's go full power now now for the term claim coverage we want do the cited sources support all the claims made in the eye sentence so it's really every little detail that the eye writes out here supported here by let's say arrive references Great. Then they decided, you know, so we have now the idea of the benchmark, we have the metric and the subindicator of the benchmark. And they say, you know what, now we build a vehicle. So we can switch now the engine of this vehicle. So we can put in either a GPT 4.1 or we can put in a GPD4 Omni or we go with a clawed model as the engine. And they built here the deep scholar base. Now this was interesting because remember I told you this is your almost rag system. No they went here with a classical rag multi multi- aent structure. So at first you have the query generation. So an LLM generates now a set of diverse search query based on the human input. So whatever I have the AI the LLM tells me hey that was a little bit too complicated. I generate now a set of diver search queries and then I send this up. So those queries now AI generated queries now run against the search API in our example archive and then we have a filtering now a semantic filtering and now we use another LLM is used to filter here to retrieve documents and anything that is irrelevant we just want to ignore this no but we have still a lot of documents like say 200 documents left now after the filtering so we have to do a reranking because we want to go down to the top 20 no so we have another llm based step that now reranks in detail to the only 200 remaining documents and it's a new prioritization here. What are the most relevance only of these 200 and then if we have the top 20 we want to have a semantic aggregation of the content of all 20 papers. No. So we say now the final top 20 documents are fed now as a context to an LLM and again you can choose your LLM that you like with a detailed prompt and then synthesization and the LLM writes it the final related works action. So clearly structured data processing pipeline and you can switch on and off any LLM in any place that you want and they tested here variation of different LLMs and they say so what are now the best performance combination so you see pipeline performing research this is great they run this here with a GPD4 as the default and then they exchange and said hey what happens if I add here let's say an 03 or a cloud model to GPD4.1 one all the prompts for this single steps that you just noticed here you have here in detail in the annex here of the documentation of this new research paper like here I show you the revised ODS react agent prompt here for calling here a tool so a lot of additional data I just want to give you the result this is it again you see one two three dimension and then all the different sub indicator in this dimension. Yeah, I think we can forget about the opensource research system from 2014. 
Relevance score: -9.185

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: 85. So I give you here my summary. OpenI is really here the leader in crafting a well ststructured and seemingly comprehensive narrative. Open deep research really excels in this synthesis and in the retrieval task and we have here the numerical data for the highest organization score 85 and the highest nugget coverage 39. output are well organized and capture really the essential fact better than the competitors can perform. And for the retrieval quality also the winner here. If you look here at the relevance rate 62 covers the most important human-sided reference with a reference coins of8 and finds the most impactful paper.1 but and this is now a big but the performance of open research collapses in the third dimension and this is the most crucial aspect if you have some scientific writing trust and verifiability. If you just write something that is not verifiable. 
Relevance score: -10.381

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: But just a warning, you always have to check your resources. There's always a possibility, even with the best current systems, that the system is hallucinating. It is referencing to archive preprints that are non-existent or it is coming up with an explanation that is logically incorrect. But otherwise, enjoy AI, subscribe to the channel, and I see you in the next video.
Relevance score: -10.981

----------------------------------------------------------------
-------------------- New Query at 2025-09-03T01:19:00.371105 -------------

        ## Task
        The user's query: how deepscholar methodology work?

        Answer the user's query based only on the retrieved context.
        Use chat history if relevant, but never go beyond the given context.
        Write a natural, context-based answer.


        ## Context
        Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Great. So verifiability here open research scores shockingly low. This is what Stanford and UC Berkeler tells us. Citation precision numerical value is only 39. The claim coverage is only.13. And they tell us this is one of the worst performers here. that why is this happening? They suggest here a system optimized for fluent plausible sounding text is great on the one hand. So it's excellent in synthesizing here story or story line. But if you work in science, if you want to be absolute precise, you want to reduce hallucinating to the max and you want to make sure making logical leaps that are not strictly supported by the cited sources of archive are not part of your document. Now unfortunately this is happening here with open deep research. So for purely scientific work where you have to have absolute precision this could turn out to be a fatal flaw. Now, interestingly, if we look here at the new matter, the deep scholar base as I just showed you and we put here an 03 as the engine into this benchmark, you see, interestingly for the knowledge synthesis, we have the exact same benchmark 857.857. If we look here at the nugget, we even have a better performance here at the sequential tokenization if you want. And and this is now interesting with this new method. Look at the variability from one we jump to 6. So yeah, wow. This is an impressive improvement against the commercial openi deep research performance. So this new method by Stanford and UC Berkeley even if you use it with an openi model you see in one of the indicators they are at least as good as the commercial models but they outperform the commercial model significantly in variability. Isn't this beautiful? And if you ask yourself hey why I have suddenly two LLMs I told you you can have multiple LLMs here in the pipeline. So they decided for the SAM filter and the SAM top K data they go with GBD 4.1 but for the heavyduty SAM the aggregation step they go here with an 03 or a Claude or a Gemini just to make sure we on the same page. So this is the process. So we have the filter process then the top K the top 20 papers then the final aggregation the final section generation and I told you I show you always the prompt and here you have the prompt. So whatever LLM you choose an 03 or a claude OPUS 4 or a Gemini 2.5 Pro this is here the prompt for the aggregation for the final aggregation here of your research. So now we can see the paper here highlights here the deep scholar base can achieve up to 6.3 times higher claim coverage. So one indicator and one dimension six times three better than openi deep research. Nice. So we do have new systems that really outperform the classical commercial blackbox systems. But on the other hand, if you want just a plausible, well-written draft and you are not in science and you are willing, if you are in science, to meticulously fact check every single claim, every single half sentence that EI writes for you, then if you as a human do this verification and correct it, then open EI is really maybe also interesting. However, if you are a scientist and you say, I will go with the AI system that I can trust at least partially, then the deep scholar base here with claude or gemini model as the engine is dramatically superior. If you look at the data from Stanford and UC Berkeley, the paper states also that no method achieves a score greater than.19 when you average across the metric. So this is not an incremental challenge. You know, go from 78% to 82%. No, we at 19%. This is the best model we have. So we have quite a monumental challenge in front of us to make our AI models better, more performance, more power. So there's a lot of work in front of us. And if you say okay, I don't like here the numerical values in a table. I want to see this here in a visualization. Here you have exactly all the different elements here in um yeah to radar visualization. But this is exactly the data here from the numerical table I just showed you. But if you prefer to have it here in the radar, beautiful. Here is it. 
Relevance score: -8.676

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Hello community so great that you are back. We have a new paper by Stanford University and UC Berkeley and it is all about science and a deep scholar AI system that is brand new. Now you know we have open-source research system no that do a research and synthesize known facts and new findings and you are familiar with this no for example here open scholar this is here the paper or if you go for another open source this is here storm or maybe you are familiar with deep researcher from April 2025 so we do have open systems but of course it's interesting how do they compare to the commercial system let's say openi deep research or we We have brand new search EIS where you remember MCP and here multi- aent system. So how do they compare here to multi- aent search EI systems in well let's say you do a research on the internet. Now for those we have now the study where they compare Llama forced to GPD 4.1 and 03 an Opus 4 by Claude and a Gemini 2.5 Pro. And if you ask me why not some other models, those are the set of models Stanford and UC Berkeley decided to perform their tests on. This is it. The framework they used here is here open deep search. The search with open-source reasoning agents and you remember we have here multi- aent system. We have an open reasoning agent here itself that interprets here the given human task and completes it here by orchestrating a sequence of action and one of those action might include here calling tools and one of those tools that is available to this agent is here the open search tool to connect to the internet and everything. So a multi- aent search utility and now we have a new paper, we have a new method, we have some new research benchmark results and some new ideas. So what is the task that we are looking at? Let's say you're a researcher and you have your title and maybe you have already an idea from a technical abstract. Yeah. And then you want to ask AI to write here the related work section of your publication for you. In the old times we call this a literature review. Have a look who published what in your particular specific domain, when it was published, what were the topics, how was it related to your work. Now everything should be done by the eye. Now this new system is called the deep scholar benchmark and yes it promises to generate here the deep the related work section the deep research on internet on archive on meta archive for you. So what it needs three capabilities you are looking for the retrieval. So you want to have a vast corpus the live internet or the live archive whatever you have then you want to have a synthesis report. So everything all the data that were replied here from multiple sources weave them together the findings into a coherent long form narrative and then you want and this is the most important constraint absolute verifiability. So whatever you are claiming here and whatever you are citing those must really exist. So the citing sources must be accurate on every claim and must be back traced back to its origin. You must have references. Great. So the authors here Stanford and UC Berkeley built here an automated data pipeline directly and they said you know what we not just for the internet let's go here for science let's go to archive. So we want to have a fresh data set retrieved absolutely fresh here daily or every minute here from archive. So how we do this? So three topics the recency the quality control and diversity and here they also limited theirel to computer science as a topic and they made sure that they have a diverse top with 18 different domains in computer science. So yeah we are restricted to computer science but I think you will see the results they speak for themselves. Here's the paper published today August 27, 2025. Deep scholar bench live benchmark on automated evolation for generative research systems from Stanford University in UC Berkeley. And the beauty is not that they have now a new benchmark, but what the new benchmark shows us. It shows us the true difficulty AI systems have if they do have to do a research synthesis. Here's the result. I would say wait a minute explain here at first the metric and the benchmark methodology. Okay. So they decided to go here for a threedimensional differentiation to say we look for the knowledge synthesis of this new system for the retrieval quality. It's more or less a rack system. No. And then for the verifiability because we are working in science with archive. It must be really that we can site real sources and there's no hallucination. Great. And then you can see even in those three dimension they decided to go with two or three sub indicators. So let's have a look. So for the knowledge synthesis or what I call hey how well is this generated EI text written? Is it informative? They have two indicators. the organization where an LLM as a J performs here a pairwise comparison and then they have here a metric that assesses here the informationational completeness. So what you do you have now from your text here a human written um segment here and this is automatically if you want chucked up here by the eye so you get single sentences for example but a human like red at all introduced here the transformer architecture which relies here on a self attention mechanism now the eye might say hey this is a two long sentence there are too many objects too many subjects I reduce this I chunk it up here in nuggets so the first nugget is redford at introduce the transformer stop. This is it. So simple to verify and then it goes on and you see exactly what's happening. Now you have then this essential nuggets and you want to make sure that those really represent here in the I generated text what is the main content of the human written sentence or paragraph or paper. Then second dimension the retrieval quality. Simple in my words. Did the I system find the right papers on archive for my topic. Here we have three indicators. The relevance rate, the reference coverage or simply hey if the system found here the real important papers and you know importance is just one but we can also measure here how impactful this is with an impact um quote. So we have a document importance metric. So this is a proxy for retrieving impactful word that uses here the citation comes from a specific API. Great. And finally the most important for me we have to have verifiability. So whatever we claim there is a check against some other published work. So are the claims now actually supported here by the citation? And here they go with two indicators. the citation precision where they say hey does the cited source support at least one claim made in the eye sentence just one and then they are say hey let's go full power now now for the term claim coverage we want do the cited sources support all the claims made in the eye sentence so it's really every little detail that the eye writes out here supported here by let's say arrive references Great. Then they decided, you know, so we have now the idea of the benchmark, we have the metric and the subindicator of the benchmark. And they say, you know what, now we build a vehicle. So we can switch now the engine of this vehicle. So we can put in either a GPT 4.1 or we can put in a GPD4 Omni or we go with a clawed model as the engine. And they built here the deep scholar base. Now this was interesting because remember I told you this is your almost rag system. No they went here with a classical rag multi multi- aent structure. So at first you have the query generation. So an LLM generates now a set of diverse search query based on the human input. So whatever I have the AI the LLM tells me hey that was a little bit too complicated. I generate now a set of diver search queries and then I send this up. So those queries now AI generated queries now run against the search API in our example archive and then we have a filtering now a semantic filtering and now we use another LLM is used to filter here to retrieve documents and anything that is irrelevant we just want to ignore this no but we have still a lot of documents like say 200 documents left now after the filtering so we have to do a reranking because we want to go down to the top 20 no so we have another llm based step that now reranks in detail to the only 200 remaining documents and it's a new prioritization here. What are the most relevance only of these 200 and then if we have the top 20 we want to have a semantic aggregation of the content of all 20 papers. No. So we say now the final top 20 documents are fed now as a context to an LLM and again you can choose your LLM that you like with a detailed prompt and then synthesization and the LLM writes it the final related works action. So clearly structured data processing pipeline and you can switch on and off any LLM in any place that you want and they tested here variation of different LLMs and they say so what are now the best performance combination so you see pipeline performing research this is great they run this here with a GPD4 as the default and then they exchange and said hey what happens if I add here let's say an 03 or a cloud model to GPD4.1 one all the prompts for this single steps that you just noticed here you have here in detail in the annex here of the documentation of this new research paper like here I show you the revised ODS react agent prompt here for calling here a tool so a lot of additional data I just want to give you the result this is it again you see one two three dimension and then all the different sub indicator in this dimension. Yeah, I think we can forget about the opensource research system from 2014. 
Relevance score: -9.185

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: 85. So I give you here my summary. OpenI is really here the leader in crafting a well ststructured and seemingly comprehensive narrative. Open deep research really excels in this synthesis and in the retrieval task and we have here the numerical data for the highest organization score 85 and the highest nugget coverage 39. output are well organized and capture really the essential fact better than the competitors can perform. And for the retrieval quality also the winner here. If you look here at the relevance rate 62 covers the most important human-sided reference with a reference coins of8 and finds the most impactful paper.1 but and this is now a big but the performance of open research collapses in the third dimension and this is the most crucial aspect if you have some scientific writing trust and verifiability. If you just write something that is not verifiable. 
Relevance score: -10.381

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: But just a warning, you always have to check your resources. There's always a possibility, even with the best current systems, that the system is hallucinating. It is referencing to archive preprints that are non-existent or it is coming up with an explanation that is logically incorrect. But otherwise, enjoy AI, subscribe to the channel, and I see you in the next video.
Relevance score: -10.981

----------------------------------------------------------------
-------------------- New Query at 2025-09-03T01:20:33.920001 -------------

        ## Task
        The user's query: how deepscholar methodology work?

        Answer the user's query based only on the retrieved context.
        Use chat history if relevant, but never go beyond the given context.
        Write a natural, context-based answer.


        ## Context
        Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Great. So verifiability here open research scores shockingly low. This is what Stanford and UC Berkeler tells us. Citation precision numerical value is only 39. The claim coverage is only.13. And they tell us this is one of the worst performers here. that why is this happening? They suggest here a system optimized for fluent plausible sounding text is great on the one hand. So it's excellent in synthesizing here story or story line. But if you work in science, if you want to be absolute precise, you want to reduce hallucinating to the max and you want to make sure making logical leaps that are not strictly supported by the cited sources of archive are not part of your document. Now unfortunately this is happening here with open deep research. So for purely scientific work where you have to have absolute precision this could turn out to be a fatal flaw. Now, interestingly, if we look here at the new matter, the deep scholar base as I just showed you and we put here an 03 as the engine into this benchmark, you see, interestingly for the knowledge synthesis, we have the exact same benchmark 857.857. If we look here at the nugget, we even have a better performance here at the sequential tokenization if you want. And and this is now interesting with this new method. Look at the variability from one we jump to 6. So yeah, wow. This is an impressive improvement against the commercial openi deep research performance. So this new method by Stanford and UC Berkeley even if you use it with an openi model you see in one of the indicators they are at least as good as the commercial models but they outperform the commercial model significantly in variability. Isn't this beautiful? And if you ask yourself hey why I have suddenly two LLMs I told you you can have multiple LLMs here in the pipeline. So they decided for the SAM filter and the SAM top K data they go with GBD 4.1 but for the heavyduty SAM the aggregation step they go here with an 03 or a Claude or a Gemini just to make sure we on the same page. So this is the process. So we have the filter process then the top K the top 20 papers then the final aggregation the final section generation and I told you I show you always the prompt and here you have the prompt. So whatever LLM you choose an 03 or a claude OPUS 4 or a Gemini 2.5 Pro this is here the prompt for the aggregation for the final aggregation here of your research. So now we can see the paper here highlights here the deep scholar base can achieve up to 6.3 times higher claim coverage. So one indicator and one dimension six times three better than openi deep research. Nice. So we do have new systems that really outperform the classical commercial blackbox systems. But on the other hand, if you want just a plausible, well-written draft and you are not in science and you are willing, if you are in science, to meticulously fact check every single claim, every single half sentence that EI writes for you, then if you as a human do this verification and correct it, then open EI is really maybe also interesting. However, if you are a scientist and you say, I will go with the AI system that I can trust at least partially, then the deep scholar base here with claude or gemini model as the engine is dramatically superior. If you look at the data from Stanford and UC Berkeley, the paper states also that no method achieves a score greater than.19 when you average across the metric. So this is not an incremental challenge. You know, go from 78% to 82%. No, we at 19%. This is the best model we have. So we have quite a monumental challenge in front of us to make our AI models better, more performance, more power. So there's a lot of work in front of us. And if you say okay, I don't like here the numerical values in a table. I want to see this here in a visualization. Here you have exactly all the different elements here in um yeah to radar visualization. But this is exactly the data here from the numerical table I just showed you. But if you prefer to have it here in the radar, beautiful. Here is it. 
Relevance score: -8.676

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Hello community so great that you are back. We have a new paper by Stanford University and UC Berkeley and it is all about science and a deep scholar AI system that is brand new. Now you know we have open-source research system no that do a research and synthesize known facts and new findings and you are familiar with this no for example here open scholar this is here the paper or if you go for another open source this is here storm or maybe you are familiar with deep researcher from April 2025 so we do have open systems but of course it's interesting how do they compare to the commercial system let's say openi deep research or we We have brand new search EIS where you remember MCP and here multi- aent system. So how do they compare here to multi- aent search EI systems in well let's say you do a research on the internet. Now for those we have now the study where they compare Llama forced to GPD 4.1 and 03 an Opus 4 by Claude and a Gemini 2.5 Pro. And if you ask me why not some other models, those are the set of models Stanford and UC Berkeley decided to perform their tests on. This is it. The framework they used here is here open deep search. The search with open-source reasoning agents and you remember we have here multi- aent system. We have an open reasoning agent here itself that interprets here the given human task and completes it here by orchestrating a sequence of action and one of those action might include here calling tools and one of those tools that is available to this agent is here the open search tool to connect to the internet and everything. So a multi- aent search utility and now we have a new paper, we have a new method, we have some new research benchmark results and some new ideas. So what is the task that we are looking at? Let's say you're a researcher and you have your title and maybe you have already an idea from a technical abstract. Yeah. And then you want to ask AI to write here the related work section of your publication for you. In the old times we call this a literature review. Have a look who published what in your particular specific domain, when it was published, what were the topics, how was it related to your work. Now everything should be done by the eye. Now this new system is called the deep scholar benchmark and yes it promises to generate here the deep the related work section the deep research on internet on archive on meta archive for you. So what it needs three capabilities you are looking for the retrieval. So you want to have a vast corpus the live internet or the live archive whatever you have then you want to have a synthesis report. So everything all the data that were replied here from multiple sources weave them together the findings into a coherent long form narrative and then you want and this is the most important constraint absolute verifiability. So whatever you are claiming here and whatever you are citing those must really exist. So the citing sources must be accurate on every claim and must be back traced back to its origin. You must have references. Great. So the authors here Stanford and UC Berkeley built here an automated data pipeline directly and they said you know what we not just for the internet let's go here for science let's go to archive. So we want to have a fresh data set retrieved absolutely fresh here daily or every minute here from archive. So how we do this? So three topics the recency the quality control and diversity and here they also limited theirel to computer science as a topic and they made sure that they have a diverse top with 18 different domains in computer science. So yeah we are restricted to computer science but I think you will see the results they speak for themselves. Here's the paper published today August 27, 2025. Deep scholar bench live benchmark on automated evolation for generative research systems from Stanford University in UC Berkeley. And the beauty is not that they have now a new benchmark, but what the new benchmark shows us. It shows us the true difficulty AI systems have if they do have to do a research synthesis. Here's the result. I would say wait a minute explain here at first the metric and the benchmark methodology. Okay. So they decided to go here for a threedimensional differentiation to say we look for the knowledge synthesis of this new system for the retrieval quality. It's more or less a rack system. No. And then for the verifiability because we are working in science with archive. It must be really that we can site real sources and there's no hallucination. Great. And then you can see even in those three dimension they decided to go with two or three sub indicators. So let's have a look. So for the knowledge synthesis or what I call hey how well is this generated EI text written? Is it informative? They have two indicators. the organization where an LLM as a J performs here a pairwise comparison and then they have here a metric that assesses here the informationational completeness. So what you do you have now from your text here a human written um segment here and this is automatically if you want chucked up here by the eye so you get single sentences for example but a human like red at all introduced here the transformer architecture which relies here on a self attention mechanism now the eye might say hey this is a two long sentence there are too many objects too many subjects I reduce this I chunk it up here in nuggets so the first nugget is redford at introduce the transformer stop. This is it. So simple to verify and then it goes on and you see exactly what's happening. Now you have then this essential nuggets and you want to make sure that those really represent here in the I generated text what is the main content of the human written sentence or paragraph or paper. Then second dimension the retrieval quality. Simple in my words. Did the I system find the right papers on archive for my topic. Here we have three indicators. The relevance rate, the reference coverage or simply hey if the system found here the real important papers and you know importance is just one but we can also measure here how impactful this is with an impact um quote. So we have a document importance metric. So this is a proxy for retrieving impactful word that uses here the citation comes from a specific API. Great. And finally the most important for me we have to have verifiability. So whatever we claim there is a check against some other published work. So are the claims now actually supported here by the citation? And here they go with two indicators. the citation precision where they say hey does the cited source support at least one claim made in the eye sentence just one and then they are say hey let's go full power now now for the term claim coverage we want do the cited sources support all the claims made in the eye sentence so it's really every little detail that the eye writes out here supported here by let's say arrive references Great. Then they decided, you know, so we have now the idea of the benchmark, we have the metric and the subindicator of the benchmark. And they say, you know what, now we build a vehicle. So we can switch now the engine of this vehicle. So we can put in either a GPT 4.1 or we can put in a GPD4 Omni or we go with a clawed model as the engine. And they built here the deep scholar base. Now this was interesting because remember I told you this is your almost rag system. No they went here with a classical rag multi multi- aent structure. So at first you have the query generation. So an LLM generates now a set of diverse search query based on the human input. So whatever I have the AI the LLM tells me hey that was a little bit too complicated. I generate now a set of diver search queries and then I send this up. So those queries now AI generated queries now run against the search API in our example archive and then we have a filtering now a semantic filtering and now we use another LLM is used to filter here to retrieve documents and anything that is irrelevant we just want to ignore this no but we have still a lot of documents like say 200 documents left now after the filtering so we have to do a reranking because we want to go down to the top 20 no so we have another llm based step that now reranks in detail to the only 200 remaining documents and it's a new prioritization here. What are the most relevance only of these 200 and then if we have the top 20 we want to have a semantic aggregation of the content of all 20 papers. No. So we say now the final top 20 documents are fed now as a context to an LLM and again you can choose your LLM that you like with a detailed prompt and then synthesization and the LLM writes it the final related works action. So clearly structured data processing pipeline and you can switch on and off any LLM in any place that you want and they tested here variation of different LLMs and they say so what are now the best performance combination so you see pipeline performing research this is great they run this here with a GPD4 as the default and then they exchange and said hey what happens if I add here let's say an 03 or a cloud model to GPD4.1 one all the prompts for this single steps that you just noticed here you have here in detail in the annex here of the documentation of this new research paper like here I show you the revised ODS react agent prompt here for calling here a tool so a lot of additional data I just want to give you the result this is it again you see one two three dimension and then all the different sub indicator in this dimension. Yeah, I think we can forget about the opensource research system from 2014. 
Relevance score: -9.185

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: 85. So I give you here my summary. OpenI is really here the leader in crafting a well ststructured and seemingly comprehensive narrative. Open deep research really excels in this synthesis and in the retrieval task and we have here the numerical data for the highest organization score 85 and the highest nugget coverage 39. output are well organized and capture really the essential fact better than the competitors can perform. And for the retrieval quality also the winner here. If you look here at the relevance rate 62 covers the most important human-sided reference with a reference coins of8 and finds the most impactful paper.1 but and this is now a big but the performance of open research collapses in the third dimension and this is the most crucial aspect if you have some scientific writing trust and verifiability. If you just write something that is not verifiable. 
Relevance score: -10.381

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: But just a warning, you always have to check your resources. There's always a possibility, even with the best current systems, that the system is hallucinating. It is referencing to archive preprints that are non-existent or it is coming up with an explanation that is logically incorrect. But otherwise, enjoy AI, subscribe to the channel, and I see you in the next video.
Relevance score: -10.981

----------------------------------------------------------------
-------------------- New Query at 2025-09-03T15:17:40.680004 -------------

        ## Task
        The user's query: How deepscholar methodology work?

        Answer the user's query based only on the retrieved context.
        Use chat history if relevant, but never go beyond the given context.
        Write a natural, context-based answer.


        ## Context
        Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Great. So verifiability here open research scores shockingly low. This is what Stanford and UC Berkeler tells us. Citation precision numerical value is only 39. The claim coverage is only.13. And they tell us this is one of the worst performers here. that why is this happening? They suggest here a system optimized for fluent plausible sounding text is great on the one hand. So it's excellent in synthesizing here story or story line. But if you work in science, if you want to be absolute precise, you want to reduce hallucinating to the max and you want to make sure making logical leaps that are not strictly supported by the cited sources of archive are not part of your document. Now unfortunately this is happening here with open deep research. So for purely scientific work where you have to have absolute precision this could turn out to be a fatal flaw. Now, interestingly, if we look here at the new matter, the deep scholar base as I just showed you and we put here an 03 as the engine into this benchmark, you see, interestingly for the knowledge synthesis, we have the exact same benchmark 857.857. If we look here at the nugget, we even have a better performance here at the sequential tokenization if you want. And and this is now interesting with this new method. Look at the variability from one we jump to 6. So yeah, wow. This is an impressive improvement against the commercial openi deep research performance. So this new method by Stanford and UC Berkeley even if you use it with an openi model you see in one of the indicators they are at least as good as the commercial models but they outperform the commercial model significantly in variability. Isn't this beautiful? And if you ask yourself hey why I have suddenly two LLMs I told you you can have multiple LLMs here in the pipeline. So they decided for the SAM filter and the SAM top K data they go with GBD 4.1 but for the heavyduty SAM the aggregation step they go here with an 03 or a Claude or a Gemini just to make sure we on the same page. So this is the process. So we have the filter process then the top K the top 20 papers then the final aggregation the final section generation and I told you I show you always the prompt and here you have the prompt. So whatever LLM you choose an 03 or a claude OPUS 4 or a Gemini 2.5 Pro this is here the prompt for the aggregation for the final aggregation here of your research. So now we can see the paper here highlights here the deep scholar base can achieve up to 6.3 times higher claim coverage. So one indicator and one dimension six times three better than openi deep research. Nice. So we do have new systems that really outperform the classical commercial blackbox systems. But on the other hand, if you want just a plausible, well-written draft and you are not in science and you are willing, if you are in science, to meticulously fact check every single claim, every single half sentence that EI writes for you, then if you as a human do this verification and correct it, then open EI is really maybe also interesting. However, if you are a scientist and you say, I will go with the AI system that I can trust at least partially, then the deep scholar base here with claude or gemini model as the engine is dramatically superior. If you look at the data from Stanford and UC Berkeley, the paper states also that no method achieves a score greater than.19 when you average across the metric. So this is not an incremental challenge. You know, go from 78% to 82%. No, we at 19%. This is the best model we have. So we have quite a monumental challenge in front of us to make our AI models better, more performance, more power. So there's a lot of work in front of us. And if you say okay, I don't like here the numerical values in a table. I want to see this here in a visualization. Here you have exactly all the different elements here in um yeah to radar visualization. But this is exactly the data here from the numerical table I just showed you. But if you prefer to have it here in the radar, beautiful. Here is it. 
Relevance score: -8.676

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Hello community so great that you are back. We have a new paper by Stanford University and UC Berkeley and it is all about science and a deep scholar AI system that is brand new. Now you know we have open-source research system no that do a research and synthesize known facts and new findings and you are familiar with this no for example here open scholar this is here the paper or if you go for another open source this is here storm or maybe you are familiar with deep researcher from April 2025 so we do have open systems but of course it's interesting how do they compare to the commercial system let's say openi deep research or we We have brand new search EIS where you remember MCP and here multi- aent system. So how do they compare here to multi- aent search EI systems in well let's say you do a research on the internet. Now for those we have now the study where they compare Llama forced to GPD 4.1 and 03 an Opus 4 by Claude and a Gemini 2.5 Pro. And if you ask me why not some other models, those are the set of models Stanford and UC Berkeley decided to perform their tests on. This is it. The framework they used here is here open deep search. The search with open-source reasoning agents and you remember we have here multi- aent system. We have an open reasoning agent here itself that interprets here the given human task and completes it here by orchestrating a sequence of action and one of those action might include here calling tools and one of those tools that is available to this agent is here the open search tool to connect to the internet and everything. So a multi- aent search utility and now we have a new paper, we have a new method, we have some new research benchmark results and some new ideas. So what is the task that we are looking at? Let's say you're a researcher and you have your title and maybe you have already an idea from a technical abstract. Yeah. And then you want to ask AI to write here the related work section of your publication for you. In the old times we call this a literature review. Have a look who published what in your particular specific domain, when it was published, what were the topics, how was it related to your work. Now everything should be done by the eye. Now this new system is called the deep scholar benchmark and yes it promises to generate here the deep the related work section the deep research on internet on archive on meta archive for you. So what it needs three capabilities you are looking for the retrieval. So you want to have a vast corpus the live internet or the live archive whatever you have then you want to have a synthesis report. So everything all the data that were replied here from multiple sources weave them together the findings into a coherent long form narrative and then you want and this is the most important constraint absolute verifiability. So whatever you are claiming here and whatever you are citing those must really exist. So the citing sources must be accurate on every claim and must be back traced back to its origin. You must have references. Great. So the authors here Stanford and UC Berkeley built here an automated data pipeline directly and they said you know what we not just for the internet let's go here for science let's go to archive. So we want to have a fresh data set retrieved absolutely fresh here daily or every minute here from archive. So how we do this? So three topics the recency the quality control and diversity and here they also limited theirel to computer science as a topic and they made sure that they have a diverse top with 18 different domains in computer science. So yeah we are restricted to computer science but I think you will see the results they speak for themselves. Here's the paper published today August 27, 2025. Deep scholar bench live benchmark on automated evolation for generative research systems from Stanford University in UC Berkeley. And the beauty is not that they have now a new benchmark, but what the new benchmark shows us. It shows us the true difficulty AI systems have if they do have to do a research synthesis. Here's the result. I would say wait a minute explain here at first the metric and the benchmark methodology. Okay. So they decided to go here for a threedimensional differentiation to say we look for the knowledge synthesis of this new system for the retrieval quality. It's more or less a rack system. No. And then for the verifiability because we are working in science with archive. It must be really that we can site real sources and there's no hallucination. Great. And then you can see even in those three dimension they decided to go with two or three sub indicators. So let's have a look. So for the knowledge synthesis or what I call hey how well is this generated EI text written? Is it informative? They have two indicators. the organization where an LLM as a J performs here a pairwise comparison and then they have here a metric that assesses here the informationational completeness. So what you do you have now from your text here a human written um segment here and this is automatically if you want chucked up here by the eye so you get single sentences for example but a human like red at all introduced here the transformer architecture which relies here on a self attention mechanism now the eye might say hey this is a two long sentence there are too many objects too many subjects I reduce this I chunk it up here in nuggets so the first nugget is redford at introduce the transformer stop. This is it. So simple to verify and then it goes on and you see exactly what's happening. Now you have then this essential nuggets and you want to make sure that those really represent here in the I generated text what is the main content of the human written sentence or paragraph or paper. Then second dimension the retrieval quality. Simple in my words. Did the I system find the right papers on archive for my topic. Here we have three indicators. The relevance rate, the reference coverage or simply hey if the system found here the real important papers and you know importance is just one but we can also measure here how impactful this is with an impact um quote. So we have a document importance metric. So this is a proxy for retrieving impactful word that uses here the citation comes from a specific API. Great. And finally the most important for me we have to have verifiability. So whatever we claim there is a check against some other published work. So are the claims now actually supported here by the citation? And here they go with two indicators. the citation precision where they say hey does the cited source support at least one claim made in the eye sentence just one and then they are say hey let's go full power now now for the term claim coverage we want do the cited sources support all the claims made in the eye sentence so it's really every little detail that the eye writes out here supported here by let's say arrive references Great. Then they decided, you know, so we have now the idea of the benchmark, we have the metric and the subindicator of the benchmark. And they say, you know what, now we build a vehicle. So we can switch now the engine of this vehicle. So we can put in either a GPT 4.1 or we can put in a GPD4 Omni or we go with a clawed model as the engine. And they built here the deep scholar base. Now this was interesting because remember I told you this is your almost rag system. No they went here with a classical rag multi multi- aent structure. So at first you have the query generation. So an LLM generates now a set of diverse search query based on the human input. So whatever I have the AI the LLM tells me hey that was a little bit too complicated. I generate now a set of diver search queries and then I send this up. So those queries now AI generated queries now run against the search API in our example archive and then we have a filtering now a semantic filtering and now we use another LLM is used to filter here to retrieve documents and anything that is irrelevant we just want to ignore this no but we have still a lot of documents like say 200 documents left now after the filtering so we have to do a reranking because we want to go down to the top 20 no so we have another llm based step that now reranks in detail to the only 200 remaining documents and it's a new prioritization here. What are the most relevance only of these 200 and then if we have the top 20 we want to have a semantic aggregation of the content of all 20 papers. No. So we say now the final top 20 documents are fed now as a context to an LLM and again you can choose your LLM that you like with a detailed prompt and then synthesization and the LLM writes it the final related works action. So clearly structured data processing pipeline and you can switch on and off any LLM in any place that you want and they tested here variation of different LLMs and they say so what are now the best performance combination so you see pipeline performing research this is great they run this here with a GPD4 as the default and then they exchange and said hey what happens if I add here let's say an 03 or a cloud model to GPD4.1 one all the prompts for this single steps that you just noticed here you have here in detail in the annex here of the documentation of this new research paper like here I show you the revised ODS react agent prompt here for calling here a tool so a lot of additional data I just want to give you the result this is it again you see one two three dimension and then all the different sub indicator in this dimension. Yeah, I think we can forget about the opensource research system from 2014. 
Relevance score: -9.185

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: 85. So I give you here my summary. OpenI is really here the leader in crafting a well ststructured and seemingly comprehensive narrative. Open deep research really excels in this synthesis and in the retrieval task and we have here the numerical data for the highest organization score 85 and the highest nugget coverage 39. output are well organized and capture really the essential fact better than the competitors can perform. And for the retrieval quality also the winner here. If you look here at the relevance rate 62 covers the most important human-sided reference with a reference coins of8 and finds the most impactful paper.1 but and this is now a big but the performance of open research collapses in the third dimension and this is the most crucial aspect if you have some scientific writing trust and verifiability. If you just write something that is not verifiable. 
Relevance score: -10.381

----------------------------------------------------------------
-------------------- New Query at 2025-09-03T18:05:16.485305 -------------

        ## Task
        The user's query: How deepscholar methodology works?

        Answer the user's query based only on the retrieved context.
        Use chat history if relevant, but never go beyond the given context.
        Write a natural, context-based answer.


        ## Context
        Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Great. So verifiability here open research scores shockingly low. This is what Stanford and UC Berkeler tells us. Citation precision numerical value is only 39. The claim coverage is only.13. And they tell us this is one of the worst performers here. that why is this happening? They suggest here a system optimized for fluent plausible sounding text is great on the one hand. So it's excellent in synthesizing here story or story line. But if you work in science, if you want to be absolute precise, you want to reduce hallucinating to the max and you want to make sure making logical leaps that are not strictly supported by the cited sources of archive are not part of your document. Now unfortunately this is happening here with open deep research. So for purely scientific work where you have to have absolute precision this could turn out to be a fatal flaw. Now, interestingly, if we look here at the new matter, the deep scholar base as I just showed you and we put here an 03 as the engine into this benchmark, you see, interestingly for the knowledge synthesis, we have the exact same benchmark 857.857. If we look here at the nugget, we even have a better performance here at the sequential tokenization if you want. And and this is now interesting with this new method. Look at the variability from one we jump to 6. So yeah, wow. This is an impressive improvement against the commercial openi deep research performance. So this new method by Stanford and UC Berkeley even if you use it with an openi model you see in one of the indicators they are at least as good as the commercial models but they outperform the commercial model significantly in variability. Isn't this beautiful? And if you ask yourself hey why I have suddenly two LLMs I told you you can have multiple LLMs here in the pipeline. So they decided for the SAM filter and the SAM top K data they go with GBD 4.1 but for the heavyduty SAM the aggregation step they go here with an 03 or a Claude or a Gemini just to make sure we on the same page. So this is the process. So we have the filter process then the top K the top 20 papers then the final aggregation the final section generation and I told you I show you always the prompt and here you have the prompt. So whatever LLM you choose an 03 or a claude OPUS 4 or a Gemini 2.5 Pro this is here the prompt for the aggregation for the final aggregation here of your research. So now we can see the paper here highlights here the deep scholar base can achieve up to 6.3 times higher claim coverage. So one indicator and one dimension six times three better than openi deep research. Nice. So we do have new systems that really outperform the classical commercial blackbox systems. But on the other hand, if you want just a plausible, well-written draft and you are not in science and you are willing, if you are in science, to meticulously fact check every single claim, every single half sentence that EI writes for you, then if you as a human do this verification and correct it, then open EI is really maybe also interesting. However, if you are a scientist and you say, I will go with the AI system that I can trust at least partially, then the deep scholar base here with claude or gemini model as the engine is dramatically superior. If you look at the data from Stanford and UC Berkeley, the paper states also that no method achieves a score greater than.19 when you average across the metric. So this is not an incremental challenge. You know, go from 78% to 82%. No, we at 19%. This is the best model we have. So we have quite a monumental challenge in front of us to make our AI models better, more performance, more power. So there's a lot of work in front of us. And if you say okay, I don't like here the numerical values in a table. I want to see this here in a visualization. Here you have exactly all the different elements here in um yeah to radar visualization. But this is exactly the data here from the numerical table I just showed you. But if you prefer to have it here in the radar, beautiful. Here is it. 
Relevance score: -8.442

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Hello community so great that you are back. We have a new paper by Stanford University and UC Berkeley and it is all about science and a deep scholar AI system that is brand new. Now you know we have open-source research system no that do a research and synthesize known facts and new findings and you are familiar with this no for example here open scholar this is here the paper or if you go for another open source this is here storm or maybe you are familiar with deep researcher from April 2025 so we do have open systems but of course it's interesting how do they compare to the commercial system let's say openi deep research or we We have brand new search EIS where you remember MCP and here multi- aent system. So how do they compare here to multi- aent search EI systems in well let's say you do a research on the internet. Now for those we have now the study where they compare Llama forced to GPD 4.1 and 03 an Opus 4 by Claude and a Gemini 2.5 Pro. And if you ask me why not some other models, those are the set of models Stanford and UC Berkeley decided to perform their tests on. This is it. The framework they used here is here open deep search. The search with open-source reasoning agents and you remember we have here multi- aent system. We have an open reasoning agent here itself that interprets here the given human task and completes it here by orchestrating a sequence of action and one of those action might include here calling tools and one of those tools that is available to this agent is here the open search tool to connect to the internet and everything. So a multi- aent search utility and now we have a new paper, we have a new method, we have some new research benchmark results and some new ideas. So what is the task that we are looking at? Let's say you're a researcher and you have your title and maybe you have already an idea from a technical abstract. Yeah. And then you want to ask AI to write here the related work section of your publication for you. In the old times we call this a literature review. Have a look who published what in your particular specific domain, when it was published, what were the topics, how was it related to your work. Now everything should be done by the eye. Now this new system is called the deep scholar benchmark and yes it promises to generate here the deep the related work section the deep research on internet on archive on meta archive for you. So what it needs three capabilities you are looking for the retrieval. So you want to have a vast corpus the live internet or the live archive whatever you have then you want to have a synthesis report. So everything all the data that were replied here from multiple sources weave them together the findings into a coherent long form narrative and then you want and this is the most important constraint absolute verifiability. So whatever you are claiming here and whatever you are citing those must really exist. So the citing sources must be accurate on every claim and must be back traced back to its origin. You must have references. Great. So the authors here Stanford and UC Berkeley built here an automated data pipeline directly and they said you know what we not just for the internet let's go here for science let's go to archive. So we want to have a fresh data set retrieved absolutely fresh here daily or every minute here from archive. So how we do this? So three topics the recency the quality control and diversity and here they also limited theirel to computer science as a topic and they made sure that they have a diverse top with 18 different domains in computer science. So yeah we are restricted to computer science but I think you will see the results they speak for themselves. Here's the paper published today August 27, 2025. Deep scholar bench live benchmark on automated evolation for generative research systems from Stanford University in UC Berkeley. And the beauty is not that they have now a new benchmark, but what the new benchmark shows us. It shows us the true difficulty AI systems have if they do have to do a research synthesis. Here's the result. I would say wait a minute explain here at first the metric and the benchmark methodology. Okay. So they decided to go here for a threedimensional differentiation to say we look for the knowledge synthesis of this new system for the retrieval quality. It's more or less a rack system. No. And then for the verifiability because we are working in science with archive. It must be really that we can site real sources and there's no hallucination. Great. And then you can see even in those three dimension they decided to go with two or three sub indicators. So let's have a look. So for the knowledge synthesis or what I call hey how well is this generated EI text written? Is it informative? They have two indicators. the organization where an LLM as a J performs here a pairwise comparison and then they have here a metric that assesses here the informationational completeness. So what you do you have now from your text here a human written um segment here and this is automatically if you want chucked up here by the eye so you get single sentences for example but a human like red at all introduced here the transformer architecture which relies here on a self attention mechanism now the eye might say hey this is a two long sentence there are too many objects too many subjects I reduce this I chunk it up here in nuggets so the first nugget is redford at introduce the transformer stop. This is it. So simple to verify and then it goes on and you see exactly what's happening. Now you have then this essential nuggets and you want to make sure that those really represent here in the I generated text what is the main content of the human written sentence or paragraph or paper. Then second dimension the retrieval quality. Simple in my words. Did the I system find the right papers on archive for my topic. Here we have three indicators. The relevance rate, the reference coverage or simply hey if the system found here the real important papers and you know importance is just one but we can also measure here how impactful this is with an impact um quote. So we have a document importance metric. So this is a proxy for retrieving impactful word that uses here the citation comes from a specific API. Great. And finally the most important for me we have to have verifiability. So whatever we claim there is a check against some other published work. So are the claims now actually supported here by the citation? And here they go with two indicators. the citation precision where they say hey does the cited source support at least one claim made in the eye sentence just one and then they are say hey let's go full power now now for the term claim coverage we want do the cited sources support all the claims made in the eye sentence so it's really every little detail that the eye writes out here supported here by let's say arrive references Great. Then they decided, you know, so we have now the idea of the benchmark, we have the metric and the subindicator of the benchmark. And they say, you know what, now we build a vehicle. So we can switch now the engine of this vehicle. So we can put in either a GPT 4.1 or we can put in a GPD4 Omni or we go with a clawed model as the engine. And they built here the deep scholar base. Now this was interesting because remember I told you this is your almost rag system. No they went here with a classical rag multi multi- aent structure. So at first you have the query generation. So an LLM generates now a set of diverse search query based on the human input. So whatever I have the AI the LLM tells me hey that was a little bit too complicated. I generate now a set of diver search queries and then I send this up. So those queries now AI generated queries now run against the search API in our example archive and then we have a filtering now a semantic filtering and now we use another LLM is used to filter here to retrieve documents and anything that is irrelevant we just want to ignore this no but we have still a lot of documents like say 200 documents left now after the filtering so we have to do a reranking because we want to go down to the top 20 no so we have another llm based step that now reranks in detail to the only 200 remaining documents and it's a new prioritization here. What are the most relevance only of these 200 and then if we have the top 20 we want to have a semantic aggregation of the content of all 20 papers. No. So we say now the final top 20 documents are fed now as a context to an LLM and again you can choose your LLM that you like with a detailed prompt and then synthesization and the LLM writes it the final related works action. So clearly structured data processing pipeline and you can switch on and off any LLM in any place that you want and they tested here variation of different LLMs and they say so what are now the best performance combination so you see pipeline performing research this is great they run this here with a GPD4 as the default and then they exchange and said hey what happens if I add here let's say an 03 or a cloud model to GPD4.1 one all the prompts for this single steps that you just noticed here you have here in detail in the annex here of the documentation of this new research paper like here I show you the revised ODS react agent prompt here for calling here a tool so a lot of additional data I just want to give you the result this is it again you see one two three dimension and then all the different sub indicator in this dimension. Yeah, I think we can forget about the opensource research system from 2014. 
Relevance score: -8.864

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: 85. So I give you here my summary. OpenI is really here the leader in crafting a well ststructured and seemingly comprehensive narrative. Open deep research really excels in this synthesis and in the retrieval task and we have here the numerical data for the highest organization score 85 and the highest nugget coverage 39. output are well organized and capture really the essential fact better than the competitors can perform. And for the retrieval quality also the winner here. If you look here at the relevance rate 62 covers the most important human-sided reference with a reference coins of8 and finds the most impactful paper.1 but and this is now a big but the performance of open research collapses in the third dimension and this is the most crucial aspect if you have some scientific writing trust and verifiability. If you just write something that is not verifiable. 
Relevance score: -10.210

----------------------------------------------------------------
-------------------- New Query at 2025-09-03T18:06:51.627056 -------------

        ## Task
        The user's query: Summarize the key points from this video

        Answer the user's query based only on the retrieved context.
        Use chat history if relevant, but never go beyond the given context.
        Write a natural, context-based answer.


        ## Context
        Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: 85. So I give you here my summary. OpenI is really here the leader in crafting a well ststructured and seemingly comprehensive narrative. Open deep research really excels in this synthesis and in the retrieval task and we have here the numerical data for the highest organization score 85 and the highest nugget coverage 39. output are well organized and capture really the essential fact better than the competitors can perform. And for the retrieval quality also the winner here. If you look here at the relevance rate 62 covers the most important human-sided reference with a reference coins of8 and finds the most impactful paper.1 but and this is now a big but the performance of open research collapses in the third dimension and this is the most crucial aspect if you have some scientific writing trust and verifiability. If you just write something that is not verifiable. 
Relevance score: -9.348

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Great. So verifiability here open research scores shockingly low. This is what Stanford and UC Berkeler tells us. Citation precision numerical value is only 39. The claim coverage is only.13. And they tell us this is one of the worst performers here. that why is this happening? They suggest here a system optimized for fluent plausible sounding text is great on the one hand. So it's excellent in synthesizing here story or story line. But if you work in science, if you want to be absolute precise, you want to reduce hallucinating to the max and you want to make sure making logical leaps that are not strictly supported by the cited sources of archive are not part of your document. Now unfortunately this is happening here with open deep research. So for purely scientific work where you have to have absolute precision this could turn out to be a fatal flaw. Now, interestingly, if we look here at the new matter, the deep scholar base as I just showed you and we put here an 03 as the engine into this benchmark, you see, interestingly for the knowledge synthesis, we have the exact same benchmark 857.857. If we look here at the nugget, we even have a better performance here at the sequential tokenization if you want. And and this is now interesting with this new method. Look at the variability from one we jump to 6. So yeah, wow. This is an impressive improvement against the commercial openi deep research performance. So this new method by Stanford and UC Berkeley even if you use it with an openi model you see in one of the indicators they are at least as good as the commercial models but they outperform the commercial model significantly in variability. Isn't this beautiful? And if you ask yourself hey why I have suddenly two LLMs I told you you can have multiple LLMs here in the pipeline. So they decided for the SAM filter and the SAM top K data they go with GBD 4.1 but for the heavyduty SAM the aggregation step they go here with an 03 or a Claude or a Gemini just to make sure we on the same page. So this is the process. So we have the filter process then the top K the top 20 papers then the final aggregation the final section generation and I told you I show you always the prompt and here you have the prompt. So whatever LLM you choose an 03 or a claude OPUS 4 or a Gemini 2.5 Pro this is here the prompt for the aggregation for the final aggregation here of your research. So now we can see the paper here highlights here the deep scholar base can achieve up to 6.3 times higher claim coverage. So one indicator and one dimension six times three better than openi deep research. Nice. So we do have new systems that really outperform the classical commercial blackbox systems. But on the other hand, if you want just a plausible, well-written draft and you are not in science and you are willing, if you are in science, to meticulously fact check every single claim, every single half sentence that EI writes for you, then if you as a human do this verification and correct it, then open EI is really maybe also interesting. However, if you are a scientist and you say, I will go with the AI system that I can trust at least partially, then the deep scholar base here with claude or gemini model as the engine is dramatically superior. If you look at the data from Stanford and UC Berkeley, the paper states also that no method achieves a score greater than.19 when you average across the metric. So this is not an incremental challenge. You know, go from 78% to 82%. No, we at 19%. This is the best model we have. So we have quite a monumental challenge in front of us to make our AI models better, more performance, more power. So there's a lot of work in front of us. And if you say okay, I don't like here the numerical values in a table. I want to see this here in a visualization. Here you have exactly all the different elements here in um yeah to radar visualization. But this is exactly the data here from the numerical table I just showed you. But if you prefer to have it here in the radar, beautiful. Here is it. 
Relevance score: -9.571

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: But just a warning, you always have to check your resources. There's always a possibility, even with the best current systems, that the system is hallucinating. It is referencing to archive preprints that are non-existent or it is coming up with an explanation that is logically incorrect. But otherwise, enjoy AI, subscribe to the channel, and I see you in the next video.
Relevance score: -9.896

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Hello community so great that you are back. We have a new paper by Stanford University and UC Berkeley and it is all about science and a deep scholar AI system that is brand new. Now you know we have open-source research system no that do a research and synthesize known facts and new findings and you are familiar with this no for example here open scholar this is here the paper or if you go for another open source this is here storm or maybe you are familiar with deep researcher from April 2025 so we do have open systems but of course it's interesting how do they compare to the commercial system let's say openi deep research or we We have brand new search EIS where you remember MCP and here multi- aent system. So how do they compare here to multi- aent search EI systems in well let's say you do a research on the internet. Now for those we have now the study where they compare Llama forced to GPD 4.1 and 03 an Opus 4 by Claude and a Gemini 2.5 Pro. And if you ask me why not some other models, those are the set of models Stanford and UC Berkeley decided to perform their tests on. This is it. The framework they used here is here open deep search. The search with open-source reasoning agents and you remember we have here multi- aent system. We have an open reasoning agent here itself that interprets here the given human task and completes it here by orchestrating a sequence of action and one of those action might include here calling tools and one of those tools that is available to this agent is here the open search tool to connect to the internet and everything. So a multi- aent search utility and now we have a new paper, we have a new method, we have some new research benchmark results and some new ideas. So what is the task that we are looking at? Let's say you're a researcher and you have your title and maybe you have already an idea from a technical abstract. Yeah. And then you want to ask AI to write here the related work section of your publication for you. In the old times we call this a literature review. Have a look who published what in your particular specific domain, when it was published, what were the topics, how was it related to your work. Now everything should be done by the eye. Now this new system is called the deep scholar benchmark and yes it promises to generate here the deep the related work section the deep research on internet on archive on meta archive for you. So what it needs three capabilities you are looking for the retrieval. So you want to have a vast corpus the live internet or the live archive whatever you have then you want to have a synthesis report. So everything all the data that were replied here from multiple sources weave them together the findings into a coherent long form narrative and then you want and this is the most important constraint absolute verifiability. So whatever you are claiming here and whatever you are citing those must really exist. So the citing sources must be accurate on every claim and must be back traced back to its origin. You must have references. Great. So the authors here Stanford and UC Berkeley built here an automated data pipeline directly and they said you know what we not just for the internet let's go here for science let's go to archive. So we want to have a fresh data set retrieved absolutely fresh here daily or every minute here from archive. So how we do this? So three topics the recency the quality control and diversity and here they also limited theirel to computer science as a topic and they made sure that they have a diverse top with 18 different domains in computer science. So yeah we are restricted to computer science but I think you will see the results they speak for themselves. Here's the paper published today August 27, 2025. Deep scholar bench live benchmark on automated evolation for generative research systems from Stanford University in UC Berkeley. And the beauty is not that they have now a new benchmark, but what the new benchmark shows us. It shows us the true difficulty AI systems have if they do have to do a research synthesis. Here's the result. I would say wait a minute explain here at first the metric and the benchmark methodology. Okay. So they decided to go here for a threedimensional differentiation to say we look for the knowledge synthesis of this new system for the retrieval quality. It's more or less a rack system. No. And then for the verifiability because we are working in science with archive. It must be really that we can site real sources and there's no hallucination. Great. And then you can see even in those three dimension they decided to go with two or three sub indicators. So let's have a look. So for the knowledge synthesis or what I call hey how well is this generated EI text written? Is it informative? They have two indicators. the organization where an LLM as a J performs here a pairwise comparison and then they have here a metric that assesses here the informationational completeness. So what you do you have now from your text here a human written um segment here and this is automatically if you want chucked up here by the eye so you get single sentences for example but a human like red at all introduced here the transformer architecture which relies here on a self attention mechanism now the eye might say hey this is a two long sentence there are too many objects too many subjects I reduce this I chunk it up here in nuggets so the first nugget is redford at introduce the transformer stop. This is it. So simple to verify and then it goes on and you see exactly what's happening. Now you have then this essential nuggets and you want to make sure that those really represent here in the I generated text what is the main content of the human written sentence or paragraph or paper. Then second dimension the retrieval quality. Simple in my words. Did the I system find the right papers on archive for my topic. Here we have three indicators. The relevance rate, the reference coverage or simply hey if the system found here the real important papers and you know importance is just one but we can also measure here how impactful this is with an impact um quote. So we have a document importance metric. So this is a proxy for retrieving impactful word that uses here the citation comes from a specific API. Great. And finally the most important for me we have to have verifiability. So whatever we claim there is a check against some other published work. So are the claims now actually supported here by the citation? And here they go with two indicators. the citation precision where they say hey does the cited source support at least one claim made in the eye sentence just one and then they are say hey let's go full power now now for the term claim coverage we want do the cited sources support all the claims made in the eye sentence so it's really every little detail that the eye writes out here supported here by let's say arrive references Great. Then they decided, you know, so we have now the idea of the benchmark, we have the metric and the subindicator of the benchmark. And they say, you know what, now we build a vehicle. So we can switch now the engine of this vehicle. So we can put in either a GPT 4.1 or we can put in a GPD4 Omni or we go with a clawed model as the engine. And they built here the deep scholar base. Now this was interesting because remember I told you this is your almost rag system. No they went here with a classical rag multi multi- aent structure. So at first you have the query generation. So an LLM generates now a set of diverse search query based on the human input. So whatever I have the AI the LLM tells me hey that was a little bit too complicated. I generate now a set of diver search queries and then I send this up. So those queries now AI generated queries now run against the search API in our example archive and then we have a filtering now a semantic filtering and now we use another LLM is used to filter here to retrieve documents and anything that is irrelevant we just want to ignore this no but we have still a lot of documents like say 200 documents left now after the filtering so we have to do a reranking because we want to go down to the top 20 no so we have another llm based step that now reranks in detail to the only 200 remaining documents and it's a new prioritization here. What are the most relevance only of these 200 and then if we have the top 20 we want to have a semantic aggregation of the content of all 20 papers. No. So we say now the final top 20 documents are fed now as a context to an LLM and again you can choose your LLM that you like with a detailed prompt and then synthesization and the LLM writes it the final related works action. So clearly structured data processing pipeline and you can switch on and off any LLM in any place that you want and they tested here variation of different LLMs and they say so what are now the best performance combination so you see pipeline performing research this is great they run this here with a GPD4 as the default and then they exchange and said hey what happens if I add here let's say an 03 or a cloud model to GPD4.1 one all the prompts for this single steps that you just noticed here you have here in detail in the annex here of the documentation of this new research paper like here I show you the revised ODS react agent prompt here for calling here a tool so a lot of additional data I just want to give you the result this is it again you see one two three dimension and then all the different sub indicator in this dimension. Yeah, I think we can forget about the opensource research system from 2014. 
Relevance score: -10.356

----------------------------------------------------------------
-------------------- New Query at 2025-09-03T18:17:01.229250 -------------

        ## Task
        The user's query: How deepscholar methodology works?

        Answer the user's query based only on the retrieved context.
        Use chat history if relevant, but never go beyond the given context.
        Write a natural, context-based answer.


        ## Context
        Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Great. So verifiability here open research scores shockingly low. This is what Stanford and UC Berkeler tells us. Citation precision numerical value is only 39. The claim coverage is only.13. And they tell us this is one of the worst performers here. that why is this happening? They suggest here a system optimized for fluent plausible sounding text is great on the one hand. So it's excellent in synthesizing here story or story line. But if you work in science, if you want to be absolute precise, you want to reduce hallucinating to the max and you want to make sure making logical leaps that are not strictly supported by the cited sources of archive are not part of your document. Now unfortunately this is happening here with open deep research. So for purely scientific work where you have to have absolute precision this could turn out to be a fatal flaw. Now, interestingly, if we look here at the new matter, the deep scholar base as I just showed you and we put here an 03 as the engine into this benchmark, you see, interestingly for the knowledge synthesis, we have the exact same benchmark 857.857. If we look here at the nugget, we even have a better performance here at the sequential tokenization if you want. And and this is now interesting with this new method. Look at the variability from one we jump to 6. So yeah, wow. This is an impressive improvement against the commercial openi deep research performance. So this new method by Stanford and UC Berkeley even if you use it with an openi model you see in one of the indicators they are at least as good as the commercial models but they outperform the commercial model significantly in variability. Isn't this beautiful? And if you ask yourself hey why I have suddenly two LLMs I told you you can have multiple LLMs here in the pipeline. So they decided for the SAM filter and the SAM top K data they go with GBD 4.1 but for the heavyduty SAM the aggregation step they go here with an 03 or a Claude or a Gemini just to make sure we on the same page. So this is the process. So we have the filter process then the top K the top 20 papers then the final aggregation the final section generation and I told you I show you always the prompt and here you have the prompt. So whatever LLM you choose an 03 or a claude OPUS 4 or a Gemini 2.5 Pro this is here the prompt for the aggregation for the final aggregation here of your research. So now we can see the paper here highlights here the deep scholar base can achieve up to 6.3 times higher claim coverage. So one indicator and one dimension six times three better than openi deep research. Nice. So we do have new systems that really outperform the classical commercial blackbox systems. But on the other hand, if you want just a plausible, well-written draft and you are not in science and you are willing, if you are in science, to meticulously fact check every single claim, every single half sentence that EI writes for you, then if you as a human do this verification and correct it, then open EI is really maybe also interesting. However, if you are a scientist and you say, I will go with the AI system that I can trust at least partially, then the deep scholar base here with claude or gemini model as the engine is dramatically superior. If you look at the data from Stanford and UC Berkeley, the paper states also that no method achieves a score greater than.19 when you average across the metric. So this is not an incremental challenge. You know, go from 78% to 82%. No, we at 19%. This is the best model we have. So we have quite a monumental challenge in front of us to make our AI models better, more performance, more power. So there's a lot of work in front of us. And if you say okay, I don't like here the numerical values in a table. I want to see this here in a visualization. Here you have exactly all the different elements here in um yeah to radar visualization. But this is exactly the data here from the numerical table I just showed you. But if you prefer to have it here in the radar, beautiful. Here is it. 
Relevance score: -8.442

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Hello community so great that you are back. We have a new paper by Stanford University and UC Berkeley and it is all about science and a deep scholar AI system that is brand new. Now you know we have open-source research system no that do a research and synthesize known facts and new findings and you are familiar with this no for example here open scholar this is here the paper or if you go for another open source this is here storm or maybe you are familiar with deep researcher from April 2025 so we do have open systems but of course it's interesting how do they compare to the commercial system let's say openi deep research or we We have brand new search EIS where you remember MCP and here multi- aent system. So how do they compare here to multi- aent search EI systems in well let's say you do a research on the internet. Now for those we have now the study where they compare Llama forced to GPD 4.1 and 03 an Opus 4 by Claude and a Gemini 2.5 Pro. And if you ask me why not some other models, those are the set of models Stanford and UC Berkeley decided to perform their tests on. This is it. The framework they used here is here open deep search. The search with open-source reasoning agents and you remember we have here multi- aent system. We have an open reasoning agent here itself that interprets here the given human task and completes it here by orchestrating a sequence of action and one of those action might include here calling tools and one of those tools that is available to this agent is here the open search tool to connect to the internet and everything. So a multi- aent search utility and now we have a new paper, we have a new method, we have some new research benchmark results and some new ideas. So what is the task that we are looking at? Let's say you're a researcher and you have your title and maybe you have already an idea from a technical abstract. Yeah. And then you want to ask AI to write here the related work section of your publication for you. In the old times we call this a literature review. Have a look who published what in your particular specific domain, when it was published, what were the topics, how was it related to your work. Now everything should be done by the eye. Now this new system is called the deep scholar benchmark and yes it promises to generate here the deep the related work section the deep research on internet on archive on meta archive for you. So what it needs three capabilities you are looking for the retrieval. So you want to have a vast corpus the live internet or the live archive whatever you have then you want to have a synthesis report. So everything all the data that were replied here from multiple sources weave them together the findings into a coherent long form narrative and then you want and this is the most important constraint absolute verifiability. So whatever you are claiming here and whatever you are citing those must really exist. So the citing sources must be accurate on every claim and must be back traced back to its origin. You must have references. Great. So the authors here Stanford and UC Berkeley built here an automated data pipeline directly and they said you know what we not just for the internet let's go here for science let's go to archive. So we want to have a fresh data set retrieved absolutely fresh here daily or every minute here from archive. So how we do this? So three topics the recency the quality control and diversity and here they also limited theirel to computer science as a topic and they made sure that they have a diverse top with 18 different domains in computer science. So yeah we are restricted to computer science but I think you will see the results they speak for themselves. Here's the paper published today August 27, 2025. Deep scholar bench live benchmark on automated evolation for generative research systems from Stanford University in UC Berkeley. And the beauty is not that they have now a new benchmark, but what the new benchmark shows us. It shows us the true difficulty AI systems have if they do have to do a research synthesis. Here's the result. I would say wait a minute explain here at first the metric and the benchmark methodology. Okay. So they decided to go here for a threedimensional differentiation to say we look for the knowledge synthesis of this new system for the retrieval quality. It's more or less a rack system. No. And then for the verifiability because we are working in science with archive. It must be really that we can site real sources and there's no hallucination. Great. And then you can see even in those three dimension they decided to go with two or three sub indicators. So let's have a look. So for the knowledge synthesis or what I call hey how well is this generated EI text written? Is it informative? They have two indicators. the organization where an LLM as a J performs here a pairwise comparison and then they have here a metric that assesses here the informationational completeness. So what you do you have now from your text here a human written um segment here and this is automatically if you want chucked up here by the eye so you get single sentences for example but a human like red at all introduced here the transformer architecture which relies here on a self attention mechanism now the eye might say hey this is a two long sentence there are too many objects too many subjects I reduce this I chunk it up here in nuggets so the first nugget is redford at introduce the transformer stop. This is it. So simple to verify and then it goes on and you see exactly what's happening. Now you have then this essential nuggets and you want to make sure that those really represent here in the I generated text what is the main content of the human written sentence or paragraph or paper. Then second dimension the retrieval quality. Simple in my words. Did the I system find the right papers on archive for my topic. Here we have three indicators. The relevance rate, the reference coverage or simply hey if the system found here the real important papers and you know importance is just one but we can also measure here how impactful this is with an impact um quote. So we have a document importance metric. So this is a proxy for retrieving impactful word that uses here the citation comes from a specific API. Great. And finally the most important for me we have to have verifiability. So whatever we claim there is a check against some other published work. So are the claims now actually supported here by the citation? And here they go with two indicators. the citation precision where they say hey does the cited source support at least one claim made in the eye sentence just one and then they are say hey let's go full power now now for the term claim coverage we want do the cited sources support all the claims made in the eye sentence so it's really every little detail that the eye writes out here supported here by let's say arrive references Great. Then they decided, you know, so we have now the idea of the benchmark, we have the metric and the subindicator of the benchmark. And they say, you know what, now we build a vehicle. So we can switch now the engine of this vehicle. So we can put in either a GPT 4.1 or we can put in a GPD4 Omni or we go with a clawed model as the engine. And they built here the deep scholar base. Now this was interesting because remember I told you this is your almost rag system. No they went here with a classical rag multi multi- aent structure. So at first you have the query generation. So an LLM generates now a set of diverse search query based on the human input. So whatever I have the AI the LLM tells me hey that was a little bit too complicated. I generate now a set of diver search queries and then I send this up. So those queries now AI generated queries now run against the search API in our example archive and then we have a filtering now a semantic filtering and now we use another LLM is used to filter here to retrieve documents and anything that is irrelevant we just want to ignore this no but we have still a lot of documents like say 200 documents left now after the filtering so we have to do a reranking because we want to go down to the top 20 no so we have another llm based step that now reranks in detail to the only 200 remaining documents and it's a new prioritization here. What are the most relevance only of these 200 and then if we have the top 20 we want to have a semantic aggregation of the content of all 20 papers. No. So we say now the final top 20 documents are fed now as a context to an LLM and again you can choose your LLM that you like with a detailed prompt and then synthesization and the LLM writes it the final related works action. So clearly structured data processing pipeline and you can switch on and off any LLM in any place that you want and they tested here variation of different LLMs and they say so what are now the best performance combination so you see pipeline performing research this is great they run this here with a GPD4 as the default and then they exchange and said hey what happens if I add here let's say an 03 or a cloud model to GPD4.1 one all the prompts for this single steps that you just noticed here you have here in detail in the annex here of the documentation of this new research paper like here I show you the revised ODS react agent prompt here for calling here a tool so a lot of additional data I just want to give you the result this is it again you see one two three dimension and then all the different sub indicator in this dimension. Yeah, I think we can forget about the opensource research system from 2014. 
Relevance score: -8.864

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: 85. So I give you here my summary. OpenI is really here the leader in crafting a well ststructured and seemingly comprehensive narrative. Open deep research really excels in this synthesis and in the retrieval task and we have here the numerical data for the highest organization score 85 and the highest nugget coverage 39. output are well organized and capture really the essential fact better than the competitors can perform. And for the retrieval quality also the winner here. If you look here at the relevance rate 62 covers the most important human-sided reference with a reference coins of8 and finds the most impactful paper.1 but and this is now a big but the performance of open research collapses in the third dimension and this is the most crucial aspect if you have some scientific writing trust and verifiability. If you just write something that is not verifiable. 
Relevance score: -10.210

----------------------------------------------------------------
-------------------- New Query at 2025-09-03T18:22:10.771429 -------------

        ## Task
        The user's query: how deepscholar methodology work

        Answer the user's query based only on the retrieved context.
        Use chat history if relevant, but never go beyond the given context.
        Write a natural, context-based answer.


        ## Context
        Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Hello community so great that you are back. We have a new paper by Stanford University and UC Berkeley and it is all about science and a deep scholar AI system that is brand new. Now you know we have open-source research system no that do a research and synthesize known facts and new findings and you are familiar with this no for example here open scholar this is here the paper or if you go for another open source this is here storm or maybe you are familiar with deep researcher from April 2025 so we do have open systems but of course it's interesting how do they compare to the commercial system let's say openi deep research or we We have brand new search EIS where you remember MCP and here multi- aent system. So how do they compare here to multi- aent search EI systems in well let's say you do a research on the internet. Now for those we have now the study where they compare Llama forced to GPD 4.1 and 03 an Opus 4 by Claude and a Gemini 2.5 Pro. And if you ask me why not some other models, those are the set of models Stanford and UC Berkeley decided to perform their tests on. This is it. The framework they used here is here open deep search. The search with open-source reasoning agents and you remember we have here multi- aent system. We have an open reasoning agent here itself that interprets here the given human task and completes it here by orchestrating a sequence of action and one of those action might include here calling tools and one of those tools that is available to this agent is here the open search tool to connect to the internet and everything. So a multi- aent search utility and now we have a new paper, we have a new method, we have some new research benchmark results and some new ideas. So what is the task that we are looking at? Let's say you're a researcher and you have your title and maybe you have already an idea from a technical abstract. Yeah. And then you want to ask AI to write here the related work section of your publication for you. In the old times we call this a literature review. Have a look who published what in your particular specific domain, when it was published, what were the topics, how was it related to your work. Now everything should be done by the eye. Now this new system is called the deep scholar benchmark and yes it promises to generate here the deep the related work section the deep research on internet on archive on meta archive for you. So what it needs three capabilities you are looking for the retrieval. So you want to have a vast corpus the live internet or the live archive whatever you have then you want to have a synthesis report. So everything all the data that were replied here from multiple sources weave them together the findings into a coherent long form narrative and then you want and this is the most important constraint absolute verifiability. So whatever you are claiming here and whatever you are citing those must really exist. So the citing sources must be accurate on every claim and must be back traced back to its origin. You must have references. Great. So the authors here Stanford and UC Berkeley built here an automated data pipeline directly and they said you know what we not just for the internet let's go here for science let's go to archive. So we want to have a fresh data set retrieved absolutely fresh here daily or every minute here from archive. So how we do this? So three topics the recency the quality control and diversity and here they also limited theirel to computer science as a topic and they made sure that they have a diverse top with 18 different domains in computer science. So yeah we are restricted to computer science but I think you will see the results they speak for themselves. Here's the paper published today August 27, 2025. Deep scholar bench live benchmark on automated evolation for generative research systems from Stanford University in UC Berkeley. And the beauty is not that they have now a new benchmark, but what the new benchmark shows us. It shows us the true difficulty AI systems have if they do have to do a research synthesis. Here's the result. I would say wait a minute explain here at first the metric and the benchmark methodology. Okay. So they decided to go here for a threedimensional differentiation to say we look for the knowledge synthesis of this new system for the retrieval quality. It's more or less a rack system. No. And then for the verifiability because we are working in science with archive. It must be really that we can site real sources and there's no hallucination. Great. And then you can see even in those three dimension they decided to go with two or three sub indicators. So let's have a look. So for the knowledge synthesis or what I call hey how well is this generated EI text written? Is it informative? They have two indicators. the organization where an LLM as a J performs here a pairwise comparison and then they have here a metric that assesses here the informationational completeness. So what you do you have now from your text here a human written um segment here and this is automatically if you want chucked up here by the eye so you get single sentences for example but a human like red at all introduced here the transformer architecture which relies here on a self attention mechanism now the eye might say hey this is a two long sentence there are too many objects too many subjects I reduce this I chunk it up here in nuggets so the first nugget is redford at introduce the transformer stop. This is it. So simple to verify and then it goes on and you see exactly what's happening. Now you have then this essential nuggets and you want to make sure that those really represent here in the I generated text what is the main content of the human written sentence or paragraph or paper. Then second dimension the retrieval quality. Simple in my words. Did the I system find the right papers on archive for my topic. Here we have three indicators. The relevance rate, the reference coverage or simply hey if the system found here the real important papers and you know importance is just one but we can also measure here how impactful this is with an impact um quote. So we have a document importance metric. So this is a proxy for retrieving impactful word that uses here the citation comes from a specific API. Great. And finally the most important for me we have to have verifiability. So whatever we claim there is a check against some other published work. So are the claims now actually supported here by the citation? And here they go with two indicators. the citation precision where they say hey does the cited source support at least one claim made in the eye sentence just one and then they are say hey let's go full power now now for the term claim coverage we want do the cited sources support all the claims made in the eye sentence so it's really every little detail that the eye writes out here supported here by let's say arrive references Great. Then they decided, you know, so we have now the idea of the benchmark, we have the metric and the subindicator of the benchmark. And they say, you know what, now we build a vehicle. So we can switch now the engine of this vehicle. So we can put in either a GPT 4.1 or we can put in a GPD4 Omni or we go with a clawed model as the engine. And they built here the deep scholar base. Now this was interesting because remember I told you this is your almost rag system. No they went here with a classical rag multi multi- aent structure. So at first you have the query generation. So an LLM generates now a set of diverse search query based on the human input. So whatever I have the AI the LLM tells me hey that was a little bit too complicated. I generate now a set of diver search queries and then I send this up. So those queries now AI generated queries now run against the search API in our example archive and then we have a filtering now a semantic filtering and now we use another LLM is used to filter here to retrieve documents and anything that is irrelevant we just want to ignore this no but we have still a lot of documents like say 200 documents left now after the filtering so we have to do a reranking because we want to go down to the top 20 no so we have another llm based step that now reranks in detail to the only 200 remaining documents and it's a new prioritization here. What are the most relevance only of these 200 and then if we have the top 20 we want to have a semantic aggregation of the content of all 20 papers. No. So we say now the final top 20 documents are fed now as a context to an LLM and again you can choose your LLM that you like with a detailed prompt and then synthesization and the LLM writes it the final related works action. So clearly structured data processing pipeline and you can switch on and off any LLM in any place that you want and they tested here variation of different LLMs and they say so what are now the best performance combination so you see pipeline performing research this is great they run this here with a GPD4 as the default and then they exchange and said hey what happens if I add here let's say an 03 or a cloud model to GPD4.1 one all the prompts for this single steps that you just noticed here you have here in detail in the annex here of the documentation of this new research paper like here I show you the revised ODS react agent prompt here for calling here a tool so a lot of additional data I just want to give you the result this is it again you see one two three dimension and then all the different sub indicator in this dimension. Yeah, I think we can forget about the opensource research system from 2014. 
Relevance score: -8.718

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Great. So verifiability here open research scores shockingly low. This is what Stanford and UC Berkeler tells us. Citation precision numerical value is only 39. The claim coverage is only.13. And they tell us this is one of the worst performers here. that why is this happening? They suggest here a system optimized for fluent plausible sounding text is great on the one hand. So it's excellent in synthesizing here story or story line. But if you work in science, if you want to be absolute precise, you want to reduce hallucinating to the max and you want to make sure making logical leaps that are not strictly supported by the cited sources of archive are not part of your document. Now unfortunately this is happening here with open deep research. So for purely scientific work where you have to have absolute precision this could turn out to be a fatal flaw. Now, interestingly, if we look here at the new matter, the deep scholar base as I just showed you and we put here an 03 as the engine into this benchmark, you see, interestingly for the knowledge synthesis, we have the exact same benchmark 857.857. If we look here at the nugget, we even have a better performance here at the sequential tokenization if you want. And and this is now interesting with this new method. Look at the variability from one we jump to 6. So yeah, wow. This is an impressive improvement against the commercial openi deep research performance. So this new method by Stanford and UC Berkeley even if you use it with an openi model you see in one of the indicators they are at least as good as the commercial models but they outperform the commercial model significantly in variability. Isn't this beautiful? And if you ask yourself hey why I have suddenly two LLMs I told you you can have multiple LLMs here in the pipeline. So they decided for the SAM filter and the SAM top K data they go with GBD 4.1 but for the heavyduty SAM the aggregation step they go here with an 03 or a Claude or a Gemini just to make sure we on the same page. So this is the process. So we have the filter process then the top K the top 20 papers then the final aggregation the final section generation and I told you I show you always the prompt and here you have the prompt. So whatever LLM you choose an 03 or a claude OPUS 4 or a Gemini 2.5 Pro this is here the prompt for the aggregation for the final aggregation here of your research. So now we can see the paper here highlights here the deep scholar base can achieve up to 6.3 times higher claim coverage. So one indicator and one dimension six times three better than openi deep research. Nice. So we do have new systems that really outperform the classical commercial blackbox systems. But on the other hand, if you want just a plausible, well-written draft and you are not in science and you are willing, if you are in science, to meticulously fact check every single claim, every single half sentence that EI writes for you, then if you as a human do this verification and correct it, then open EI is really maybe also interesting. However, if you are a scientist and you say, I will go with the AI system that I can trust at least partially, then the deep scholar base here with claude or gemini model as the engine is dramatically superior. If you look at the data from Stanford and UC Berkeley, the paper states also that no method achieves a score greater than.19 when you average across the metric. So this is not an incremental challenge. You know, go from 78% to 82%. No, we at 19%. This is the best model we have. So we have quite a monumental challenge in front of us to make our AI models better, more performance, more power. So there's a lot of work in front of us. And if you say okay, I don't like here the numerical values in a table. I want to see this here in a visualization. Here you have exactly all the different elements here in um yeah to radar visualization. But this is exactly the data here from the numerical table I just showed you. But if you prefer to have it here in the radar, beautiful. Here is it. 
Relevance score: -8.881

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: 85. So I give you here my summary. OpenI is really here the leader in crafting a well ststructured and seemingly comprehensive narrative. Open deep research really excels in this synthesis and in the retrieval task and we have here the numerical data for the highest organization score 85 and the highest nugget coverage 39. output are well organized and capture really the essential fact better than the competitors can perform. And for the retrieval quality also the winner here. If you look here at the relevance rate 62 covers the most important human-sided reference with a reference coins of8 and finds the most impactful paper.1 but and this is now a big but the performance of open research collapses in the third dimension and this is the most crucial aspect if you have some scientific writing trust and verifiability. If you just write something that is not verifiable. 
Relevance score: -10.103

----------------------------------------------------------------
-------------------- New Query at 2025-09-03T18:30:58.335734 -------------

        ## Task
        The user's query: how deepscholar methodology work

        Answer the user's query based only on the retrieved context.
        Use chat history if relevant, but never go beyond the given context.
        Write a natural, context-based answer.


        ## Context
        Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Hello community so great that you are back. We have a new paper by Stanford University and UC Berkeley and it is all about science and a deep scholar AI system that is brand new. Now you know we have open-source research system no that do a research and synthesize known facts and new findings and you are familiar with this no for example here open scholar this is here the paper or if you go for another open source this is here storm or maybe you are familiar with deep researcher from April 2025 so we do have open systems but of course it's interesting how do they compare to the commercial system let's say openi deep research or we We have brand new search EIS where you remember MCP and here multi- aent system. So how do they compare here to multi- aent search EI systems in well let's say you do a research on the internet. Now for those we have now the study where they compare Llama forced to GPD 4.1 and 03 an Opus 4 by Claude and a Gemini 2.5 Pro. And if you ask me why not some other models, those are the set of models Stanford and UC Berkeley decided to perform their tests on. This is it. The framework they used here is here open deep search. The search with open-source reasoning agents and you remember we have here multi- aent system. We have an open reasoning agent here itself that interprets here the given human task and completes it here by orchestrating a sequence of action and one of those action might include here calling tools and one of those tools that is available to this agent is here the open search tool to connect to the internet and everything. So a multi- aent search utility and now we have a new paper, we have a new method, we have some new research benchmark results and some new ideas. So what is the task that we are looking at? Let's say you're a researcher and you have your title and maybe you have already an idea from a technical abstract. Yeah. And then you want to ask AI to write here the related work section of your publication for you. In the old times we call this a literature review. Have a look who published what in your particular specific domain, when it was published, what were the topics, how was it related to your work. Now everything should be done by the eye. Now this new system is called the deep scholar benchmark and yes it promises to generate here the deep the related work section the deep research on internet on archive on meta archive for you. So what it needs three capabilities you are looking for the retrieval. So you want to have a vast corpus the live internet or the live archive whatever you have then you want to have a synthesis report. So everything all the data that were replied here from multiple sources weave them together the findings into a coherent long form narrative and then you want and this is the most important constraint absolute verifiability. So whatever you are claiming here and whatever you are citing those must really exist. So the citing sources must be accurate on every claim and must be back traced back to its origin. You must have references. Great. So the authors here Stanford and UC Berkeley built here an automated data pipeline directly and they said you know what we not just for the internet let's go here for science let's go to archive. So we want to have a fresh data set retrieved absolutely fresh here daily or every minute here from archive. So how we do this? So three topics the recency the quality control and diversity and here they also limited theirel to computer science as a topic and they made sure that they have a diverse top with 18 different domains in computer science. So yeah we are restricted to computer science but I think you will see the results they speak for themselves. Here's the paper published today August 27, 2025. Deep scholar bench live benchmark on automated evolation for generative research systems from Stanford University in UC Berkeley. And the beauty is not that they have now a new benchmark, but what the new benchmark shows us. It shows us the true difficulty AI systems have if they do have to do a research synthesis. Here's the result. I would say wait a minute explain here at first the metric and the benchmark methodology. Okay. So they decided to go here for a threedimensional differentiation to say we look for the knowledge synthesis of this new system for the retrieval quality. It's more or less a rack system. No. And then for the verifiability because we are working in science with archive. It must be really that we can site real sources and there's no hallucination. Great. And then you can see even in those three dimension they decided to go with two or three sub indicators. So let's have a look. So for the knowledge synthesis or what I call hey how well is this generated EI text written? Is it informative? They have two indicators. the organization where an LLM as a J performs here a pairwise comparison and then they have here a metric that assesses here the informationational completeness. So what you do you have now from your text here a human written um segment here and this is automatically if you want chucked up here by the eye so you get single sentences for example but a human like red at all introduced here the transformer architecture which relies here on a self attention mechanism now the eye might say hey this is a two long sentence there are too many objects too many subjects I reduce this I chunk it up here in nuggets so the first nugget is redford at introduce the transformer stop. This is it. So simple to verify and then it goes on and you see exactly what's happening. Now you have then this essential nuggets and you want to make sure that those really represent here in the I generated text what is the main content of the human written sentence or paragraph or paper. Then second dimension the retrieval quality. Simple in my words. Did the I system find the right papers on archive for my topic. Here we have three indicators. The relevance rate, the reference coverage or simply hey if the system found here the real important papers and you know importance is just one but we can also measure here how impactful this is with an impact um quote. So we have a document importance metric. So this is a proxy for retrieving impactful word that uses here the citation comes from a specific API. Great. And finally the most important for me we have to have verifiability. So whatever we claim there is a check against some other published work. So are the claims now actually supported here by the citation? And here they go with two indicators. the citation precision where they say hey does the cited source support at least one claim made in the eye sentence just one and then they are say hey let's go full power now now for the term claim coverage we want do the cited sources support all the claims made in the eye sentence so it's really every little detail that the eye writes out here supported here by let's say arrive references Great. Then they decided, you know, so we have now the idea of the benchmark, we have the metric and the subindicator of the benchmark. And they say, you know what, now we build a vehicle. So we can switch now the engine of this vehicle. So we can put in either a GPT 4.1 or we can put in a GPD4 Omni or we go with a clawed model as the engine. And they built here the deep scholar base. Now this was interesting because remember I told you this is your almost rag system. No they went here with a classical rag multi multi- aent structure. So at first you have the query generation. So an LLM generates now a set of diverse search query based on the human input. So whatever I have the AI the LLM tells me hey that was a little bit too complicated. I generate now a set of diver search queries and then I send this up. So those queries now AI generated queries now run against the search API in our example archive and then we have a filtering now a semantic filtering and now we use another LLM is used to filter here to retrieve documents and anything that is irrelevant we just want to ignore this no but we have still a lot of documents like say 200 documents left now after the filtering so we have to do a reranking because we want to go down to the top 20 no so we have another llm based step that now reranks in detail to the only 200 remaining documents and it's a new prioritization here. What are the most relevance only of these 200 and then if we have the top 20 we want to have a semantic aggregation of the content of all 20 papers. No. So we say now the final top 20 documents are fed now as a context to an LLM and again you can choose your LLM that you like with a detailed prompt and then synthesization and the LLM writes it the final related works action. So clearly structured data processing pipeline and you can switch on and off any LLM in any place that you want and they tested here variation of different LLMs and they say so what are now the best performance combination so you see pipeline performing research this is great they run this here with a GPD4 as the default and then they exchange and said hey what happens if I add here let's say an 03 or a cloud model to GPD4.1 one all the prompts for this single steps that you just noticed here you have here in detail in the annex here of the documentation of this new research paper like here I show you the revised ODS react agent prompt here for calling here a tool so a lot of additional data I just want to give you the result this is it again you see one two three dimension and then all the different sub indicator in this dimension. Yeah, I think we can forget about the opensource research system from 2014. 
Relevance score: -8.718

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Great. So verifiability here open research scores shockingly low. This is what Stanford and UC Berkeler tells us. Citation precision numerical value is only 39. The claim coverage is only.13. And they tell us this is one of the worst performers here. that why is this happening? They suggest here a system optimized for fluent plausible sounding text is great on the one hand. So it's excellent in synthesizing here story or story line. But if you work in science, if you want to be absolute precise, you want to reduce hallucinating to the max and you want to make sure making logical leaps that are not strictly supported by the cited sources of archive are not part of your document. Now unfortunately this is happening here with open deep research. So for purely scientific work where you have to have absolute precision this could turn out to be a fatal flaw. Now, interestingly, if we look here at the new matter, the deep scholar base as I just showed you and we put here an 03 as the engine into this benchmark, you see, interestingly for the knowledge synthesis, we have the exact same benchmark 857.857. If we look here at the nugget, we even have a better performance here at the sequential tokenization if you want. And and this is now interesting with this new method. Look at the variability from one we jump to 6. So yeah, wow. This is an impressive improvement against the commercial openi deep research performance. So this new method by Stanford and UC Berkeley even if you use it with an openi model you see in one of the indicators they are at least as good as the commercial models but they outperform the commercial model significantly in variability. Isn't this beautiful? And if you ask yourself hey why I have suddenly two LLMs I told you you can have multiple LLMs here in the pipeline. So they decided for the SAM filter and the SAM top K data they go with GBD 4.1 but for the heavyduty SAM the aggregation step they go here with an 03 or a Claude or a Gemini just to make sure we on the same page. So this is the process. So we have the filter process then the top K the top 20 papers then the final aggregation the final section generation and I told you I show you always the prompt and here you have the prompt. So whatever LLM you choose an 03 or a claude OPUS 4 or a Gemini 2.5 Pro this is here the prompt for the aggregation for the final aggregation here of your research. So now we can see the paper here highlights here the deep scholar base can achieve up to 6.3 times higher claim coverage. So one indicator and one dimension six times three better than openi deep research. Nice. So we do have new systems that really outperform the classical commercial blackbox systems. But on the other hand, if you want just a plausible, well-written draft and you are not in science and you are willing, if you are in science, to meticulously fact check every single claim, every single half sentence that EI writes for you, then if you as a human do this verification and correct it, then open EI is really maybe also interesting. However, if you are a scientist and you say, I will go with the AI system that I can trust at least partially, then the deep scholar base here with claude or gemini model as the engine is dramatically superior. If you look at the data from Stanford and UC Berkeley, the paper states also that no method achieves a score greater than.19 when you average across the metric. So this is not an incremental challenge. You know, go from 78% to 82%. No, we at 19%. This is the best model we have. So we have quite a monumental challenge in front of us to make our AI models better, more performance, more power. So there's a lot of work in front of us. And if you say okay, I don't like here the numerical values in a table. I want to see this here in a visualization. Here you have exactly all the different elements here in um yeah to radar visualization. But this is exactly the data here from the numerical table I just showed you. But if you prefer to have it here in the radar, beautiful. Here is it. 
Relevance score: -8.881

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: 85. So I give you here my summary. OpenI is really here the leader in crafting a well ststructured and seemingly comprehensive narrative. Open deep research really excels in this synthesis and in the retrieval task and we have here the numerical data for the highest organization score 85 and the highest nugget coverage 39. output are well organized and capture really the essential fact better than the competitors can perform. And for the retrieval quality also the winner here. If you look here at the relevance rate 62 covers the most important human-sided reference with a reference coins of8 and finds the most impactful paper.1 but and this is now a big but the performance of open research collapses in the third dimension and this is the most crucial aspect if you have some scientific writing trust and verifiability. If you just write something that is not verifiable. 
Relevance score: -10.103

----------------------------------------------------------------
-------------------- New Query at 2025-09-03T18:35:12.399592 -------------

        ## Task
        The user's query: what is the importance of the citation

        Answer the user's query based only on the retrieved context.
        Use chat history if relevant, but never go beyond the given context.
        Write a natural, context-based answer.


        ## Context
        Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Great. So verifiability here open research scores shockingly low. This is what Stanford and UC Berkeler tells us. Citation precision numerical value is only 39. The claim coverage is only.13. And they tell us this is one of the worst performers here. that why is this happening? They suggest here a system optimized for fluent plausible sounding text is great on the one hand. So it's excellent in synthesizing here story or story line. But if you work in science, if you want to be absolute precise, you want to reduce hallucinating to the max and you want to make sure making logical leaps that are not strictly supported by the cited sources of archive are not part of your document. Now unfortunately this is happening here with open deep research. So for purely scientific work where you have to have absolute precision this could turn out to be a fatal flaw. Now, interestingly, if we look here at the new matter, the deep scholar base as I just showed you and we put here an 03 as the engine into this benchmark, you see, interestingly for the knowledge synthesis, we have the exact same benchmark 857.857. If we look here at the nugget, we even have a better performance here at the sequential tokenization if you want. And and this is now interesting with this new method. Look at the variability from one we jump to 6. So yeah, wow. This is an impressive improvement against the commercial openi deep research performance. So this new method by Stanford and UC Berkeley even if you use it with an openi model you see in one of the indicators they are at least as good as the commercial models but they outperform the commercial model significantly in variability. Isn't this beautiful? And if you ask yourself hey why I have suddenly two LLMs I told you you can have multiple LLMs here in the pipeline. So they decided for the SAM filter and the SAM top K data they go with GBD 4.1 but for the heavyduty SAM the aggregation step they go here with an 03 or a Claude or a Gemini just to make sure we on the same page. So this is the process. So we have the filter process then the top K the top 20 papers then the final aggregation the final section generation and I told you I show you always the prompt and here you have the prompt. So whatever LLM you choose an 03 or a claude OPUS 4 or a Gemini 2.5 Pro this is here the prompt for the aggregation for the final aggregation here of your research. So now we can see the paper here highlights here the deep scholar base can achieve up to 6.3 times higher claim coverage. So one indicator and one dimension six times three better than openi deep research. Nice. So we do have new systems that really outperform the classical commercial blackbox systems. But on the other hand, if you want just a plausible, well-written draft and you are not in science and you are willing, if you are in science, to meticulously fact check every single claim, every single half sentence that EI writes for you, then if you as a human do this verification and correct it, then open EI is really maybe also interesting. However, if you are a scientist and you say, I will go with the AI system that I can trust at least partially, then the deep scholar base here with claude or gemini model as the engine is dramatically superior. If you look at the data from Stanford and UC Berkeley, the paper states also that no method achieves a score greater than.19 when you average across the metric. So this is not an incremental challenge. You know, go from 78% to 82%. No, we at 19%. This is the best model we have. So we have quite a monumental challenge in front of us to make our AI models better, more performance, more power. So there's a lot of work in front of us. And if you say okay, I don't like here the numerical values in a table. I want to see this here in a visualization. Here you have exactly all the different elements here in um yeah to radar visualization. But this is exactly the data here from the numerical table I just showed you. But if you prefer to have it here in the radar, beautiful. Here is it. 
Relevance score: -9.431

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: 85. So I give you here my summary. OpenI is really here the leader in crafting a well ststructured and seemingly comprehensive narrative. Open deep research really excels in this synthesis and in the retrieval task and we have here the numerical data for the highest organization score 85 and the highest nugget coverage 39. output are well organized and capture really the essential fact better than the competitors can perform. And for the retrieval quality also the winner here. If you look here at the relevance rate 62 covers the most important human-sided reference with a reference coins of8 and finds the most impactful paper.1 but and this is now a big but the performance of open research collapses in the third dimension and this is the most crucial aspect if you have some scientific writing trust and verifiability. If you just write something that is not verifiable. 
Relevance score: -9.973

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Hello community so great that you are back. We have a new paper by Stanford University and UC Berkeley and it is all about science and a deep scholar AI system that is brand new. Now you know we have open-source research system no that do a research and synthesize known facts and new findings and you are familiar with this no for example here open scholar this is here the paper or if you go for another open source this is here storm or maybe you are familiar with deep researcher from April 2025 so we do have open systems but of course it's interesting how do they compare to the commercial system let's say openi deep research or we We have brand new search EIS where you remember MCP and here multi- aent system. So how do they compare here to multi- aent search EI systems in well let's say you do a research on the internet. Now for those we have now the study where they compare Llama forced to GPD 4.1 and 03 an Opus 4 by Claude and a Gemini 2.5 Pro. And if you ask me why not some other models, those are the set of models Stanford and UC Berkeley decided to perform their tests on. This is it. The framework they used here is here open deep search. The search with open-source reasoning agents and you remember we have here multi- aent system. We have an open reasoning agent here itself that interprets here the given human task and completes it here by orchestrating a sequence of action and one of those action might include here calling tools and one of those tools that is available to this agent is here the open search tool to connect to the internet and everything. So a multi- aent search utility and now we have a new paper, we have a new method, we have some new research benchmark results and some new ideas. So what is the task that we are looking at? Let's say you're a researcher and you have your title and maybe you have already an idea from a technical abstract. Yeah. And then you want to ask AI to write here the related work section of your publication for you. In the old times we call this a literature review. Have a look who published what in your particular specific domain, when it was published, what were the topics, how was it related to your work. Now everything should be done by the eye. Now this new system is called the deep scholar benchmark and yes it promises to generate here the deep the related work section the deep research on internet on archive on meta archive for you. So what it needs three capabilities you are looking for the retrieval. So you want to have a vast corpus the live internet or the live archive whatever you have then you want to have a synthesis report. So everything all the data that were replied here from multiple sources weave them together the findings into a coherent long form narrative and then you want and this is the most important constraint absolute verifiability. So whatever you are claiming here and whatever you are citing those must really exist. So the citing sources must be accurate on every claim and must be back traced back to its origin. You must have references. Great. So the authors here Stanford and UC Berkeley built here an automated data pipeline directly and they said you know what we not just for the internet let's go here for science let's go to archive. So we want to have a fresh data set retrieved absolutely fresh here daily or every minute here from archive. So how we do this? So three topics the recency the quality control and diversity and here they also limited theirel to computer science as a topic and they made sure that they have a diverse top with 18 different domains in computer science. So yeah we are restricted to computer science but I think you will see the results they speak for themselves. Here's the paper published today August 27, 2025. Deep scholar bench live benchmark on automated evolation for generative research systems from Stanford University in UC Berkeley. And the beauty is not that they have now a new benchmark, but what the new benchmark shows us. It shows us the true difficulty AI systems have if they do have to do a research synthesis. Here's the result. I would say wait a minute explain here at first the metric and the benchmark methodology. Okay. So they decided to go here for a threedimensional differentiation to say we look for the knowledge synthesis of this new system for the retrieval quality. It's more or less a rack system. No. And then for the verifiability because we are working in science with archive. It must be really that we can site real sources and there's no hallucination. Great. And then you can see even in those three dimension they decided to go with two or three sub indicators. So let's have a look. So for the knowledge synthesis or what I call hey how well is this generated EI text written? Is it informative? They have two indicators. the organization where an LLM as a J performs here a pairwise comparison and then they have here a metric that assesses here the informationational completeness. So what you do you have now from your text here a human written um segment here and this is automatically if you want chucked up here by the eye so you get single sentences for example but a human like red at all introduced here the transformer architecture which relies here on a self attention mechanism now the eye might say hey this is a two long sentence there are too many objects too many subjects I reduce this I chunk it up here in nuggets so the first nugget is redford at introduce the transformer stop. This is it. So simple to verify and then it goes on and you see exactly what's happening. Now you have then this essential nuggets and you want to make sure that those really represent here in the I generated text what is the main content of the human written sentence or paragraph or paper. Then second dimension the retrieval quality. Simple in my words. Did the I system find the right papers on archive for my topic. Here we have three indicators. The relevance rate, the reference coverage or simply hey if the system found here the real important papers and you know importance is just one but we can also measure here how impactful this is with an impact um quote. So we have a document importance metric. So this is a proxy for retrieving impactful word that uses here the citation comes from a specific API. Great. And finally the most important for me we have to have verifiability. So whatever we claim there is a check against some other published work. So are the claims now actually supported here by the citation? And here they go with two indicators. the citation precision where they say hey does the cited source support at least one claim made in the eye sentence just one and then they are say hey let's go full power now now for the term claim coverage we want do the cited sources support all the claims made in the eye sentence so it's really every little detail that the eye writes out here supported here by let's say arrive references Great. Then they decided, you know, so we have now the idea of the benchmark, we have the metric and the subindicator of the benchmark. And they say, you know what, now we build a vehicle. So we can switch now the engine of this vehicle. So we can put in either a GPT 4.1 or we can put in a GPD4 Omni or we go with a clawed model as the engine. And they built here the deep scholar base. Now this was interesting because remember I told you this is your almost rag system. No they went here with a classical rag multi multi- aent structure. So at first you have the query generation. So an LLM generates now a set of diverse search query based on the human input. So whatever I have the AI the LLM tells me hey that was a little bit too complicated. I generate now a set of diver search queries and then I send this up. So those queries now AI generated queries now run against the search API in our example archive and then we have a filtering now a semantic filtering and now we use another LLM is used to filter here to retrieve documents and anything that is irrelevant we just want to ignore this no but we have still a lot of documents like say 200 documents left now after the filtering so we have to do a reranking because we want to go down to the top 20 no so we have another llm based step that now reranks in detail to the only 200 remaining documents and it's a new prioritization here. What are the most relevance only of these 200 and then if we have the top 20 we want to have a semantic aggregation of the content of all 20 papers. No. So we say now the final top 20 documents are fed now as a context to an LLM and again you can choose your LLM that you like with a detailed prompt and then synthesization and the LLM writes it the final related works action. So clearly structured data processing pipeline and you can switch on and off any LLM in any place that you want and they tested here variation of different LLMs and they say so what are now the best performance combination so you see pipeline performing research this is great they run this here with a GPD4 as the default and then they exchange and said hey what happens if I add here let's say an 03 or a cloud model to GPD4.1 one all the prompts for this single steps that you just noticed here you have here in detail in the annex here of the documentation of this new research paper like here I show you the revised ODS react agent prompt here for calling here a tool so a lot of additional data I just want to give you the result this is it again you see one two three dimension and then all the different sub indicator in this dimension. Yeah, I think we can forget about the opensource research system from 2014. 
Relevance score: -10.762

----------------------------------------------------------------
-------------------- New Query at 2025-09-03T18:42:07.016662 -------------

        ## Task
        The user's query: what is methodology?

        Answer the user's query based only on the retrieved context.
        Use chat history if relevant, but never go beyond the given context.
        Write a natural, context-based answer.


        ## Context
        Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Hello community so great that you are back. We have a new paper by Stanford University and UC Berkeley and it is all about science and a deep scholar AI system that is brand new. Now you know we have open-source research system no that do a research and synthesize known facts and new findings and you are familiar with this no for example here open scholar this is here the paper or if you go for another open source this is here storm or maybe you are familiar with deep researcher from April 2025 so we do have open systems but of course it's interesting how do they compare to the commercial system let's say openi deep research or we We have brand new search EIS where you remember MCP and here multi- aent system. So how do they compare here to multi- aent search EI systems in well let's say you do a research on the internet. Now for those we have now the study where they compare Llama forced to GPD 4.1 and 03 an Opus 4 by Claude and a Gemini 2.5 Pro. And if you ask me why not some other models, those are the set of models Stanford and UC Berkeley decided to perform their tests on. This is it. The framework they used here is here open deep search. The search with open-source reasoning agents and you remember we have here multi- aent system. We have an open reasoning agent here itself that interprets here the given human task and completes it here by orchestrating a sequence of action and one of those action might include here calling tools and one of those tools that is available to this agent is here the open search tool to connect to the internet and everything. So a multi- aent search utility and now we have a new paper, we have a new method, we have some new research benchmark results and some new ideas. So what is the task that we are looking at? Let's say you're a researcher and you have your title and maybe you have already an idea from a technical abstract. Yeah. And then you want to ask AI to write here the related work section of your publication for you. In the old times we call this a literature review. Have a look who published what in your particular specific domain, when it was published, what were the topics, how was it related to your work. Now everything should be done by the eye. Now this new system is called the deep scholar benchmark and yes it promises to generate here the deep the related work section the deep research on internet on archive on meta archive for you. So what it needs three capabilities you are looking for the retrieval. So you want to have a vast corpus the live internet or the live archive whatever you have then you want to have a synthesis report. So everything all the data that were replied here from multiple sources weave them together the findings into a coherent long form narrative and then you want and this is the most important constraint absolute verifiability. So whatever you are claiming here and whatever you are citing those must really exist. So the citing sources must be accurate on every claim and must be back traced back to its origin. You must have references. Great. So the authors here Stanford and UC Berkeley built here an automated data pipeline directly and they said you know what we not just for the internet let's go here for science let's go to archive. So we want to have a fresh data set retrieved absolutely fresh here daily or every minute here from archive. So how we do this? So three topics the recency the quality control and diversity and here they also limited theirel to computer science as a topic and they made sure that they have a diverse top with 18 different domains in computer science. So yeah we are restricted to computer science but I think you will see the results they speak for themselves. Here's the paper published today August 27, 2025. Deep scholar bench live benchmark on automated evolation for generative research systems from Stanford University in UC Berkeley. And the beauty is not that they have now a new benchmark, but what the new benchmark shows us. It shows us the true difficulty AI systems have if they do have to do a research synthesis. Here's the result. I would say wait a minute explain here at first the metric and the benchmark methodology. Okay. So they decided to go here for a threedimensional differentiation to say we look for the knowledge synthesis of this new system for the retrieval quality. It's more or less a rack system. No. And then for the verifiability because we are working in science with archive. It must be really that we can site real sources and there's no hallucination. Great. And then you can see even in those three dimension they decided to go with two or three sub indicators. So let's have a look. So for the knowledge synthesis or what I call hey how well is this generated EI text written? Is it informative? They have two indicators. the organization where an LLM as a J performs here a pairwise comparison and then they have here a metric that assesses here the informationational completeness. So what you do you have now from your text here a human written um segment here and this is automatically if you want chucked up here by the eye so you get single sentences for example but a human like red at all introduced here the transformer architecture which relies here on a self attention mechanism now the eye might say hey this is a two long sentence there are too many objects too many subjects I reduce this I chunk it up here in nuggets so the first nugget is redford at introduce the transformer stop. This is it. So simple to verify and then it goes on and you see exactly what's happening. Now you have then this essential nuggets and you want to make sure that those really represent here in the I generated text what is the main content of the human written sentence or paragraph or paper. Then second dimension the retrieval quality. Simple in my words. Did the I system find the right papers on archive for my topic. Here we have three indicators. The relevance rate, the reference coverage or simply hey if the system found here the real important papers and you know importance is just one but we can also measure here how impactful this is with an impact um quote. So we have a document importance metric. So this is a proxy for retrieving impactful word that uses here the citation comes from a specific API. Great. And finally the most important for me we have to have verifiability. So whatever we claim there is a check against some other published work. So are the claims now actually supported here by the citation? And here they go with two indicators. the citation precision where they say hey does the cited source support at least one claim made in the eye sentence just one and then they are say hey let's go full power now now for the term claim coverage we want do the cited sources support all the claims made in the eye sentence so it's really every little detail that the eye writes out here supported here by let's say arrive references Great. Then they decided, you know, so we have now the idea of the benchmark, we have the metric and the subindicator of the benchmark. And they say, you know what, now we build a vehicle. So we can switch now the engine of this vehicle. So we can put in either a GPT 4.1 or we can put in a GPD4 Omni or we go with a clawed model as the engine. And they built here the deep scholar base. Now this was interesting because remember I told you this is your almost rag system. No they went here with a classical rag multi multi- aent structure. So at first you have the query generation. So an LLM generates now a set of diverse search query based on the human input. So whatever I have the AI the LLM tells me hey that was a little bit too complicated. I generate now a set of diver search queries and then I send this up. So those queries now AI generated queries now run against the search API in our example archive and then we have a filtering now a semantic filtering and now we use another LLM is used to filter here to retrieve documents and anything that is irrelevant we just want to ignore this no but we have still a lot of documents like say 200 documents left now after the filtering so we have to do a reranking because we want to go down to the top 20 no so we have another llm based step that now reranks in detail to the only 200 remaining documents and it's a new prioritization here. What are the most relevance only of these 200 and then if we have the top 20 we want to have a semantic aggregation of the content of all 20 papers. No. So we say now the final top 20 documents are fed now as a context to an LLM and again you can choose your LLM that you like with a detailed prompt and then synthesization and the LLM writes it the final related works action. So clearly structured data processing pipeline and you can switch on and off any LLM in any place that you want and they tested here variation of different LLMs and they say so what are now the best performance combination so you see pipeline performing research this is great they run this here with a GPD4 as the default and then they exchange and said hey what happens if I add here let's say an 03 or a cloud model to GPD4.1 one all the prompts for this single steps that you just noticed here you have here in detail in the annex here of the documentation of this new research paper like here I show you the revised ODS react agent prompt here for calling here a tool so a lot of additional data I just want to give you the result this is it again you see one two three dimension and then all the different sub indicator in this dimension. Yeah, I think we can forget about the opensource research system from 2014. 
Relevance score: -9.663

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Great. So verifiability here open research scores shockingly low. This is what Stanford and UC Berkeler tells us. Citation precision numerical value is only 39. The claim coverage is only.13. And they tell us this is one of the worst performers here. that why is this happening? They suggest here a system optimized for fluent plausible sounding text is great on the one hand. So it's excellent in synthesizing here story or story line. But if you work in science, if you want to be absolute precise, you want to reduce hallucinating to the max and you want to make sure making logical leaps that are not strictly supported by the cited sources of archive are not part of your document. Now unfortunately this is happening here with open deep research. So for purely scientific work where you have to have absolute precision this could turn out to be a fatal flaw. Now, interestingly, if we look here at the new matter, the deep scholar base as I just showed you and we put here an 03 as the engine into this benchmark, you see, interestingly for the knowledge synthesis, we have the exact same benchmark 857.857. If we look here at the nugget, we even have a better performance here at the sequential tokenization if you want. And and this is now interesting with this new method. Look at the variability from one we jump to 6. So yeah, wow. This is an impressive improvement against the commercial openi deep research performance. So this new method by Stanford and UC Berkeley even if you use it with an openi model you see in one of the indicators they are at least as good as the commercial models but they outperform the commercial model significantly in variability. Isn't this beautiful? And if you ask yourself hey why I have suddenly two LLMs I told you you can have multiple LLMs here in the pipeline. So they decided for the SAM filter and the SAM top K data they go with GBD 4.1 but for the heavyduty SAM the aggregation step they go here with an 03 or a Claude or a Gemini just to make sure we on the same page. So this is the process. So we have the filter process then the top K the top 20 papers then the final aggregation the final section generation and I told you I show you always the prompt and here you have the prompt. So whatever LLM you choose an 03 or a claude OPUS 4 or a Gemini 2.5 Pro this is here the prompt for the aggregation for the final aggregation here of your research. So now we can see the paper here highlights here the deep scholar base can achieve up to 6.3 times higher claim coverage. So one indicator and one dimension six times three better than openi deep research. Nice. So we do have new systems that really outperform the classical commercial blackbox systems. But on the other hand, if you want just a plausible, well-written draft and you are not in science and you are willing, if you are in science, to meticulously fact check every single claim, every single half sentence that EI writes for you, then if you as a human do this verification and correct it, then open EI is really maybe also interesting. However, if you are a scientist and you say, I will go with the AI system that I can trust at least partially, then the deep scholar base here with claude or gemini model as the engine is dramatically superior. If you look at the data from Stanford and UC Berkeley, the paper states also that no method achieves a score greater than.19 when you average across the metric. So this is not an incremental challenge. You know, go from 78% to 82%. No, we at 19%. This is the best model we have. So we have quite a monumental challenge in front of us to make our AI models better, more performance, more power. So there's a lot of work in front of us. And if you say okay, I don't like here the numerical values in a table. I want to see this here in a visualization. Here you have exactly all the different elements here in um yeah to radar visualization. But this is exactly the data here from the numerical table I just showed you. But if you prefer to have it here in the radar, beautiful. Here is it. 
Relevance score: -9.988

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: I'm a little bit early. So they have not yet uploaded here the code here in the demonstration to GitHub. And here you have on GitHub the gastrin lab at Stanford University. Here you have your link. So whenever you should have a look, they should have uploaded here this new methodology already. For the moment it is not available yet. 
Relevance score: -10.648

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: 85. So I give you here my summary. OpenI is really here the leader in crafting a well ststructured and seemingly comprehensive narrative. Open deep research really excels in this synthesis and in the retrieval task and we have here the numerical data for the highest organization score 85 and the highest nugget coverage 39. output are well organized and capture really the essential fact better than the competitors can perform. And for the retrieval quality also the winner here. If you look here at the relevance rate 62 covers the most important human-sided reference with a reference coins of8 and finds the most impactful paper.1 but and this is now a big but the performance of open research collapses in the third dimension and this is the most crucial aspect if you have some scientific writing trust and verifiability. If you just write something that is not verifiable. 
Relevance score: -10.916

----------------------------------------------------------------
-------------------- New Query at 2025-09-03T18:42:55.871562 -------------

        ## Task
        The user's query: a summary of the methodology of the new benchmark. 

        Answer the user's query based only on the retrieved context.
        Use chat history if relevant, but never go beyond the given context.
        Write a natural, context-based answer.


        ## Context
        Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Great. So verifiability here open research scores shockingly low. This is what Stanford and UC Berkeler tells us. Citation precision numerical value is only 39. The claim coverage is only.13. And they tell us this is one of the worst performers here. that why is this happening? They suggest here a system optimized for fluent plausible sounding text is great on the one hand. So it's excellent in synthesizing here story or story line. But if you work in science, if you want to be absolute precise, you want to reduce hallucinating to the max and you want to make sure making logical leaps that are not strictly supported by the cited sources of archive are not part of your document. Now unfortunately this is happening here with open deep research. So for purely scientific work where you have to have absolute precision this could turn out to be a fatal flaw. Now, interestingly, if we look here at the new matter, the deep scholar base as I just showed you and we put here an 03 as the engine into this benchmark, you see, interestingly for the knowledge synthesis, we have the exact same benchmark 857.857. If we look here at the nugget, we even have a better performance here at the sequential tokenization if you want. And and this is now interesting with this new method. Look at the variability from one we jump to 6. So yeah, wow. This is an impressive improvement against the commercial openi deep research performance. So this new method by Stanford and UC Berkeley even if you use it with an openi model you see in one of the indicators they are at least as good as the commercial models but they outperform the commercial model significantly in variability. Isn't this beautiful? And if you ask yourself hey why I have suddenly two LLMs I told you you can have multiple LLMs here in the pipeline. So they decided for the SAM filter and the SAM top K data they go with GBD 4.1 but for the heavyduty SAM the aggregation step they go here with an 03 or a Claude or a Gemini just to make sure we on the same page. So this is the process. So we have the filter process then the top K the top 20 papers then the final aggregation the final section generation and I told you I show you always the prompt and here you have the prompt. So whatever LLM you choose an 03 or a claude OPUS 4 or a Gemini 2.5 Pro this is here the prompt for the aggregation for the final aggregation here of your research. So now we can see the paper here highlights here the deep scholar base can achieve up to 6.3 times higher claim coverage. So one indicator and one dimension six times three better than openi deep research. Nice. So we do have new systems that really outperform the classical commercial blackbox systems. But on the other hand, if you want just a plausible, well-written draft and you are not in science and you are willing, if you are in science, to meticulously fact check every single claim, every single half sentence that EI writes for you, then if you as a human do this verification and correct it, then open EI is really maybe also interesting. However, if you are a scientist and you say, I will go with the AI system that I can trust at least partially, then the deep scholar base here with claude or gemini model as the engine is dramatically superior. If you look at the data from Stanford and UC Berkeley, the paper states also that no method achieves a score greater than.19 when you average across the metric. So this is not an incremental challenge. You know, go from 78% to 82%. No, we at 19%. This is the best model we have. So we have quite a monumental challenge in front of us to make our AI models better, more performance, more power. So there's a lot of work in front of us. And if you say okay, I don't like here the numerical values in a table. I want to see this here in a visualization. Here you have exactly all the different elements here in um yeah to radar visualization. But this is exactly the data here from the numerical table I just showed you. But if you prefer to have it here in the radar, beautiful. Here is it. 
Relevance score: -7.710

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Hello community so great that you are back. We have a new paper by Stanford University and UC Berkeley and it is all about science and a deep scholar AI system that is brand new. Now you know we have open-source research system no that do a research and synthesize known facts and new findings and you are familiar with this no for example here open scholar this is here the paper or if you go for another open source this is here storm or maybe you are familiar with deep researcher from April 2025 so we do have open systems but of course it's interesting how do they compare to the commercial system let's say openi deep research or we We have brand new search EIS where you remember MCP and here multi- aent system. So how do they compare here to multi- aent search EI systems in well let's say you do a research on the internet. Now for those we have now the study where they compare Llama forced to GPD 4.1 and 03 an Opus 4 by Claude and a Gemini 2.5 Pro. And if you ask me why not some other models, those are the set of models Stanford and UC Berkeley decided to perform their tests on. This is it. The framework they used here is here open deep search. The search with open-source reasoning agents and you remember we have here multi- aent system. We have an open reasoning agent here itself that interprets here the given human task and completes it here by orchestrating a sequence of action and one of those action might include here calling tools and one of those tools that is available to this agent is here the open search tool to connect to the internet and everything. So a multi- aent search utility and now we have a new paper, we have a new method, we have some new research benchmark results and some new ideas. So what is the task that we are looking at? Let's say you're a researcher and you have your title and maybe you have already an idea from a technical abstract. Yeah. And then you want to ask AI to write here the related work section of your publication for you. In the old times we call this a literature review. Have a look who published what in your particular specific domain, when it was published, what were the topics, how was it related to your work. Now everything should be done by the eye. Now this new system is called the deep scholar benchmark and yes it promises to generate here the deep the related work section the deep research on internet on archive on meta archive for you. So what it needs three capabilities you are looking for the retrieval. So you want to have a vast corpus the live internet or the live archive whatever you have then you want to have a synthesis report. So everything all the data that were replied here from multiple sources weave them together the findings into a coherent long form narrative and then you want and this is the most important constraint absolute verifiability. So whatever you are claiming here and whatever you are citing those must really exist. So the citing sources must be accurate on every claim and must be back traced back to its origin. You must have references. Great. So the authors here Stanford and UC Berkeley built here an automated data pipeline directly and they said you know what we not just for the internet let's go here for science let's go to archive. So we want to have a fresh data set retrieved absolutely fresh here daily or every minute here from archive. So how we do this? So three topics the recency the quality control and diversity and here they also limited theirel to computer science as a topic and they made sure that they have a diverse top with 18 different domains in computer science. So yeah we are restricted to computer science but I think you will see the results they speak for themselves. Here's the paper published today August 27, 2025. Deep scholar bench live benchmark on automated evolation for generative research systems from Stanford University in UC Berkeley. And the beauty is not that they have now a new benchmark, but what the new benchmark shows us. It shows us the true difficulty AI systems have if they do have to do a research synthesis. Here's the result. I would say wait a minute explain here at first the metric and the benchmark methodology. Okay. So they decided to go here for a threedimensional differentiation to say we look for the knowledge synthesis of this new system for the retrieval quality. It's more or less a rack system. No. And then for the verifiability because we are working in science with archive. It must be really that we can site real sources and there's no hallucination. Great. And then you can see even in those three dimension they decided to go with two or three sub indicators. So let's have a look. So for the knowledge synthesis or what I call hey how well is this generated EI text written? Is it informative? They have two indicators. the organization where an LLM as a J performs here a pairwise comparison and then they have here a metric that assesses here the informationational completeness. So what you do you have now from your text here a human written um segment here and this is automatically if you want chucked up here by the eye so you get single sentences for example but a human like red at all introduced here the transformer architecture which relies here on a self attention mechanism now the eye might say hey this is a two long sentence there are too many objects too many subjects I reduce this I chunk it up here in nuggets so the first nugget is redford at introduce the transformer stop. This is it. So simple to verify and then it goes on and you see exactly what's happening. Now you have then this essential nuggets and you want to make sure that those really represent here in the I generated text what is the main content of the human written sentence or paragraph or paper. Then second dimension the retrieval quality. Simple in my words. Did the I system find the right papers on archive for my topic. Here we have three indicators. The relevance rate, the reference coverage or simply hey if the system found here the real important papers and you know importance is just one but we can also measure here how impactful this is with an impact um quote. So we have a document importance metric. So this is a proxy for retrieving impactful word that uses here the citation comes from a specific API. Great. And finally the most important for me we have to have verifiability. So whatever we claim there is a check against some other published work. So are the claims now actually supported here by the citation? And here they go with two indicators. the citation precision where they say hey does the cited source support at least one claim made in the eye sentence just one and then they are say hey let's go full power now now for the term claim coverage we want do the cited sources support all the claims made in the eye sentence so it's really every little detail that the eye writes out here supported here by let's say arrive references Great. Then they decided, you know, so we have now the idea of the benchmark, we have the metric and the subindicator of the benchmark. And they say, you know what, now we build a vehicle. So we can switch now the engine of this vehicle. So we can put in either a GPT 4.1 or we can put in a GPD4 Omni or we go with a clawed model as the engine. And they built here the deep scholar base. Now this was interesting because remember I told you this is your almost rag system. No they went here with a classical rag multi multi- aent structure. So at first you have the query generation. So an LLM generates now a set of diverse search query based on the human input. So whatever I have the AI the LLM tells me hey that was a little bit too complicated. I generate now a set of diver search queries and then I send this up. So those queries now AI generated queries now run against the search API in our example archive and then we have a filtering now a semantic filtering and now we use another LLM is used to filter here to retrieve documents and anything that is irrelevant we just want to ignore this no but we have still a lot of documents like say 200 documents left now after the filtering so we have to do a reranking because we want to go down to the top 20 no so we have another llm based step that now reranks in detail to the only 200 remaining documents and it's a new prioritization here. What are the most relevance only of these 200 and then if we have the top 20 we want to have a semantic aggregation of the content of all 20 papers. No. So we say now the final top 20 documents are fed now as a context to an LLM and again you can choose your LLM that you like with a detailed prompt and then synthesization and the LLM writes it the final related works action. So clearly structured data processing pipeline and you can switch on and off any LLM in any place that you want and they tested here variation of different LLMs and they say so what are now the best performance combination so you see pipeline performing research this is great they run this here with a GPD4 as the default and then they exchange and said hey what happens if I add here let's say an 03 or a cloud model to GPD4.1 one all the prompts for this single steps that you just noticed here you have here in detail in the annex here of the documentation of this new research paper like here I show you the revised ODS react agent prompt here for calling here a tool so a lot of additional data I just want to give you the result this is it again you see one two three dimension and then all the different sub indicator in this dimension. Yeah, I think we can forget about the opensource research system from 2014. 
Relevance score: -8.523

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: 85. So I give you here my summary. OpenI is really here the leader in crafting a well ststructured and seemingly comprehensive narrative. Open deep research really excels in this synthesis and in the retrieval task and we have here the numerical data for the highest organization score 85 and the highest nugget coverage 39. output are well organized and capture really the essential fact better than the competitors can perform. And for the retrieval quality also the winner here. If you look here at the relevance rate 62 covers the most important human-sided reference with a reference coins of8 and finds the most impactful paper.1 but and this is now a big but the performance of open research collapses in the third dimension and this is the most crucial aspect if you have some scientific writing trust and verifiability. If you just write something that is not verifiable. 
Relevance score: -10.524

----------------------------------------------------------------
-------------------- New Query at 2025-09-03T18:44:55.462553 -------------

        ## Task
        The user's query: a summary of the methodology of the new benchmark. 

        Answer the user's query based only on the retrieved context.
        Use chat history if relevant, but never go beyond the given context.
        Write a natural, context-based answer.


        ## Context
        Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Great. So verifiability here open research scores shockingly low. This is what Stanford and UC Berkeler tells us. Citation precision numerical value is only 39. The claim coverage is only.13. And they tell us this is one of the worst performers here. that why is this happening? They suggest here a system optimized for fluent plausible sounding text is great on the one hand. So it's excellent in synthesizing here story or story line. But if you work in science, if you want to be absolute precise, you want to reduce hallucinating to the max and you want to make sure making logical leaps that are not strictly supported by the cited sources of archive are not part of your document. Now unfortunately this is happening here with open deep research. So for purely scientific work where you have to have absolute precision this could turn out to be a fatal flaw. Now, interestingly, if we look here at the new matter, the deep scholar base as I just showed you and we put here an 03 as the engine into this benchmark, you see, interestingly for the knowledge synthesis, we have the exact same benchmark 857.857. If we look here at the nugget, we even have a better performance here at the sequential tokenization if you want. And and this is now interesting with this new method. Look at the variability from one we jump to 6. So yeah, wow. This is an impressive improvement against the commercial openi deep research performance. So this new method by Stanford and UC Berkeley even if you use it with an openi model you see in one of the indicators they are at least as good as the commercial models but they outperform the commercial model significantly in variability. Isn't this beautiful? And if you ask yourself hey why I have suddenly two LLMs I told you you can have multiple LLMs here in the pipeline. So they decided for the SAM filter and the SAM top K data they go with GBD 4.1 but for the heavyduty SAM the aggregation step they go here with an 03 or a Claude or a Gemini just to make sure we on the same page. So this is the process. So we have the filter process then the top K the top 20 papers then the final aggregation the final section generation and I told you I show you always the prompt and here you have the prompt. So whatever LLM you choose an 03 or a claude OPUS 4 or a Gemini 2.5 Pro this is here the prompt for the aggregation for the final aggregation here of your research. So now we can see the paper here highlights here the deep scholar base can achieve up to 6.3 times higher claim coverage. So one indicator and one dimension six times three better than openi deep research. Nice. So we do have new systems that really outperform the classical commercial blackbox systems. But on the other hand, if you want just a plausible, well-written draft and you are not in science and you are willing, if you are in science, to meticulously fact check every single claim, every single half sentence that EI writes for you, then if you as a human do this verification and correct it, then open EI is really maybe also interesting. However, if you are a scientist and you say, I will go with the AI system that I can trust at least partially, then the deep scholar base here with claude or gemini model as the engine is dramatically superior. If you look at the data from Stanford and UC Berkeley, the paper states also that no method achieves a score greater than.19 when you average across the metric. So this is not an incremental challenge. You know, go from 78% to 82%. No, we at 19%. This is the best model we have. So we have quite a monumental challenge in front of us to make our AI models better, more performance, more power. So there's a lot of work in front of us. And if you say okay, I don't like here the numerical values in a table. I want to see this here in a visualization. Here you have exactly all the different elements here in um yeah to radar visualization. But this is exactly the data here from the numerical table I just showed you. But if you prefer to have it here in the radar, beautiful. Here is it. 
Relevance score: -7.710

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Hello community so great that you are back. We have a new paper by Stanford University and UC Berkeley and it is all about science and a deep scholar AI system that is brand new. Now you know we have open-source research system no that do a research and synthesize known facts and new findings and you are familiar with this no for example here open scholar this is here the paper or if you go for another open source this is here storm or maybe you are familiar with deep researcher from April 2025 so we do have open systems but of course it's interesting how do they compare to the commercial system let's say openi deep research or we We have brand new search EIS where you remember MCP and here multi- aent system. So how do they compare here to multi- aent search EI systems in well let's say you do a research on the internet. Now for those we have now the study where they compare Llama forced to GPD 4.1 and 03 an Opus 4 by Claude and a Gemini 2.5 Pro. And if you ask me why not some other models, those are the set of models Stanford and UC Berkeley decided to perform their tests on. This is it. The framework they used here is here open deep search. The search with open-source reasoning agents and you remember we have here multi- aent system. We have an open reasoning agent here itself that interprets here the given human task and completes it here by orchestrating a sequence of action and one of those action might include here calling tools and one of those tools that is available to this agent is here the open search tool to connect to the internet and everything. So a multi- aent search utility and now we have a new paper, we have a new method, we have some new research benchmark results and some new ideas. So what is the task that we are looking at? Let's say you're a researcher and you have your title and maybe you have already an idea from a technical abstract. Yeah. And then you want to ask AI to write here the related work section of your publication for you. In the old times we call this a literature review. Have a look who published what in your particular specific domain, when it was published, what were the topics, how was it related to your work. Now everything should be done by the eye. Now this new system is called the deep scholar benchmark and yes it promises to generate here the deep the related work section the deep research on internet on archive on meta archive for you. So what it needs three capabilities you are looking for the retrieval. So you want to have a vast corpus the live internet or the live archive whatever you have then you want to have a synthesis report. So everything all the data that were replied here from multiple sources weave them together the findings into a coherent long form narrative and then you want and this is the most important constraint absolute verifiability. So whatever you are claiming here and whatever you are citing those must really exist. So the citing sources must be accurate on every claim and must be back traced back to its origin. You must have references. Great. So the authors here Stanford and UC Berkeley built here an automated data pipeline directly and they said you know what we not just for the internet let's go here for science let's go to archive. So we want to have a fresh data set retrieved absolutely fresh here daily or every minute here from archive. So how we do this? So three topics the recency the quality control and diversity and here they also limited theirel to computer science as a topic and they made sure that they have a diverse top with 18 different domains in computer science. So yeah we are restricted to computer science but I think you will see the results they speak for themselves. Here's the paper published today August 27, 2025. Deep scholar bench live benchmark on automated evolation for generative research systems from Stanford University in UC Berkeley. And the beauty is not that they have now a new benchmark, but what the new benchmark shows us. It shows us the true difficulty AI systems have if they do have to do a research synthesis. Here's the result. I would say wait a minute explain here at first the metric and the benchmark methodology. Okay. So they decided to go here for a threedimensional differentiation to say we look for the knowledge synthesis of this new system for the retrieval quality. It's more or less a rack system. No. And then for the verifiability because we are working in science with archive. It must be really that we can site real sources and there's no hallucination. Great. And then you can see even in those three dimension they decided to go with two or three sub indicators. So let's have a look. So for the knowledge synthesis or what I call hey how well is this generated EI text written? Is it informative? They have two indicators. the organization where an LLM as a J performs here a pairwise comparison and then they have here a metric that assesses here the informationational completeness. So what you do you have now from your text here a human written um segment here and this is automatically if you want chucked up here by the eye so you get single sentences for example but a human like red at all introduced here the transformer architecture which relies here on a self attention mechanism now the eye might say hey this is a two long sentence there are too many objects too many subjects I reduce this I chunk it up here in nuggets so the first nugget is redford at introduce the transformer stop. This is it. So simple to verify and then it goes on and you see exactly what's happening. Now you have then this essential nuggets and you want to make sure that those really represent here in the I generated text what is the main content of the human written sentence or paragraph or paper. Then second dimension the retrieval quality. Simple in my words. Did the I system find the right papers on archive for my topic. Here we have three indicators. The relevance rate, the reference coverage or simply hey if the system found here the real important papers and you know importance is just one but we can also measure here how impactful this is with an impact um quote. So we have a document importance metric. So this is a proxy for retrieving impactful word that uses here the citation comes from a specific API. Great. And finally the most important for me we have to have verifiability. So whatever we claim there is a check against some other published work. So are the claims now actually supported here by the citation? And here they go with two indicators. the citation precision where they say hey does the cited source support at least one claim made in the eye sentence just one and then they are say hey let's go full power now now for the term claim coverage we want do the cited sources support all the claims made in the eye sentence so it's really every little detail that the eye writes out here supported here by let's say arrive references Great. Then they decided, you know, so we have now the idea of the benchmark, we have the metric and the subindicator of the benchmark. And they say, you know what, now we build a vehicle. So we can switch now the engine of this vehicle. So we can put in either a GPT 4.1 or we can put in a GPD4 Omni or we go with a clawed model as the engine. And they built here the deep scholar base. Now this was interesting because remember I told you this is your almost rag system. No they went here with a classical rag multi multi- aent structure. So at first you have the query generation. So an LLM generates now a set of diverse search query based on the human input. So whatever I have the AI the LLM tells me hey that was a little bit too complicated. I generate now a set of diver search queries and then I send this up. So those queries now AI generated queries now run against the search API in our example archive and then we have a filtering now a semantic filtering and now we use another LLM is used to filter here to retrieve documents and anything that is irrelevant we just want to ignore this no but we have still a lot of documents like say 200 documents left now after the filtering so we have to do a reranking because we want to go down to the top 20 no so we have another llm based step that now reranks in detail to the only 200 remaining documents and it's a new prioritization here. What are the most relevance only of these 200 and then if we have the top 20 we want to have a semantic aggregation of the content of all 20 papers. No. So we say now the final top 20 documents are fed now as a context to an LLM and again you can choose your LLM that you like with a detailed prompt and then synthesization and the LLM writes it the final related works action. So clearly structured data processing pipeline and you can switch on and off any LLM in any place that you want and they tested here variation of different LLMs and they say so what are now the best performance combination so you see pipeline performing research this is great they run this here with a GPD4 as the default and then they exchange and said hey what happens if I add here let's say an 03 or a cloud model to GPD4.1 one all the prompts for this single steps that you just noticed here you have here in detail in the annex here of the documentation of this new research paper like here I show you the revised ODS react agent prompt here for calling here a tool so a lot of additional data I just want to give you the result this is it again you see one two three dimension and then all the different sub indicator in this dimension. Yeah, I think we can forget about the opensource research system from 2014. 
Relevance score: -8.523

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: 85. So I give you here my summary. OpenI is really here the leader in crafting a well ststructured and seemingly comprehensive narrative. Open deep research really excels in this synthesis and in the retrieval task and we have here the numerical data for the highest organization score 85 and the highest nugget coverage 39. output are well organized and capture really the essential fact better than the competitors can perform. And for the retrieval quality also the winner here. If you look here at the relevance rate 62 covers the most important human-sided reference with a reference coins of8 and finds the most impactful paper.1 but and this is now a big but the performance of open research collapses in the third dimension and this is the most crucial aspect if you have some scientific writing trust and verifiability. If you just write something that is not verifiable. 
Relevance score: -10.524

----------------------------------------------------------------
-------------------- New Query at 2025-09-03T18:46:42.897882 -------------

        ## Task
        The user's query: How the methodology work for scientific research

        Answer the user's query based only on the retrieved context.
        Use chat history if relevant, but never go beyond the given context.
        Write a natural, context-based answer.


        ## Context
        Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Hello community so great that you are back. We have a new paper by Stanford University and UC Berkeley and it is all about science and a deep scholar AI system that is brand new. Now you know we have open-source research system no that do a research and synthesize known facts and new findings and you are familiar with this no for example here open scholar this is here the paper or if you go for another open source this is here storm or maybe you are familiar with deep researcher from April 2025 so we do have open systems but of course it's interesting how do they compare to the commercial system let's say openi deep research or we We have brand new search EIS where you remember MCP and here multi- aent system. So how do they compare here to multi- aent search EI systems in well let's say you do a research on the internet. Now for those we have now the study where they compare Llama forced to GPD 4.1 and 03 an Opus 4 by Claude and a Gemini 2.5 Pro. And if you ask me why not some other models, those are the set of models Stanford and UC Berkeley decided to perform their tests on. This is it. The framework they used here is here open deep search. The search with open-source reasoning agents and you remember we have here multi- aent system. We have an open reasoning agent here itself that interprets here the given human task and completes it here by orchestrating a sequence of action and one of those action might include here calling tools and one of those tools that is available to this agent is here the open search tool to connect to the internet and everything. So a multi- aent search utility and now we have a new paper, we have a new method, we have some new research benchmark results and some new ideas. So what is the task that we are looking at? Let's say you're a researcher and you have your title and maybe you have already an idea from a technical abstract. Yeah. And then you want to ask AI to write here the related work section of your publication for you. In the old times we call this a literature review. Have a look who published what in your particular specific domain, when it was published, what were the topics, how was it related to your work. Now everything should be done by the eye. Now this new system is called the deep scholar benchmark and yes it promises to generate here the deep the related work section the deep research on internet on archive on meta archive for you. So what it needs three capabilities you are looking for the retrieval. So you want to have a vast corpus the live internet or the live archive whatever you have then you want to have a synthesis report. So everything all the data that were replied here from multiple sources weave them together the findings into a coherent long form narrative and then you want and this is the most important constraint absolute verifiability. So whatever you are claiming here and whatever you are citing those must really exist. So the citing sources must be accurate on every claim and must be back traced back to its origin. You must have references. Great. So the authors here Stanford and UC Berkeley built here an automated data pipeline directly and they said you know what we not just for the internet let's go here for science let's go to archive. So we want to have a fresh data set retrieved absolutely fresh here daily or every minute here from archive. So how we do this? So three topics the recency the quality control and diversity and here they also limited theirel to computer science as a topic and they made sure that they have a diverse top with 18 different domains in computer science. So yeah we are restricted to computer science but I think you will see the results they speak for themselves. Here's the paper published today August 27, 2025. Deep scholar bench live benchmark on automated evolation for generative research systems from Stanford University in UC Berkeley. And the beauty is not that they have now a new benchmark, but what the new benchmark shows us. It shows us the true difficulty AI systems have if they do have to do a research synthesis. Here's the result. I would say wait a minute explain here at first the metric and the benchmark methodology. Okay. So they decided to go here for a threedimensional differentiation to say we look for the knowledge synthesis of this new system for the retrieval quality. It's more or less a rack system. No. And then for the verifiability because we are working in science with archive. It must be really that we can site real sources and there's no hallucination. Great. And then you can see even in those three dimension they decided to go with two or three sub indicators. So let's have a look. So for the knowledge synthesis or what I call hey how well is this generated EI text written? Is it informative? They have two indicators. the organization where an LLM as a J performs here a pairwise comparison and then they have here a metric that assesses here the informationational completeness. So what you do you have now from your text here a human written um segment here and this is automatically if you want chucked up here by the eye so you get single sentences for example but a human like red at all introduced here the transformer architecture which relies here on a self attention mechanism now the eye might say hey this is a two long sentence there are too many objects too many subjects I reduce this I chunk it up here in nuggets so the first nugget is redford at introduce the transformer stop. This is it. So simple to verify and then it goes on and you see exactly what's happening. Now you have then this essential nuggets and you want to make sure that those really represent here in the I generated text what is the main content of the human written sentence or paragraph or paper. Then second dimension the retrieval quality. Simple in my words. Did the I system find the right papers on archive for my topic. Here we have three indicators. The relevance rate, the reference coverage or simply hey if the system found here the real important papers and you know importance is just one but we can also measure here how impactful this is with an impact um quote. So we have a document importance metric. So this is a proxy for retrieving impactful word that uses here the citation comes from a specific API. Great. And finally the most important for me we have to have verifiability. So whatever we claim there is a check against some other published work. So are the claims now actually supported here by the citation? And here they go with two indicators. the citation precision where they say hey does the cited source support at least one claim made in the eye sentence just one and then they are say hey let's go full power now now for the term claim coverage we want do the cited sources support all the claims made in the eye sentence so it's really every little detail that the eye writes out here supported here by let's say arrive references Great. Then they decided, you know, so we have now the idea of the benchmark, we have the metric and the subindicator of the benchmark. And they say, you know what, now we build a vehicle. So we can switch now the engine of this vehicle. So we can put in either a GPT 4.1 or we can put in a GPD4 Omni or we go with a clawed model as the engine. And they built here the deep scholar base. Now this was interesting because remember I told you this is your almost rag system. No they went here with a classical rag multi multi- aent structure. So at first you have the query generation. So an LLM generates now a set of diverse search query based on the human input. So whatever I have the AI the LLM tells me hey that was a little bit too complicated. I generate now a set of diver search queries and then I send this up. So those queries now AI generated queries now run against the search API in our example archive and then we have a filtering now a semantic filtering and now we use another LLM is used to filter here to retrieve documents and anything that is irrelevant we just want to ignore this no but we have still a lot of documents like say 200 documents left now after the filtering so we have to do a reranking because we want to go down to the top 20 no so we have another llm based step that now reranks in detail to the only 200 remaining documents and it's a new prioritization here. What are the most relevance only of these 200 and then if we have the top 20 we want to have a semantic aggregation of the content of all 20 papers. No. So we say now the final top 20 documents are fed now as a context to an LLM and again you can choose your LLM that you like with a detailed prompt and then synthesization and the LLM writes it the final related works action. So clearly structured data processing pipeline and you can switch on and off any LLM in any place that you want and they tested here variation of different LLMs and they say so what are now the best performance combination so you see pipeline performing research this is great they run this here with a GPD4 as the default and then they exchange and said hey what happens if I add here let's say an 03 or a cloud model to GPD4.1 one all the prompts for this single steps that you just noticed here you have here in detail in the annex here of the documentation of this new research paper like here I show you the revised ODS react agent prompt here for calling here a tool so a lot of additional data I just want to give you the result this is it again you see one two three dimension and then all the different sub indicator in this dimension. Yeah, I think we can forget about the opensource research system from 2014. 
Relevance score: -8.566

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Great. So verifiability here open research scores shockingly low. This is what Stanford and UC Berkeler tells us. Citation precision numerical value is only 39. The claim coverage is only.13. And they tell us this is one of the worst performers here. that why is this happening? They suggest here a system optimized for fluent plausible sounding text is great on the one hand. So it's excellent in synthesizing here story or story line. But if you work in science, if you want to be absolute precise, you want to reduce hallucinating to the max and you want to make sure making logical leaps that are not strictly supported by the cited sources of archive are not part of your document. Now unfortunately this is happening here with open deep research. So for purely scientific work where you have to have absolute precision this could turn out to be a fatal flaw. Now, interestingly, if we look here at the new matter, the deep scholar base as I just showed you and we put here an 03 as the engine into this benchmark, you see, interestingly for the knowledge synthesis, we have the exact same benchmark 857.857. If we look here at the nugget, we even have a better performance here at the sequential tokenization if you want. And and this is now interesting with this new method. Look at the variability from one we jump to 6. So yeah, wow. This is an impressive improvement against the commercial openi deep research performance. So this new method by Stanford and UC Berkeley even if you use it with an openi model you see in one of the indicators they are at least as good as the commercial models but they outperform the commercial model significantly in variability. Isn't this beautiful? And if you ask yourself hey why I have suddenly two LLMs I told you you can have multiple LLMs here in the pipeline. So they decided for the SAM filter and the SAM top K data they go with GBD 4.1 but for the heavyduty SAM the aggregation step they go here with an 03 or a Claude or a Gemini just to make sure we on the same page. So this is the process. So we have the filter process then the top K the top 20 papers then the final aggregation the final section generation and I told you I show you always the prompt and here you have the prompt. So whatever LLM you choose an 03 or a claude OPUS 4 or a Gemini 2.5 Pro this is here the prompt for the aggregation for the final aggregation here of your research. So now we can see the paper here highlights here the deep scholar base can achieve up to 6.3 times higher claim coverage. So one indicator and one dimension six times three better than openi deep research. Nice. So we do have new systems that really outperform the classical commercial blackbox systems. But on the other hand, if you want just a plausible, well-written draft and you are not in science and you are willing, if you are in science, to meticulously fact check every single claim, every single half sentence that EI writes for you, then if you as a human do this verification and correct it, then open EI is really maybe also interesting. However, if you are a scientist and you say, I will go with the AI system that I can trust at least partially, then the deep scholar base here with claude or gemini model as the engine is dramatically superior. If you look at the data from Stanford and UC Berkeley, the paper states also that no method achieves a score greater than.19 when you average across the metric. So this is not an incremental challenge. You know, go from 78% to 82%. No, we at 19%. This is the best model we have. So we have quite a monumental challenge in front of us to make our AI models better, more performance, more power. So there's a lot of work in front of us. And if you say okay, I don't like here the numerical values in a table. I want to see this here in a visualization. Here you have exactly all the different elements here in um yeah to radar visualization. But this is exactly the data here from the numerical table I just showed you. But if you prefer to have it here in the radar, beautiful. Here is it. 
Relevance score: -8.744

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: 85. So I give you here my summary. OpenI is really here the leader in crafting a well ststructured and seemingly comprehensive narrative. Open deep research really excels in this synthesis and in the retrieval task and we have here the numerical data for the highest organization score 85 and the highest nugget coverage 39. output are well organized and capture really the essential fact better than the competitors can perform. And for the retrieval quality also the winner here. If you look here at the relevance rate 62 covers the most important human-sided reference with a reference coins of8 and finds the most impactful paper.1 but and this is now a big but the performance of open research collapses in the third dimension and this is the most crucial aspect if you have some scientific writing trust and verifiability. If you just write something that is not verifiable. 
Relevance score: -9.688

----------------------------------------------------------------
-------------------- New Query at 2025-09-03T19:22:29.440728 -------------

        ## Task
        The user's query: what is deepscholar ai

        Answer the user's query based only on the retrieved context.
        Use chat history if relevant, but never go beyond the given context.
        Write a natural, context-based answer.


        ## Context
        Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Hello community so great that you are back. We have a new paper by Stanford University and UC Berkeley and it is all about science and a deep scholar AI system that is brand new. Now you know we have open-source research system no that do a research and synthesize known facts and new findings and you are familiar with this no for example here open scholar this is here the paper or if you go for another open source this is here storm or maybe you are familiar with deep researcher from April 2025 so we do have open systems but of course it's interesting how do they compare to the commercial system let's say openi deep research or we We have brand new search EIS where you remember MCP and here multi- aent system. So how do they compare here to multi- aent search EI systems in well let's say you do a research on the internet. Now for those we have now the study where they compare Llama forced to GPD 4.1 and 03 an Opus 4 by Claude and a Gemini 2.5 Pro. And if you ask me why not some other models, those are the set of models Stanford and UC Berkeley decided to perform their tests on. This is it. The framework they used here is here open deep search. The search with open-source reasoning agents and you remember we have here multi- aent system. We have an open reasoning agent here itself that interprets here the given human task and completes it here by orchestrating a sequence of action and one of those action might include here calling tools and one of those tools that is available to this agent is here the open search tool to connect to the internet and everything. So a multi- aent search utility and now we have a new paper, we have a new method, we have some new research benchmark results and some new ideas. So what is the task that we are looking at? Let's say you're a researcher and you have your title and maybe you have already an idea from a technical abstract. Yeah. And then you want to ask AI to write here the related work section of your publication for you. In the old times we call this a literature review. Have a look who published what in your particular specific domain, when it was published, what were the topics, how was it related to your work. Now everything should be done by the eye. Now this new system is called the deep scholar benchmark and yes it promises to generate here the deep the related work section the deep research on internet on archive on meta archive for you. So what it needs three capabilities you are looking for the retrieval. So you want to have a vast corpus the live internet or the live archive whatever you have then you want to have a synthesis report. So everything all the data that were replied here from multiple sources weave them together the findings into a coherent long form narrative and then you want and this is the most important constraint absolute verifiability. So whatever you are claiming here and whatever you are citing those must really exist. So the citing sources must be accurate on every claim and must be back traced back to its origin. You must have references. Great. So the authors here Stanford and UC Berkeley built here an automated data pipeline directly and they said you know what we not just for the internet let's go here for science let's go to archive. So we want to have a fresh data set retrieved absolutely fresh here daily or every minute here from archive. So how we do this? So three topics the recency the quality control and diversity and here they also limited theirel to computer science as a topic and they made sure that they have a diverse top with 18 different domains in computer science. So yeah we are restricted to computer science but I think you will see the results they speak for themselves. Here's the paper published today August 27, 2025. Deep scholar bench live benchmark on automated evolation for generative research systems from Stanford University in UC Berkeley. And the beauty is not that they have now a new benchmark, but what the new benchmark shows us. It shows us the true difficulty AI systems have if they do have to do a research synthesis. Here's the result. I would say wait a minute explain here at first the metric and the benchmark methodology. Okay. So they decided to go here for a threedimensional differentiation to say we look for the knowledge synthesis of this new system for the retrieval quality. It's more or less a rack system. No. And then for the verifiability because we are working in science with archive. It must be really that we can site real sources and there's no hallucination. Great. And then you can see even in those three dimension they decided to go with two or three sub indicators. So let's have a look. So for the knowledge synthesis or what I call hey how well is this generated EI text written? Is it informative? They have two indicators. the organization where an LLM as a J performs here a pairwise comparison and then they have here a metric that assesses here the informationational completeness. So what you do you have now from your text here a human written um segment here and this is automatically if you want chucked up here by the eye so you get single sentences for example but a human like red at all introduced here the transformer architecture which relies here on a self attention mechanism now the eye might say hey this is a two long sentence there are too many objects too many subjects I reduce this I chunk it up here in nuggets so the first nugget is redford at introduce the transformer stop. This is it. So simple to verify and then it goes on and you see exactly what's happening. Now you have then this essential nuggets and you want to make sure that those really represent here in the I generated text what is the main content of the human written sentence or paragraph or paper. Then second dimension the retrieval quality. Simple in my words. Did the I system find the right papers on archive for my topic. Here we have three indicators. The relevance rate, the reference coverage or simply hey if the system found here the real important papers and you know importance is just one but we can also measure here how impactful this is with an impact um quote. So we have a document importance metric. So this is a proxy for retrieving impactful word that uses here the citation comes from a specific API. Great. And finally the most important for me we have to have verifiability. So whatever we claim there is a check against some other published work. So are the claims now actually supported here by the citation? And here they go with two indicators. the citation precision where they say hey does the cited source support at least one claim made in the eye sentence just one and then they are say hey let's go full power now now for the term claim coverage we want do the cited sources support all the claims made in the eye sentence so it's really every little detail that the eye writes out here supported here by let's say arrive references Great. Then they decided, you know, so we have now the idea of the benchmark, we have the metric and the subindicator of the benchmark. And they say, you know what, now we build a vehicle. So we can switch now the engine of this vehicle. So we can put in either a GPT 4.1 or we can put in a GPD4 Omni or we go with a clawed model as the engine. And they built here the deep scholar base. Now this was interesting because remember I told you this is your almost rag system. No they went here with a classical rag multi multi- aent structure. So at first you have the query generation. So an LLM generates now a set of diverse search query based on the human input. So whatever I have the AI the LLM tells me hey that was a little bit too complicated. I generate now a set of diver search queries and then I send this up. So those queries now AI generated queries now run against the search API in our example archive and then we have a filtering now a semantic filtering and now we use another LLM is used to filter here to retrieve documents and anything that is irrelevant we just want to ignore this no but we have still a lot of documents like say 200 documents left now after the filtering so we have to do a reranking because we want to go down to the top 20 no so we have another llm based step that now reranks in detail to the only 200 remaining documents and it's a new prioritization here. What are the most relevance only of these 200 and then if we have the top 20 we want to have a semantic aggregation of the content of all 20 papers. No. So we say now the final top 20 documents are fed now as a context to an LLM and again you can choose your LLM that you like with a detailed prompt and then synthesization and the LLM writes it the final related works action. So clearly structured data processing pipeline and you can switch on and off any LLM in any place that you want and they tested here variation of different LLMs and they say so what are now the best performance combination so you see pipeline performing research this is great they run this here with a GPD4 as the default and then they exchange and said hey what happens if I add here let's say an 03 or a cloud model to GPD4.1 one all the prompts for this single steps that you just noticed here you have here in detail in the annex here of the documentation of this new research paper like here I show you the revised ODS react agent prompt here for calling here a tool so a lot of additional data I just want to give you the result this is it again you see one two three dimension and then all the different sub indicator in this dimension. Yeah, I think we can forget about the opensource research system from 2014. 
Relevance score: -10.120

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Great. So verifiability here open research scores shockingly low. This is what Stanford and UC Berkeler tells us. Citation precision numerical value is only 39. The claim coverage is only.13. And they tell us this is one of the worst performers here. that why is this happening? They suggest here a system optimized for fluent plausible sounding text is great on the one hand. So it's excellent in synthesizing here story or story line. But if you work in science, if you want to be absolute precise, you want to reduce hallucinating to the max and you want to make sure making logical leaps that are not strictly supported by the cited sources of archive are not part of your document. Now unfortunately this is happening here with open deep research. So for purely scientific work where you have to have absolute precision this could turn out to be a fatal flaw. Now, interestingly, if we look here at the new matter, the deep scholar base as I just showed you and we put here an 03 as the engine into this benchmark, you see, interestingly for the knowledge synthesis, we have the exact same benchmark 857.857. If we look here at the nugget, we even have a better performance here at the sequential tokenization if you want. And and this is now interesting with this new method. Look at the variability from one we jump to 6. So yeah, wow. This is an impressive improvement against the commercial openi deep research performance. So this new method by Stanford and UC Berkeley even if you use it with an openi model you see in one of the indicators they are at least as good as the commercial models but they outperform the commercial model significantly in variability. Isn't this beautiful? And if you ask yourself hey why I have suddenly two LLMs I told you you can have multiple LLMs here in the pipeline. So they decided for the SAM filter and the SAM top K data they go with GBD 4.1 but for the heavyduty SAM the aggregation step they go here with an 03 or a Claude or a Gemini just to make sure we on the same page. So this is the process. So we have the filter process then the top K the top 20 papers then the final aggregation the final section generation and I told you I show you always the prompt and here you have the prompt. So whatever LLM you choose an 03 or a claude OPUS 4 or a Gemini 2.5 Pro this is here the prompt for the aggregation for the final aggregation here of your research. So now we can see the paper here highlights here the deep scholar base can achieve up to 6.3 times higher claim coverage. So one indicator and one dimension six times three better than openi deep research. Nice. So we do have new systems that really outperform the classical commercial blackbox systems. But on the other hand, if you want just a plausible, well-written draft and you are not in science and you are willing, if you are in science, to meticulously fact check every single claim, every single half sentence that EI writes for you, then if you as a human do this verification and correct it, then open EI is really maybe also interesting. However, if you are a scientist and you say, I will go with the AI system that I can trust at least partially, then the deep scholar base here with claude or gemini model as the engine is dramatically superior. If you look at the data from Stanford and UC Berkeley, the paper states also that no method achieves a score greater than.19 when you average across the metric. So this is not an incremental challenge. You know, go from 78% to 82%. No, we at 19%. This is the best model we have. So we have quite a monumental challenge in front of us to make our AI models better, more performance, more power. So there's a lot of work in front of us. And if you say okay, I don't like here the numerical values in a table. I want to see this here in a visualization. Here you have exactly all the different elements here in um yeah to radar visualization. But this is exactly the data here from the numerical table I just showed you. But if you prefer to have it here in the radar, beautiful. Here is it. 
Relevance score: -10.633

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: And there we have it. A new research paper by Stanford University UC Berkeley about AI doing here the research the literature review here on brand new topics live in a multi- aent network deep scholar to research and synthesize knowledge. And if you are in science this could be a real benefit, a real help. 
Relevance score: -10.675

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: 85. So I give you here my summary. OpenI is really here the leader in crafting a well ststructured and seemingly comprehensive narrative. Open deep research really excels in this synthesis and in the retrieval task and we have here the numerical data for the highest organization score 85 and the highest nugget coverage 39. output are well organized and capture really the essential fact better than the competitors can perform. And for the retrieval quality also the winner here. If you look here at the relevance rate 62 covers the most important human-sided reference with a reference coins of8 and finds the most impactful paper.1 but and this is now a big but the performance of open research collapses in the third dimension and this is the most crucial aspect if you have some scientific writing trust and verifiability. If you just write something that is not verifiable. 
Relevance score: -10.857

----------------------------------------------------------------
-------------------- New Query at 2025-09-03T19:23:59.115955 -------------

        ## Task
        The user's query: what is limitation of the benchmark

        Answer the user's query based only on the retrieved context.
        Use chat history if relevant, but never go beyond the given context.
        Write a natural, context-based answer.


        ## Context
        Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Great. So verifiability here open research scores shockingly low. This is what Stanford and UC Berkeler tells us. Citation precision numerical value is only 39. The claim coverage is only.13. And they tell us this is one of the worst performers here. that why is this happening? They suggest here a system optimized for fluent plausible sounding text is great on the one hand. So it's excellent in synthesizing here story or story line. But if you work in science, if you want to be absolute precise, you want to reduce hallucinating to the max and you want to make sure making logical leaps that are not strictly supported by the cited sources of archive are not part of your document. Now unfortunately this is happening here with open deep research. So for purely scientific work where you have to have absolute precision this could turn out to be a fatal flaw. Now, interestingly, if we look here at the new matter, the deep scholar base as I just showed you and we put here an 03 as the engine into this benchmark, you see, interestingly for the knowledge synthesis, we have the exact same benchmark 857.857. If we look here at the nugget, we even have a better performance here at the sequential tokenization if you want. And and this is now interesting with this new method. Look at the variability from one we jump to 6. So yeah, wow. This is an impressive improvement against the commercial openi deep research performance. So this new method by Stanford and UC Berkeley even if you use it with an openi model you see in one of the indicators they are at least as good as the commercial models but they outperform the commercial model significantly in variability. Isn't this beautiful? And if you ask yourself hey why I have suddenly two LLMs I told you you can have multiple LLMs here in the pipeline. So they decided for the SAM filter and the SAM top K data they go with GBD 4.1 but for the heavyduty SAM the aggregation step they go here with an 03 or a Claude or a Gemini just to make sure we on the same page. So this is the process. So we have the filter process then the top K the top 20 papers then the final aggregation the final section generation and I told you I show you always the prompt and here you have the prompt. So whatever LLM you choose an 03 or a claude OPUS 4 or a Gemini 2.5 Pro this is here the prompt for the aggregation for the final aggregation here of your research. So now we can see the paper here highlights here the deep scholar base can achieve up to 6.3 times higher claim coverage. So one indicator and one dimension six times three better than openi deep research. Nice. So we do have new systems that really outperform the classical commercial blackbox systems. But on the other hand, if you want just a plausible, well-written draft and you are not in science and you are willing, if you are in science, to meticulously fact check every single claim, every single half sentence that EI writes for you, then if you as a human do this verification and correct it, then open EI is really maybe also interesting. However, if you are a scientist and you say, I will go with the AI system that I can trust at least partially, then the deep scholar base here with claude or gemini model as the engine is dramatically superior. If you look at the data from Stanford and UC Berkeley, the paper states also that no method achieves a score greater than.19 when you average across the metric. So this is not an incremental challenge. You know, go from 78% to 82%. No, we at 19%. This is the best model we have. So we have quite a monumental challenge in front of us to make our AI models better, more performance, more power. So there's a lot of work in front of us. And if you say okay, I don't like here the numerical values in a table. I want to see this here in a visualization. Here you have exactly all the different elements here in um yeah to radar visualization. But this is exactly the data here from the numerical table I just showed you. But if you prefer to have it here in the radar, beautiful. Here is it. 
Relevance score: -10.525

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Hello community so great that you are back. We have a new paper by Stanford University and UC Berkeley and it is all about science and a deep scholar AI system that is brand new. Now you know we have open-source research system no that do a research and synthesize known facts and new findings and you are familiar with this no for example here open scholar this is here the paper or if you go for another open source this is here storm or maybe you are familiar with deep researcher from April 2025 so we do have open systems but of course it's interesting how do they compare to the commercial system let's say openi deep research or we We have brand new search EIS where you remember MCP and here multi- aent system. So how do they compare here to multi- aent search EI systems in well let's say you do a research on the internet. Now for those we have now the study where they compare Llama forced to GPD 4.1 and 03 an Opus 4 by Claude and a Gemini 2.5 Pro. And if you ask me why not some other models, those are the set of models Stanford and UC Berkeley decided to perform their tests on. This is it. The framework they used here is here open deep search. The search with open-source reasoning agents and you remember we have here multi- aent system. We have an open reasoning agent here itself that interprets here the given human task and completes it here by orchestrating a sequence of action and one of those action might include here calling tools and one of those tools that is available to this agent is here the open search tool to connect to the internet and everything. So a multi- aent search utility and now we have a new paper, we have a new method, we have some new research benchmark results and some new ideas. So what is the task that we are looking at? Let's say you're a researcher and you have your title and maybe you have already an idea from a technical abstract. Yeah. And then you want to ask AI to write here the related work section of your publication for you. In the old times we call this a literature review. Have a look who published what in your particular specific domain, when it was published, what were the topics, how was it related to your work. Now everything should be done by the eye. Now this new system is called the deep scholar benchmark and yes it promises to generate here the deep the related work section the deep research on internet on archive on meta archive for you. So what it needs three capabilities you are looking for the retrieval. So you want to have a vast corpus the live internet or the live archive whatever you have then you want to have a synthesis report. So everything all the data that were replied here from multiple sources weave them together the findings into a coherent long form narrative and then you want and this is the most important constraint absolute verifiability. So whatever you are claiming here and whatever you are citing those must really exist. So the citing sources must be accurate on every claim and must be back traced back to its origin. You must have references. Great. So the authors here Stanford and UC Berkeley built here an automated data pipeline directly and they said you know what we not just for the internet let's go here for science let's go to archive. So we want to have a fresh data set retrieved absolutely fresh here daily or every minute here from archive. So how we do this? So three topics the recency the quality control and diversity and here they also limited theirel to computer science as a topic and they made sure that they have a diverse top with 18 different domains in computer science. So yeah we are restricted to computer science but I think you will see the results they speak for themselves. Here's the paper published today August 27, 2025. Deep scholar bench live benchmark on automated evolation for generative research systems from Stanford University in UC Berkeley. And the beauty is not that they have now a new benchmark, but what the new benchmark shows us. It shows us the true difficulty AI systems have if they do have to do a research synthesis. Here's the result. I would say wait a minute explain here at first the metric and the benchmark methodology. Okay. So they decided to go here for a threedimensional differentiation to say we look for the knowledge synthesis of this new system for the retrieval quality. It's more or less a rack system. No. And then for the verifiability because we are working in science with archive. It must be really that we can site real sources and there's no hallucination. Great. And then you can see even in those three dimension they decided to go with two or three sub indicators. So let's have a look. So for the knowledge synthesis or what I call hey how well is this generated EI text written? Is it informative? They have two indicators. the organization where an LLM as a J performs here a pairwise comparison and then they have here a metric that assesses here the informationational completeness. So what you do you have now from your text here a human written um segment here and this is automatically if you want chucked up here by the eye so you get single sentences for example but a human like red at all introduced here the transformer architecture which relies here on a self attention mechanism now the eye might say hey this is a two long sentence there are too many objects too many subjects I reduce this I chunk it up here in nuggets so the first nugget is redford at introduce the transformer stop. This is it. So simple to verify and then it goes on and you see exactly what's happening. Now you have then this essential nuggets and you want to make sure that those really represent here in the I generated text what is the main content of the human written sentence or paragraph or paper. Then second dimension the retrieval quality. Simple in my words. Did the I system find the right papers on archive for my topic. Here we have three indicators. The relevance rate, the reference coverage or simply hey if the system found here the real important papers and you know importance is just one but we can also measure here how impactful this is with an impact um quote. So we have a document importance metric. So this is a proxy for retrieving impactful word that uses here the citation comes from a specific API. Great. And finally the most important for me we have to have verifiability. So whatever we claim there is a check against some other published work. So are the claims now actually supported here by the citation? And here they go with two indicators. the citation precision where they say hey does the cited source support at least one claim made in the eye sentence just one and then they are say hey let's go full power now now for the term claim coverage we want do the cited sources support all the claims made in the eye sentence so it's really every little detail that the eye writes out here supported here by let's say arrive references Great. Then they decided, you know, so we have now the idea of the benchmark, we have the metric and the subindicator of the benchmark. And they say, you know what, now we build a vehicle. So we can switch now the engine of this vehicle. So we can put in either a GPT 4.1 or we can put in a GPD4 Omni or we go with a clawed model as the engine. And they built here the deep scholar base. Now this was interesting because remember I told you this is your almost rag system. No they went here with a classical rag multi multi- aent structure. So at first you have the query generation. So an LLM generates now a set of diverse search query based on the human input. So whatever I have the AI the LLM tells me hey that was a little bit too complicated. I generate now a set of diver search queries and then I send this up. So those queries now AI generated queries now run against the search API in our example archive and then we have a filtering now a semantic filtering and now we use another LLM is used to filter here to retrieve documents and anything that is irrelevant we just want to ignore this no but we have still a lot of documents like say 200 documents left now after the filtering so we have to do a reranking because we want to go down to the top 20 no so we have another llm based step that now reranks in detail to the only 200 remaining documents and it's a new prioritization here. What are the most relevance only of these 200 and then if we have the top 20 we want to have a semantic aggregation of the content of all 20 papers. No. So we say now the final top 20 documents are fed now as a context to an LLM and again you can choose your LLM that you like with a detailed prompt and then synthesization and the LLM writes it the final related works action. So clearly structured data processing pipeline and you can switch on and off any LLM in any place that you want and they tested here variation of different LLMs and they say so what are now the best performance combination so you see pipeline performing research this is great they run this here with a GPD4 as the default and then they exchange and said hey what happens if I add here let's say an 03 or a cloud model to GPD4.1 one all the prompts for this single steps that you just noticed here you have here in detail in the annex here of the documentation of this new research paper like here I show you the revised ODS react agent prompt here for calling here a tool so a lot of additional data I just want to give you the result this is it again you see one two three dimension and then all the different sub indicator in this dimension. Yeah, I think we can forget about the opensource research system from 2014. 
Relevance score: -10.910

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: 85. So I give you here my summary. OpenI is really here the leader in crafting a well ststructured and seemingly comprehensive narrative. Open deep research really excels in this synthesis and in the retrieval task and we have here the numerical data for the highest organization score 85 and the highest nugget coverage 39. output are well organized and capture really the essential fact better than the competitors can perform. And for the retrieval quality also the winner here. If you look here at the relevance rate 62 covers the most important human-sided reference with a reference coins of8 and finds the most impactful paper.1 but and this is now a big but the performance of open research collapses in the third dimension and this is the most crucial aspect if you have some scientific writing trust and verifiability. If you just write something that is not verifiable. 
Relevance score: -11.353

----------------------------------------------------------------
-------------------- New Query at 2025-09-03T19:25:03.217117 -------------

        ## Task
        The user's query: tell the methodology

        Answer the user's query based only on the retrieved context.
        Use chat history if relevant, but never go beyond the given context.
        Write a natural, context-based answer.


        ## Context
        Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Great. So verifiability here open research scores shockingly low. This is what Stanford and UC Berkeler tells us. Citation precision numerical value is only 39. The claim coverage is only.13. And they tell us this is one of the worst performers here. that why is this happening? They suggest here a system optimized for fluent plausible sounding text is great on the one hand. So it's excellent in synthesizing here story or story line. But if you work in science, if you want to be absolute precise, you want to reduce hallucinating to the max and you want to make sure making logical leaps that are not strictly supported by the cited sources of archive are not part of your document. Now unfortunately this is happening here with open deep research. So for purely scientific work where you have to have absolute precision this could turn out to be a fatal flaw. Now, interestingly, if we look here at the new matter, the deep scholar base as I just showed you and we put here an 03 as the engine into this benchmark, you see, interestingly for the knowledge synthesis, we have the exact same benchmark 857.857. If we look here at the nugget, we even have a better performance here at the sequential tokenization if you want. And and this is now interesting with this new method. Look at the variability from one we jump to 6. So yeah, wow. This is an impressive improvement against the commercial openi deep research performance. So this new method by Stanford and UC Berkeley even if you use it with an openi model you see in one of the indicators they are at least as good as the commercial models but they outperform the commercial model significantly in variability. Isn't this beautiful? And if you ask yourself hey why I have suddenly two LLMs I told you you can have multiple LLMs here in the pipeline. So they decided for the SAM filter and the SAM top K data they go with GBD 4.1 but for the heavyduty SAM the aggregation step they go here with an 03 or a Claude or a Gemini just to make sure we on the same page. So this is the process. So we have the filter process then the top K the top 20 papers then the final aggregation the final section generation and I told you I show you always the prompt and here you have the prompt. So whatever LLM you choose an 03 or a claude OPUS 4 or a Gemini 2.5 Pro this is here the prompt for the aggregation for the final aggregation here of your research. So now we can see the paper here highlights here the deep scholar base can achieve up to 6.3 times higher claim coverage. So one indicator and one dimension six times three better than openi deep research. Nice. So we do have new systems that really outperform the classical commercial blackbox systems. But on the other hand, if you want just a plausible, well-written draft and you are not in science and you are willing, if you are in science, to meticulously fact check every single claim, every single half sentence that EI writes for you, then if you as a human do this verification and correct it, then open EI is really maybe also interesting. However, if you are a scientist and you say, I will go with the AI system that I can trust at least partially, then the deep scholar base here with claude or gemini model as the engine is dramatically superior. If you look at the data from Stanford and UC Berkeley, the paper states also that no method achieves a score greater than.19 when you average across the metric. So this is not an incremental challenge. You know, go from 78% to 82%. No, we at 19%. This is the best model we have. So we have quite a monumental challenge in front of us to make our AI models better, more performance, more power. So there's a lot of work in front of us. And if you say okay, I don't like here the numerical values in a table. I want to see this here in a visualization. Here you have exactly all the different elements here in um yeah to radar visualization. But this is exactly the data here from the numerical table I just showed you. But if you prefer to have it here in the radar, beautiful. Here is it. 
Relevance score: -7.274

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Hello community so great that you are back. We have a new paper by Stanford University and UC Berkeley and it is all about science and a deep scholar AI system that is brand new. Now you know we have open-source research system no that do a research and synthesize known facts and new findings and you are familiar with this no for example here open scholar this is here the paper or if you go for another open source this is here storm or maybe you are familiar with deep researcher from April 2025 so we do have open systems but of course it's interesting how do they compare to the commercial system let's say openi deep research or we We have brand new search EIS where you remember MCP and here multi- aent system. So how do they compare here to multi- aent search EI systems in well let's say you do a research on the internet. Now for those we have now the study where they compare Llama forced to GPD 4.1 and 03 an Opus 4 by Claude and a Gemini 2.5 Pro. And if you ask me why not some other models, those are the set of models Stanford and UC Berkeley decided to perform their tests on. This is it. The framework they used here is here open deep search. The search with open-source reasoning agents and you remember we have here multi- aent system. We have an open reasoning agent here itself that interprets here the given human task and completes it here by orchestrating a sequence of action and one of those action might include here calling tools and one of those tools that is available to this agent is here the open search tool to connect to the internet and everything. So a multi- aent search utility and now we have a new paper, we have a new method, we have some new research benchmark results and some new ideas. So what is the task that we are looking at? Let's say you're a researcher and you have your title and maybe you have already an idea from a technical abstract. Yeah. And then you want to ask AI to write here the related work section of your publication for you. In the old times we call this a literature review. Have a look who published what in your particular specific domain, when it was published, what were the topics, how was it related to your work. Now everything should be done by the eye. Now this new system is called the deep scholar benchmark and yes it promises to generate here the deep the related work section the deep research on internet on archive on meta archive for you. So what it needs three capabilities you are looking for the retrieval. So you want to have a vast corpus the live internet or the live archive whatever you have then you want to have a synthesis report. So everything all the data that were replied here from multiple sources weave them together the findings into a coherent long form narrative and then you want and this is the most important constraint absolute verifiability. So whatever you are claiming here and whatever you are citing those must really exist. So the citing sources must be accurate on every claim and must be back traced back to its origin. You must have references. Great. So the authors here Stanford and UC Berkeley built here an automated data pipeline directly and they said you know what we not just for the internet let's go here for science let's go to archive. So we want to have a fresh data set retrieved absolutely fresh here daily or every minute here from archive. So how we do this? So three topics the recency the quality control and diversity and here they also limited theirel to computer science as a topic and they made sure that they have a diverse top with 18 different domains in computer science. So yeah we are restricted to computer science but I think you will see the results they speak for themselves. Here's the paper published today August 27, 2025. Deep scholar bench live benchmark on automated evolation for generative research systems from Stanford University in UC Berkeley. And the beauty is not that they have now a new benchmark, but what the new benchmark shows us. It shows us the true difficulty AI systems have if they do have to do a research synthesis. Here's the result. I would say wait a minute explain here at first the metric and the benchmark methodology. Okay. So they decided to go here for a threedimensional differentiation to say we look for the knowledge synthesis of this new system for the retrieval quality. It's more or less a rack system. No. And then for the verifiability because we are working in science with archive. It must be really that we can site real sources and there's no hallucination. Great. And then you can see even in those three dimension they decided to go with two or three sub indicators. So let's have a look. So for the knowledge synthesis or what I call hey how well is this generated EI text written? Is it informative? They have two indicators. the organization where an LLM as a J performs here a pairwise comparison and then they have here a metric that assesses here the informationational completeness. So what you do you have now from your text here a human written um segment here and this is automatically if you want chucked up here by the eye so you get single sentences for example but a human like red at all introduced here the transformer architecture which relies here on a self attention mechanism now the eye might say hey this is a two long sentence there are too many objects too many subjects I reduce this I chunk it up here in nuggets so the first nugget is redford at introduce the transformer stop. This is it. So simple to verify and then it goes on and you see exactly what's happening. Now you have then this essential nuggets and you want to make sure that those really represent here in the I generated text what is the main content of the human written sentence or paragraph or paper. Then second dimension the retrieval quality. Simple in my words. Did the I system find the right papers on archive for my topic. Here we have three indicators. The relevance rate, the reference coverage or simply hey if the system found here the real important papers and you know importance is just one but we can also measure here how impactful this is with an impact um quote. So we have a document importance metric. So this is a proxy for retrieving impactful word that uses here the citation comes from a specific API. Great. And finally the most important for me we have to have verifiability. So whatever we claim there is a check against some other published work. So are the claims now actually supported here by the citation? And here they go with two indicators. the citation precision where they say hey does the cited source support at least one claim made in the eye sentence just one and then they are say hey let's go full power now now for the term claim coverage we want do the cited sources support all the claims made in the eye sentence so it's really every little detail that the eye writes out here supported here by let's say arrive references Great. Then they decided, you know, so we have now the idea of the benchmark, we have the metric and the subindicator of the benchmark. And they say, you know what, now we build a vehicle. So we can switch now the engine of this vehicle. So we can put in either a GPT 4.1 or we can put in a GPD4 Omni or we go with a clawed model as the engine. And they built here the deep scholar base. Now this was interesting because remember I told you this is your almost rag system. No they went here with a classical rag multi multi- aent structure. So at first you have the query generation. So an LLM generates now a set of diverse search query based on the human input. So whatever I have the AI the LLM tells me hey that was a little bit too complicated. I generate now a set of diver search queries and then I send this up. So those queries now AI generated queries now run against the search API in our example archive and then we have a filtering now a semantic filtering and now we use another LLM is used to filter here to retrieve documents and anything that is irrelevant we just want to ignore this no but we have still a lot of documents like say 200 documents left now after the filtering so we have to do a reranking because we want to go down to the top 20 no so we have another llm based step that now reranks in detail to the only 200 remaining documents and it's a new prioritization here. What are the most relevance only of these 200 and then if we have the top 20 we want to have a semantic aggregation of the content of all 20 papers. No. So we say now the final top 20 documents are fed now as a context to an LLM and again you can choose your LLM that you like with a detailed prompt and then synthesization and the LLM writes it the final related works action. So clearly structured data processing pipeline and you can switch on and off any LLM in any place that you want and they tested here variation of different LLMs and they say so what are now the best performance combination so you see pipeline performing research this is great they run this here with a GPD4 as the default and then they exchange and said hey what happens if I add here let's say an 03 or a cloud model to GPD4.1 one all the prompts for this single steps that you just noticed here you have here in detail in the annex here of the documentation of this new research paper like here I show you the revised ODS react agent prompt here for calling here a tool so a lot of additional data I just want to give you the result this is it again you see one two three dimension and then all the different sub indicator in this dimension. Yeah, I think we can forget about the opensource research system from 2014. 
Relevance score: -8.125

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: 85. So I give you here my summary. OpenI is really here the leader in crafting a well ststructured and seemingly comprehensive narrative. Open deep research really excels in this synthesis and in the retrieval task and we have here the numerical data for the highest organization score 85 and the highest nugget coverage 39. output are well organized and capture really the essential fact better than the competitors can perform. And for the retrieval quality also the winner here. If you look here at the relevance rate 62 covers the most important human-sided reference with a reference coins of8 and finds the most impactful paper.1 but and this is now a big but the performance of open research collapses in the third dimension and this is the most crucial aspect if you have some scientific writing trust and verifiability. If you just write something that is not verifiable. 
Relevance score: -9.950

----------------------------------------------------------------
-------------------- New Query at 2025-09-03T22:14:47.256178 -------------

        ## Task
        The user's query: how deepscholar works?

        Answer the user's query based only on the retrieved context.
        Use chat history if relevant, but never go beyond the given context.
        Write a natural, context-based answer.


        ## Context
        Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Great. So verifiability here open research scores shockingly low. This is what Stanford and UC Berkeler tells us. Citation precision numerical value is only 39. The claim coverage is only.13. And they tell us this is one of the worst performers here. that why is this happening? They suggest here a system optimized for fluent plausible sounding text is great on the one hand. So it's excellent in synthesizing here story or story line. But if you work in science, if you want to be absolute precise, you want to reduce hallucinating to the max and you want to make sure making logical leaps that are not strictly supported by the cited sources of archive are not part of your document. Now unfortunately this is happening here with open deep research. So for purely scientific work where you have to have absolute precision this could turn out to be a fatal flaw. Now, interestingly, if we look here at the new matter, the deep scholar base as I just showed you and we put here an 03 as the engine into this benchmark, you see, interestingly for the knowledge synthesis, we have the exact same benchmark 857.857. If we look here at the nugget, we even have a better performance here at the sequential tokenization if you want. And and this is now interesting with this new method. Look at the variability from one we jump to 6. So yeah, wow. This is an impressive improvement against the commercial openi deep research performance. So this new method by Stanford and UC Berkeley even if you use it with an openi model you see in one of the indicators they are at least as good as the commercial models but they outperform the commercial model significantly in variability. Isn't this beautiful? And if you ask yourself hey why I have suddenly two LLMs I told you you can have multiple LLMs here in the pipeline. So they decided for the SAM filter and the SAM top K data they go with GBD 4.1 but for the heavyduty SAM the aggregation step they go here with an 03 or a Claude or a Gemini just to make sure we on the same page. So this is the process. So we have the filter process then the top K the top 20 papers then the final aggregation the final section generation and I told you I show you always the prompt and here you have the prompt. So whatever LLM you choose an 03 or a claude OPUS 4 or a Gemini 2.5 Pro this is here the prompt for the aggregation for the final aggregation here of your research. So now we can see the paper here highlights here the deep scholar base can achieve up to 6.3 times higher claim coverage. So one indicator and one dimension six times three better than openi deep research. Nice. So we do have new systems that really outperform the classical commercial blackbox systems. But on the other hand, if you want just a plausible, well-written draft and you are not in science and you are willing, if you are in science, to meticulously fact check every single claim, every single half sentence that EI writes for you, then if you as a human do this verification and correct it, then open EI is really maybe also interesting. However, if you are a scientist and you say, I will go with the AI system that I can trust at least partially, then the deep scholar base here with claude or gemini model as the engine is dramatically superior. If you look at the data from Stanford and UC Berkeley, the paper states also that no method achieves a score greater than.19 when you average across the metric. So this is not an incremental challenge. You know, go from 78% to 82%. No, we at 19%. This is the best model we have. So we have quite a monumental challenge in front of us to make our AI models better, more performance, more power. So there's a lot of work in front of us. And if you say okay, I don't like here the numerical values in a table. I want to see this here in a visualization. Here you have exactly all the different elements here in um yeah to radar visualization. But this is exactly the data here from the numerical table I just showed you. But if you prefer to have it here in the radar, beautiful. Here is it. 
Relevance score: -9.290

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Hello community so great that you are back. We have a new paper by Stanford University and UC Berkeley and it is all about science and a deep scholar AI system that is brand new. Now you know we have open-source research system no that do a research and synthesize known facts and new findings and you are familiar with this no for example here open scholar this is here the paper or if you go for another open source this is here storm or maybe you are familiar with deep researcher from April 2025 so we do have open systems but of course it's interesting how do they compare to the commercial system let's say openi deep research or we We have brand new search EIS where you remember MCP and here multi- aent system. So how do they compare here to multi- aent search EI systems in well let's say you do a research on the internet. Now for those we have now the study where they compare Llama forced to GPD 4.1 and 03 an Opus 4 by Claude and a Gemini 2.5 Pro. And if you ask me why not some other models, those are the set of models Stanford and UC Berkeley decided to perform their tests on. This is it. The framework they used here is here open deep search. The search with open-source reasoning agents and you remember we have here multi- aent system. We have an open reasoning agent here itself that interprets here the given human task and completes it here by orchestrating a sequence of action and one of those action might include here calling tools and one of those tools that is available to this agent is here the open search tool to connect to the internet and everything. So a multi- aent search utility and now we have a new paper, we have a new method, we have some new research benchmark results and some new ideas. So what is the task that we are looking at? Let's say you're a researcher and you have your title and maybe you have already an idea from a technical abstract. Yeah. And then you want to ask AI to write here the related work section of your publication for you. In the old times we call this a literature review. Have a look who published what in your particular specific domain, when it was published, what were the topics, how was it related to your work. Now everything should be done by the eye. Now this new system is called the deep scholar benchmark and yes it promises to generate here the deep the related work section the deep research on internet on archive on meta archive for you. So what it needs three capabilities you are looking for the retrieval. So you want to have a vast corpus the live internet or the live archive whatever you have then you want to have a synthesis report. So everything all the data that were replied here from multiple sources weave them together the findings into a coherent long form narrative and then you want and this is the most important constraint absolute verifiability. So whatever you are claiming here and whatever you are citing those must really exist. So the citing sources must be accurate on every claim and must be back traced back to its origin. You must have references. Great. So the authors here Stanford and UC Berkeley built here an automated data pipeline directly and they said you know what we not just for the internet let's go here for science let's go to archive. So we want to have a fresh data set retrieved absolutely fresh here daily or every minute here from archive. So how we do this? So three topics the recency the quality control and diversity and here they also limited theirel to computer science as a topic and they made sure that they have a diverse top with 18 different domains in computer science. So yeah we are restricted to computer science but I think you will see the results they speak for themselves. Here's the paper published today August 27, 2025. Deep scholar bench live benchmark on automated evolation for generative research systems from Stanford University in UC Berkeley. And the beauty is not that they have now a new benchmark, but what the new benchmark shows us. It shows us the true difficulty AI systems have if they do have to do a research synthesis. Here's the result. I would say wait a minute explain here at first the metric and the benchmark methodology. Okay. So they decided to go here for a threedimensional differentiation to say we look for the knowledge synthesis of this new system for the retrieval quality. It's more or less a rack system. No. And then for the verifiability because we are working in science with archive. It must be really that we can site real sources and there's no hallucination. Great. And then you can see even in those three dimension they decided to go with two or three sub indicators. So let's have a look. So for the knowledge synthesis or what I call hey how well is this generated EI text written? Is it informative? They have two indicators. the organization where an LLM as a J performs here a pairwise comparison and then they have here a metric that assesses here the informationational completeness. So what you do you have now from your text here a human written um segment here and this is automatically if you want chucked up here by the eye so you get single sentences for example but a human like red at all introduced here the transformer architecture which relies here on a self attention mechanism now the eye might say hey this is a two long sentence there are too many objects too many subjects I reduce this I chunk it up here in nuggets so the first nugget is redford at introduce the transformer stop. This is it. So simple to verify and then it goes on and you see exactly what's happening. Now you have then this essential nuggets and you want to make sure that those really represent here in the I generated text what is the main content of the human written sentence or paragraph or paper. Then second dimension the retrieval quality. Simple in my words. Did the I system find the right papers on archive for my topic. Here we have three indicators. The relevance rate, the reference coverage or simply hey if the system found here the real important papers and you know importance is just one but we can also measure here how impactful this is with an impact um quote. So we have a document importance metric. So this is a proxy for retrieving impactful word that uses here the citation comes from a specific API. Great. And finally the most important for me we have to have verifiability. So whatever we claim there is a check against some other published work. So are the claims now actually supported here by the citation? And here they go with two indicators. the citation precision where they say hey does the cited source support at least one claim made in the eye sentence just one and then they are say hey let's go full power now now for the term claim coverage we want do the cited sources support all the claims made in the eye sentence so it's really every little detail that the eye writes out here supported here by let's say arrive references Great. Then they decided, you know, so we have now the idea of the benchmark, we have the metric and the subindicator of the benchmark. And they say, you know what, now we build a vehicle. So we can switch now the engine of this vehicle. So we can put in either a GPT 4.1 or we can put in a GPD4 Omni or we go with a clawed model as the engine. And they built here the deep scholar base. Now this was interesting because remember I told you this is your almost rag system. No they went here with a classical rag multi multi- aent structure. So at first you have the query generation. So an LLM generates now a set of diverse search query based on the human input. So whatever I have the AI the LLM tells me hey that was a little bit too complicated. I generate now a set of diver search queries and then I send this up. So those queries now AI generated queries now run against the search API in our example archive and then we have a filtering now a semantic filtering and now we use another LLM is used to filter here to retrieve documents and anything that is irrelevant we just want to ignore this no but we have still a lot of documents like say 200 documents left now after the filtering so we have to do a reranking because we want to go down to the top 20 no so we have another llm based step that now reranks in detail to the only 200 remaining documents and it's a new prioritization here. What are the most relevance only of these 200 and then if we have the top 20 we want to have a semantic aggregation of the content of all 20 papers. No. So we say now the final top 20 documents are fed now as a context to an LLM and again you can choose your LLM that you like with a detailed prompt and then synthesization and the LLM writes it the final related works action. So clearly structured data processing pipeline and you can switch on and off any LLM in any place that you want and they tested here variation of different LLMs and they say so what are now the best performance combination so you see pipeline performing research this is great they run this here with a GPD4 as the default and then they exchange and said hey what happens if I add here let's say an 03 or a cloud model to GPD4.1 one all the prompts for this single steps that you just noticed here you have here in detail in the annex here of the documentation of this new research paper like here I show you the revised ODS react agent prompt here for calling here a tool so a lot of additional data I just want to give you the result this is it again you see one two three dimension and then all the different sub indicator in this dimension. Yeah, I think we can forget about the opensource research system from 2014. 
Relevance score: -9.741

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: 85. So I give you here my summary. OpenI is really here the leader in crafting a well ststructured and seemingly comprehensive narrative. Open deep research really excels in this synthesis and in the retrieval task and we have here the numerical data for the highest organization score 85 and the highest nugget coverage 39. output are well organized and capture really the essential fact better than the competitors can perform. And for the retrieval quality also the winner here. If you look here at the relevance rate 62 covers the most important human-sided reference with a reference coins of8 and finds the most impactful paper.1 but and this is now a big but the performance of open research collapses in the third dimension and this is the most crucial aspect if you have some scientific writing trust and verifiability. If you just write something that is not verifiable. 
Relevance score: -10.005

----------------------------------------------------------------
-------------------- New Query at 2025-09-03T22:28:27.132533 -------------

        ## Task
        The user's query: how deepscholar works?

        Answer the user's query based only on the retrieved context.
        Use chat history if relevant, but never go beyond the given context.
        Write a natural, context-based answer.


        ## Context
        Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Great. So verifiability here open research scores shockingly low. This is what Stanford and UC Berkeler tells us. Citation precision numerical value is only 39. The claim coverage is only.13. And they tell us this is one of the worst performers here. that why is this happening? They suggest here a system optimized for fluent plausible sounding text is great on the one hand. So it's excellent in synthesizing here story or story line. But if you work in science, if you want to be absolute precise, you want to reduce hallucinating to the max and you want to make sure making logical leaps that are not strictly supported by the cited sources of archive are not part of your document. Now unfortunately this is happening here with open deep research. So for purely scientific work where you have to have absolute precision this could turn out to be a fatal flaw. Now, interestingly, if we look here at the new matter, the deep scholar base as I just showed you and we put here an 03 as the engine into this benchmark, you see, interestingly for the knowledge synthesis, we have the exact same benchmark 857.857. If we look here at the nugget, we even have a better performance here at the sequential tokenization if you want. And and this is now interesting with this new method. Look at the variability from one we jump to 6. So yeah, wow. This is an impressive improvement against the commercial openi deep research performance. So this new method by Stanford and UC Berkeley even if you use it with an openi model you see in one of the indicators they are at least as good as the commercial models but they outperform the commercial model significantly in variability. Isn't this beautiful? And if you ask yourself hey why I have suddenly two LLMs I told you you can have multiple LLMs here in the pipeline. So they decided for the SAM filter and the SAM top K data they go with GBD 4.1 but for the heavyduty SAM the aggregation step they go here with an 03 or a Claude or a Gemini just to make sure we on the same page. So this is the process. So we have the filter process then the top K the top 20 papers then the final aggregation the final section generation and I told you I show you always the prompt and here you have the prompt. So whatever LLM you choose an 03 or a claude OPUS 4 or a Gemini 2.5 Pro this is here the prompt for the aggregation for the final aggregation here of your research. So now we can see the paper here highlights here the deep scholar base can achieve up to 6.3 times higher claim coverage. So one indicator and one dimension six times three better than openi deep research. Nice. So we do have new systems that really outperform the classical commercial blackbox systems. But on the other hand, if you want just a plausible, well-written draft and you are not in science and you are willing, if you are in science, to meticulously fact check every single claim, every single half sentence that EI writes for you, then if you as a human do this verification and correct it, then open EI is really maybe also interesting. However, if you are a scientist and you say, I will go with the AI system that I can trust at least partially, then the deep scholar base here with claude or gemini model as the engine is dramatically superior. If you look at the data from Stanford and UC Berkeley, the paper states also that no method achieves a score greater than.19 when you average across the metric. So this is not an incremental challenge. You know, go from 78% to 82%. No, we at 19%. This is the best model we have. So we have quite a monumental challenge in front of us to make our AI models better, more performance, more power. So there's a lot of work in front of us. And if you say okay, I don't like here the numerical values in a table. I want to see this here in a visualization. Here you have exactly all the different elements here in um yeah to radar visualization. But this is exactly the data here from the numerical table I just showed you. But if you prefer to have it here in the radar, beautiful. Here is it. 
Relevance score: -9.290

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Hello community so great that you are back. We have a new paper by Stanford University and UC Berkeley and it is all about science and a deep scholar AI system that is brand new. Now you know we have open-source research system no that do a research and synthesize known facts and new findings and you are familiar with this no for example here open scholar this is here the paper or if you go for another open source this is here storm or maybe you are familiar with deep researcher from April 2025 so we do have open systems but of course it's interesting how do they compare to the commercial system let's say openi deep research or we We have brand new search EIS where you remember MCP and here multi- aent system. So how do they compare here to multi- aent search EI systems in well let's say you do a research on the internet. Now for those we have now the study where they compare Llama forced to GPD 4.1 and 03 an Opus 4 by Claude and a Gemini 2.5 Pro. And if you ask me why not some other models, those are the set of models Stanford and UC Berkeley decided to perform their tests on. This is it. The framework they used here is here open deep search. The search with open-source reasoning agents and you remember we have here multi- aent system. We have an open reasoning agent here itself that interprets here the given human task and completes it here by orchestrating a sequence of action and one of those action might include here calling tools and one of those tools that is available to this agent is here the open search tool to connect to the internet and everything. So a multi- aent search utility and now we have a new paper, we have a new method, we have some new research benchmark results and some new ideas. So what is the task that we are looking at? Let's say you're a researcher and you have your title and maybe you have already an idea from a technical abstract. Yeah. And then you want to ask AI to write here the related work section of your publication for you. In the old times we call this a literature review. Have a look who published what in your particular specific domain, when it was published, what were the topics, how was it related to your work. Now everything should be done by the eye. Now this new system is called the deep scholar benchmark and yes it promises to generate here the deep the related work section the deep research on internet on archive on meta archive for you. So what it needs three capabilities you are looking for the retrieval. So you want to have a vast corpus the live internet or the live archive whatever you have then you want to have a synthesis report. So everything all the data that were replied here from multiple sources weave them together the findings into a coherent long form narrative and then you want and this is the most important constraint absolute verifiability. So whatever you are claiming here and whatever you are citing those must really exist. So the citing sources must be accurate on every claim and must be back traced back to its origin. You must have references. Great. So the authors here Stanford and UC Berkeley built here an automated data pipeline directly and they said you know what we not just for the internet let's go here for science let's go to archive. So we want to have a fresh data set retrieved absolutely fresh here daily or every minute here from archive. So how we do this? So three topics the recency the quality control and diversity and here they also limited theirel to computer science as a topic and they made sure that they have a diverse top with 18 different domains in computer science. So yeah we are restricted to computer science but I think you will see the results they speak for themselves. Here's the paper published today August 27, 2025. Deep scholar bench live benchmark on automated evolation for generative research systems from Stanford University in UC Berkeley. And the beauty is not that they have now a new benchmark, but what the new benchmark shows us. It shows us the true difficulty AI systems have if they do have to do a research synthesis. Here's the result. I would say wait a minute explain here at first the metric and the benchmark methodology. Okay. So they decided to go here for a threedimensional differentiation to say we look for the knowledge synthesis of this new system for the retrieval quality. It's more or less a rack system. No. And then for the verifiability because we are working in science with archive. It must be really that we can site real sources and there's no hallucination. Great. And then you can see even in those three dimension they decided to go with two or three sub indicators. So let's have a look. So for the knowledge synthesis or what I call hey how well is this generated EI text written? Is it informative? They have two indicators. the organization where an LLM as a J performs here a pairwise comparison and then they have here a metric that assesses here the informationational completeness. So what you do you have now from your text here a human written um segment here and this is automatically if you want chucked up here by the eye so you get single sentences for example but a human like red at all introduced here the transformer architecture which relies here on a self attention mechanism now the eye might say hey this is a two long sentence there are too many objects too many subjects I reduce this I chunk it up here in nuggets so the first nugget is redford at introduce the transformer stop. This is it. So simple to verify and then it goes on and you see exactly what's happening. Now you have then this essential nuggets and you want to make sure that those really represent here in the I generated text what is the main content of the human written sentence or paragraph or paper. Then second dimension the retrieval quality. Simple in my words. Did the I system find the right papers on archive for my topic. Here we have three indicators. The relevance rate, the reference coverage or simply hey if the system found here the real important papers and you know importance is just one but we can also measure here how impactful this is with an impact um quote. So we have a document importance metric. So this is a proxy for retrieving impactful word that uses here the citation comes from a specific API. Great. And finally the most important for me we have to have verifiability. So whatever we claim there is a check against some other published work. So are the claims now actually supported here by the citation? And here they go with two indicators. the citation precision where they say hey does the cited source support at least one claim made in the eye sentence just one and then they are say hey let's go full power now now for the term claim coverage we want do the cited sources support all the claims made in the eye sentence so it's really every little detail that the eye writes out here supported here by let's say arrive references Great. Then they decided, you know, so we have now the idea of the benchmark, we have the metric and the subindicator of the benchmark. And they say, you know what, now we build a vehicle. So we can switch now the engine of this vehicle. So we can put in either a GPT 4.1 or we can put in a GPD4 Omni or we go with a clawed model as the engine. And they built here the deep scholar base. Now this was interesting because remember I told you this is your almost rag system. No they went here with a classical rag multi multi- aent structure. So at first you have the query generation. So an LLM generates now a set of diverse search query based on the human input. So whatever I have the AI the LLM tells me hey that was a little bit too complicated. I generate now a set of diver search queries and then I send this up. So those queries now AI generated queries now run against the search API in our example archive and then we have a filtering now a semantic filtering and now we use another LLM is used to filter here to retrieve documents and anything that is irrelevant we just want to ignore this no but we have still a lot of documents like say 200 documents left now after the filtering so we have to do a reranking because we want to go down to the top 20 no so we have another llm based step that now reranks in detail to the only 200 remaining documents and it's a new prioritization here. What are the most relevance only of these 200 and then if we have the top 20 we want to have a semantic aggregation of the content of all 20 papers. No. So we say now the final top 20 documents are fed now as a context to an LLM and again you can choose your LLM that you like with a detailed prompt and then synthesization and the LLM writes it the final related works action. So clearly structured data processing pipeline and you can switch on and off any LLM in any place that you want and they tested here variation of different LLMs and they say so what are now the best performance combination so you see pipeline performing research this is great they run this here with a GPD4 as the default and then they exchange and said hey what happens if I add here let's say an 03 or a cloud model to GPD4.1 one all the prompts for this single steps that you just noticed here you have here in detail in the annex here of the documentation of this new research paper like here I show you the revised ODS react agent prompt here for calling here a tool so a lot of additional data I just want to give you the result this is it again you see one two three dimension and then all the different sub indicator in this dimension. Yeah, I think we can forget about the opensource research system from 2014. 
Relevance score: -9.741

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: 85. So I give you here my summary. OpenI is really here the leader in crafting a well ststructured and seemingly comprehensive narrative. Open deep research really excels in this synthesis and in the retrieval task and we have here the numerical data for the highest organization score 85 and the highest nugget coverage 39. output are well organized and capture really the essential fact better than the competitors can perform. And for the retrieval quality also the winner here. If you look here at the relevance rate 62 covers the most important human-sided reference with a reference coins of8 and finds the most impactful paper.1 but and this is now a big but the performance of open research collapses in the third dimension and this is the most crucial aspect if you have some scientific writing trust and verifiability. If you just write something that is not verifiable. 
Relevance score: -10.005

----------------------------------------------------------------
-------------------- New Query at 2025-09-03T22:29:54.080008 -------------

        ## Task
        The user's query: multi-ent search approach

        Answer the user's query based only on the retrieved context.
        Use chat history if relevant, but never go beyond the given context.
        Write a natural, context-based answer.


        ## Context
        Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Hello community so great that you are back. We have a new paper by Stanford University and UC Berkeley and it is all about science and a deep scholar AI system that is brand new. Now you know we have open-source research system no that do a research and synthesize known facts and new findings and you are familiar with this no for example here open scholar this is here the paper or if you go for another open source this is here storm or maybe you are familiar with deep researcher from April 2025 so we do have open systems but of course it's interesting how do they compare to the commercial system let's say openi deep research or we We have brand new search EIS where you remember MCP and here multi- aent system. So how do they compare here to multi- aent search EI systems in well let's say you do a research on the internet. Now for those we have now the study where they compare Llama forced to GPD 4.1 and 03 an Opus 4 by Claude and a Gemini 2.5 Pro. And if you ask me why not some other models, those are the set of models Stanford and UC Berkeley decided to perform their tests on. This is it. The framework they used here is here open deep search. The search with open-source reasoning agents and you remember we have here multi- aent system. We have an open reasoning agent here itself that interprets here the given human task and completes it here by orchestrating a sequence of action and one of those action might include here calling tools and one of those tools that is available to this agent is here the open search tool to connect to the internet and everything. So a multi- aent search utility and now we have a new paper, we have a new method, we have some new research benchmark results and some new ideas. So what is the task that we are looking at? Let's say you're a researcher and you have your title and maybe you have already an idea from a technical abstract. Yeah. And then you want to ask AI to write here the related work section of your publication for you. In the old times we call this a literature review. Have a look who published what in your particular specific domain, when it was published, what were the topics, how was it related to your work. Now everything should be done by the eye. Now this new system is called the deep scholar benchmark and yes it promises to generate here the deep the related work section the deep research on internet on archive on meta archive for you. So what it needs three capabilities you are looking for the retrieval. So you want to have a vast corpus the live internet or the live archive whatever you have then you want to have a synthesis report. So everything all the data that were replied here from multiple sources weave them together the findings into a coherent long form narrative and then you want and this is the most important constraint absolute verifiability. So whatever you are claiming here and whatever you are citing those must really exist. So the citing sources must be accurate on every claim and must be back traced back to its origin. You must have references. Great. So the authors here Stanford and UC Berkeley built here an automated data pipeline directly and they said you know what we not just for the internet let's go here for science let's go to archive. So we want to have a fresh data set retrieved absolutely fresh here daily or every minute here from archive. So how we do this? So three topics the recency the quality control and diversity and here they also limited theirel to computer science as a topic and they made sure that they have a diverse top with 18 different domains in computer science. So yeah we are restricted to computer science but I think you will see the results they speak for themselves. Here's the paper published today August 27, 2025. Deep scholar bench live benchmark on automated evolation for generative research systems from Stanford University in UC Berkeley. And the beauty is not that they have now a new benchmark, but what the new benchmark shows us. It shows us the true difficulty AI systems have if they do have to do a research synthesis. Here's the result. I would say wait a minute explain here at first the metric and the benchmark methodology. Okay. So they decided to go here for a threedimensional differentiation to say we look for the knowledge synthesis of this new system for the retrieval quality. It's more or less a rack system. No. And then for the verifiability because we are working in science with archive. It must be really that we can site real sources and there's no hallucination. Great. And then you can see even in those three dimension they decided to go with two or three sub indicators. So let's have a look. So for the knowledge synthesis or what I call hey how well is this generated EI text written? Is it informative? They have two indicators. the organization where an LLM as a J performs here a pairwise comparison and then they have here a metric that assesses here the informationational completeness. So what you do you have now from your text here a human written um segment here and this is automatically if you want chucked up here by the eye so you get single sentences for example but a human like red at all introduced here the transformer architecture which relies here on a self attention mechanism now the eye might say hey this is a two long sentence there are too many objects too many subjects I reduce this I chunk it up here in nuggets so the first nugget is redford at introduce the transformer stop. This is it. So simple to verify and then it goes on and you see exactly what's happening. Now you have then this essential nuggets and you want to make sure that those really represent here in the I generated text what is the main content of the human written sentence or paragraph or paper. Then second dimension the retrieval quality. Simple in my words. Did the I system find the right papers on archive for my topic. Here we have three indicators. The relevance rate, the reference coverage or simply hey if the system found here the real important papers and you know importance is just one but we can also measure here how impactful this is with an impact um quote. So we have a document importance metric. So this is a proxy for retrieving impactful word that uses here the citation comes from a specific API. Great. And finally the most important for me we have to have verifiability. So whatever we claim there is a check against some other published work. So are the claims now actually supported here by the citation? And here they go with two indicators. the citation precision where they say hey does the cited source support at least one claim made in the eye sentence just one and then they are say hey let's go full power now now for the term claim coverage we want do the cited sources support all the claims made in the eye sentence so it's really every little detail that the eye writes out here supported here by let's say arrive references Great. Then they decided, you know, so we have now the idea of the benchmark, we have the metric and the subindicator of the benchmark. And they say, you know what, now we build a vehicle. So we can switch now the engine of this vehicle. So we can put in either a GPT 4.1 or we can put in a GPD4 Omni or we go with a clawed model as the engine. And they built here the deep scholar base. Now this was interesting because remember I told you this is your almost rag system. No they went here with a classical rag multi multi- aent structure. So at first you have the query generation. So an LLM generates now a set of diverse search query based on the human input. So whatever I have the AI the LLM tells me hey that was a little bit too complicated. I generate now a set of diver search queries and then I send this up. So those queries now AI generated queries now run against the search API in our example archive and then we have a filtering now a semantic filtering and now we use another LLM is used to filter here to retrieve documents and anything that is irrelevant we just want to ignore this no but we have still a lot of documents like say 200 documents left now after the filtering so we have to do a reranking because we want to go down to the top 20 no so we have another llm based step that now reranks in detail to the only 200 remaining documents and it's a new prioritization here. What are the most relevance only of these 200 and then if we have the top 20 we want to have a semantic aggregation of the content of all 20 papers. No. So we say now the final top 20 documents are fed now as a context to an LLM and again you can choose your LLM that you like with a detailed prompt and then synthesization and the LLM writes it the final related works action. So clearly structured data processing pipeline and you can switch on and off any LLM in any place that you want and they tested here variation of different LLMs and they say so what are now the best performance combination so you see pipeline performing research this is great they run this here with a GPD4 as the default and then they exchange and said hey what happens if I add here let's say an 03 or a cloud model to GPD4.1 one all the prompts for this single steps that you just noticed here you have here in detail in the annex here of the documentation of this new research paper like here I show you the revised ODS react agent prompt here for calling here a tool so a lot of additional data I just want to give you the result this is it again you see one two three dimension and then all the different sub indicator in this dimension. Yeah, I think we can forget about the opensource research system from 2014. 
Relevance score: -7.954

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: And there we have it. A new research paper by Stanford University UC Berkeley about AI doing here the research the literature review here on brand new topics live in a multi- aent network deep scholar to research and synthesize knowledge. And if you are in science this could be a real benefit, a real help. 
Relevance score: -11.098

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: 85. So I give you here my summary. OpenI is really here the leader in crafting a well ststructured and seemingly comprehensive narrative. Open deep research really excels in this synthesis and in the retrieval task and we have here the numerical data for the highest organization score 85 and the highest nugget coverage 39. output are well organized and capture really the essential fact better than the competitors can perform. And for the retrieval quality also the winner here. If you look here at the relevance rate 62 covers the most important human-sided reference with a reference coins of8 and finds the most impactful paper.1 but and this is now a big but the performance of open research collapses in the third dimension and this is the most crucial aspect if you have some scientific writing trust and verifiability. If you just write something that is not verifiable. 
Relevance score: -11.181

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Great. So verifiability here open research scores shockingly low. This is what Stanford and UC Berkeler tells us. Citation precision numerical value is only 39. The claim coverage is only.13. And they tell us this is one of the worst performers here. that why is this happening? They suggest here a system optimized for fluent plausible sounding text is great on the one hand. So it's excellent in synthesizing here story or story line. But if you work in science, if you want to be absolute precise, you want to reduce hallucinating to the max and you want to make sure making logical leaps that are not strictly supported by the cited sources of archive are not part of your document. Now unfortunately this is happening here with open deep research. So for purely scientific work where you have to have absolute precision this could turn out to be a fatal flaw. Now, interestingly, if we look here at the new matter, the deep scholar base as I just showed you and we put here an 03 as the engine into this benchmark, you see, interestingly for the knowledge synthesis, we have the exact same benchmark 857.857. If we look here at the nugget, we even have a better performance here at the sequential tokenization if you want. And and this is now interesting with this new method. Look at the variability from one we jump to 6. So yeah, wow. This is an impressive improvement against the commercial openi deep research performance. So this new method by Stanford and UC Berkeley even if you use it with an openi model you see in one of the indicators they are at least as good as the commercial models but they outperform the commercial model significantly in variability. Isn't this beautiful? And if you ask yourself hey why I have suddenly two LLMs I told you you can have multiple LLMs here in the pipeline. So they decided for the SAM filter and the SAM top K data they go with GBD 4.1 but for the heavyduty SAM the aggregation step they go here with an 03 or a Claude or a Gemini just to make sure we on the same page. So this is the process. So we have the filter process then the top K the top 20 papers then the final aggregation the final section generation and I told you I show you always the prompt and here you have the prompt. So whatever LLM you choose an 03 or a claude OPUS 4 or a Gemini 2.5 Pro this is here the prompt for the aggregation for the final aggregation here of your research. So now we can see the paper here highlights here the deep scholar base can achieve up to 6.3 times higher claim coverage. So one indicator and one dimension six times three better than openi deep research. Nice. So we do have new systems that really outperform the classical commercial blackbox systems. But on the other hand, if you want just a plausible, well-written draft and you are not in science and you are willing, if you are in science, to meticulously fact check every single claim, every single half sentence that EI writes for you, then if you as a human do this verification and correct it, then open EI is really maybe also interesting. However, if you are a scientist and you say, I will go with the AI system that I can trust at least partially, then the deep scholar base here with claude or gemini model as the engine is dramatically superior. If you look at the data from Stanford and UC Berkeley, the paper states also that no method achieves a score greater than.19 when you average across the metric. So this is not an incremental challenge. You know, go from 78% to 82%. No, we at 19%. This is the best model we have. So we have quite a monumental challenge in front of us to make our AI models better, more performance, more power. So there's a lot of work in front of us. And if you say okay, I don't like here the numerical values in a table. I want to see this here in a visualization. Here you have exactly all the different elements here in um yeah to radar visualization. But this is exactly the data here from the numerical table I just showed you. But if you prefer to have it here in the radar, beautiful. Here is it. 
Relevance score: -10.681

----------------------------------------------------------------
-------------------- New Query at 2025-09-03T22:48:32.815131 -------------

        ## Task
        The user's query: how deepscholar works?

        Answer the user's query based only on the retrieved context.
        Use chat history if relevant, but never go beyond the given context.
        Write a natural, context-based answer.


        ## Context
        Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Great. So verifiability here open research scores shockingly low. This is what Stanford and UC Berkeler tells us. Citation precision numerical value is only 39. The claim coverage is only.13. And they tell us this is one of the worst performers here. that why is this happening? They suggest here a system optimized for fluent plausible sounding text is great on the one hand. So it's excellent in synthesizing here story or story line. But if you work in science, if you want to be absolute precise, you want to reduce hallucinating to the max and you want to make sure making logical leaps that are not strictly supported by the cited sources of archive are not part of your document. Now unfortunately this is happening here with open deep research. So for purely scientific work where you have to have absolute precision this could turn out to be a fatal flaw. Now, interestingly, if we look here at the new matter, the deep scholar base as I just showed you and we put here an 03 as the engine into this benchmark, you see, interestingly for the knowledge synthesis, we have the exact same benchmark 857.857. If we look here at the nugget, we even have a better performance here at the sequential tokenization if you want. And and this is now interesting with this new method. Look at the variability from one we jump to 6. So yeah, wow. This is an impressive improvement against the commercial openi deep research performance. So this new method by Stanford and UC Berkeley even if you use it with an openi model you see in one of the indicators they are at least as good as the commercial models but they outperform the commercial model significantly in variability. Isn't this beautiful? And if you ask yourself hey why I have suddenly two LLMs I told you you can have multiple LLMs here in the pipeline. So they decided for the SAM filter and the SAM top K data they go with GBD 4.1 but for the heavyduty SAM the aggregation step they go here with an 03 or a Claude or a Gemini just to make sure we on the same page. So this is the process. So we have the filter process then the top K the top 20 papers then the final aggregation the final section generation and I told you I show you always the prompt and here you have the prompt. So whatever LLM you choose an 03 or a claude OPUS 4 or a Gemini 2.5 Pro this is here the prompt for the aggregation for the final aggregation here of your research. So now we can see the paper here highlights here the deep scholar base can achieve up to 6.3 times higher claim coverage. So one indicator and one dimension six times three better than openi deep research. Nice. So we do have new systems that really outperform the classical commercial blackbox systems. But on the other hand, if you want just a plausible, well-written draft and you are not in science and you are willing, if you are in science, to meticulously fact check every single claim, every single half sentence that EI writes for you, then if you as a human do this verification and correct it, then open EI is really maybe also interesting. However, if you are a scientist and you say, I will go with the AI system that I can trust at least partially, then the deep scholar base here with claude or gemini model as the engine is dramatically superior. If you look at the data from Stanford and UC Berkeley, the paper states also that no method achieves a score greater than.19 when you average across the metric. So this is not an incremental challenge. You know, go from 78% to 82%. No, we at 19%. This is the best model we have. So we have quite a monumental challenge in front of us to make our AI models better, more performance, more power. So there's a lot of work in front of us. And if you say okay, I don't like here the numerical values in a table. I want to see this here in a visualization. Here you have exactly all the different elements here in um yeah to radar visualization. But this is exactly the data here from the numerical table I just showed you. But if you prefer to have it here in the radar, beautiful. Here is it. 
Relevance score: -9.290

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Hello community so great that you are back. We have a new paper by Stanford University and UC Berkeley and it is all about science and a deep scholar AI system that is brand new. Now you know we have open-source research system no that do a research and synthesize known facts and new findings and you are familiar with this no for example here open scholar this is here the paper or if you go for another open source this is here storm or maybe you are familiar with deep researcher from April 2025 so we do have open systems but of course it's interesting how do they compare to the commercial system let's say openi deep research or we We have brand new search EIS where you remember MCP and here multi- aent system. So how do they compare here to multi- aent search EI systems in well let's say you do a research on the internet. Now for those we have now the study where they compare Llama forced to GPD 4.1 and 03 an Opus 4 by Claude and a Gemini 2.5 Pro. And if you ask me why not some other models, those are the set of models Stanford and UC Berkeley decided to perform their tests on. This is it. The framework they used here is here open deep search. The search with open-source reasoning agents and you remember we have here multi- aent system. We have an open reasoning agent here itself that interprets here the given human task and completes it here by orchestrating a sequence of action and one of those action might include here calling tools and one of those tools that is available to this agent is here the open search tool to connect to the internet and everything. So a multi- aent search utility and now we have a new paper, we have a new method, we have some new research benchmark results and some new ideas. So what is the task that we are looking at? Let's say you're a researcher and you have your title and maybe you have already an idea from a technical abstract. Yeah. And then you want to ask AI to write here the related work section of your publication for you. In the old times we call this a literature review. Have a look who published what in your particular specific domain, when it was published, what were the topics, how was it related to your work. Now everything should be done by the eye. Now this new system is called the deep scholar benchmark and yes it promises to generate here the deep the related work section the deep research on internet on archive on meta archive for you. So what it needs three capabilities you are looking for the retrieval. So you want to have a vast corpus the live internet or the live archive whatever you have then you want to have a synthesis report. So everything all the data that were replied here from multiple sources weave them together the findings into a coherent long form narrative and then you want and this is the most important constraint absolute verifiability. So whatever you are claiming here and whatever you are citing those must really exist. So the citing sources must be accurate on every claim and must be back traced back to its origin. You must have references. Great. So the authors here Stanford and UC Berkeley built here an automated data pipeline directly and they said you know what we not just for the internet let's go here for science let's go to archive. So we want to have a fresh data set retrieved absolutely fresh here daily or every minute here from archive. So how we do this? So three topics the recency the quality control and diversity and here they also limited theirel to computer science as a topic and they made sure that they have a diverse top with 18 different domains in computer science. So yeah we are restricted to computer science but I think you will see the results they speak for themselves. Here's the paper published today August 27, 2025. Deep scholar bench live benchmark on automated evolation for generative research systems from Stanford University in UC Berkeley. And the beauty is not that they have now a new benchmark, but what the new benchmark shows us. It shows us the true difficulty AI systems have if they do have to do a research synthesis. Here's the result. I would say wait a minute explain here at first the metric and the benchmark methodology. Okay. So they decided to go here for a threedimensional differentiation to say we look for the knowledge synthesis of this new system for the retrieval quality. It's more or less a rack system. No. And then for the verifiability because we are working in science with archive. It must be really that we can site real sources and there's no hallucination. Great. And then you can see even in those three dimension they decided to go with two or three sub indicators. So let's have a look. So for the knowledge synthesis or what I call hey how well is this generated EI text written? Is it informative? They have two indicators. the organization where an LLM as a J performs here a pairwise comparison and then they have here a metric that assesses here the informationational completeness. So what you do you have now from your text here a human written um segment here and this is automatically if you want chucked up here by the eye so you get single sentences for example but a human like red at all introduced here the transformer architecture which relies here on a self attention mechanism now the eye might say hey this is a two long sentence there are too many objects too many subjects I reduce this I chunk it up here in nuggets so the first nugget is redford at introduce the transformer stop. This is it. So simple to verify and then it goes on and you see exactly what's happening. Now you have then this essential nuggets and you want to make sure that those really represent here in the I generated text what is the main content of the human written sentence or paragraph or paper. Then second dimension the retrieval quality. Simple in my words. Did the I system find the right papers on archive for my topic. Here we have three indicators. The relevance rate, the reference coverage or simply hey if the system found here the real important papers and you know importance is just one but we can also measure here how impactful this is with an impact um quote. So we have a document importance metric. So this is a proxy for retrieving impactful word that uses here the citation comes from a specific API. Great. And finally the most important for me we have to have verifiability. So whatever we claim there is a check against some other published work. So are the claims now actually supported here by the citation? And here they go with two indicators. the citation precision where they say hey does the cited source support at least one claim made in the eye sentence just one and then they are say hey let's go full power now now for the term claim coverage we want do the cited sources support all the claims made in the eye sentence so it's really every little detail that the eye writes out here supported here by let's say arrive references Great. Then they decided, you know, so we have now the idea of the benchmark, we have the metric and the subindicator of the benchmark. And they say, you know what, now we build a vehicle. So we can switch now the engine of this vehicle. So we can put in either a GPT 4.1 or we can put in a GPD4 Omni or we go with a clawed model as the engine. And they built here the deep scholar base. Now this was interesting because remember I told you this is your almost rag system. No they went here with a classical rag multi multi- aent structure. So at first you have the query generation. So an LLM generates now a set of diverse search query based on the human input. So whatever I have the AI the LLM tells me hey that was a little bit too complicated. I generate now a set of diver search queries and then I send this up. So those queries now AI generated queries now run against the search API in our example archive and then we have a filtering now a semantic filtering and now we use another LLM is used to filter here to retrieve documents and anything that is irrelevant we just want to ignore this no but we have still a lot of documents like say 200 documents left now after the filtering so we have to do a reranking because we want to go down to the top 20 no so we have another llm based step that now reranks in detail to the only 200 remaining documents and it's a new prioritization here. What are the most relevance only of these 200 and then if we have the top 20 we want to have a semantic aggregation of the content of all 20 papers. No. So we say now the final top 20 documents are fed now as a context to an LLM and again you can choose your LLM that you like with a detailed prompt and then synthesization and the LLM writes it the final related works action. So clearly structured data processing pipeline and you can switch on and off any LLM in any place that you want and they tested here variation of different LLMs and they say so what are now the best performance combination so you see pipeline performing research this is great they run this here with a GPD4 as the default and then they exchange and said hey what happens if I add here let's say an 03 or a cloud model to GPD4.1 one all the prompts for this single steps that you just noticed here you have here in detail in the annex here of the documentation of this new research paper like here I show you the revised ODS react agent prompt here for calling here a tool so a lot of additional data I just want to give you the result this is it again you see one two three dimension and then all the different sub indicator in this dimension. Yeah, I think we can forget about the opensource research system from 2014. 
Relevance score: -9.741

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: 85. So I give you here my summary. OpenI is really here the leader in crafting a well ststructured and seemingly comprehensive narrative. Open deep research really excels in this synthesis and in the retrieval task and we have here the numerical data for the highest organization score 85 and the highest nugget coverage 39. output are well organized and capture really the essential fact better than the competitors can perform. And for the retrieval quality also the winner here. If you look here at the relevance rate 62 covers the most important human-sided reference with a reference coins of8 and finds the most impactful paper.1 but and this is now a big but the performance of open research collapses in the third dimension and this is the most crucial aspect if you have some scientific writing trust and verifiability. If you just write something that is not verifiable. 
Relevance score: -10.005

----------------------------------------------------------------
-------------------- New Query at 2025-09-03T22:49:53.928257 -------------

        ## Task
        The user's query: how deepscholar works?

        Answer the user's query based only on the retrieved context.
        Use chat history if relevant, but never go beyond the given context.
        Write a natural, context-based answer.


        ## Context
        Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Great. So verifiability here open research scores shockingly low. This is what Stanford and UC Berkeler tells us. Citation precision numerical value is only 39. The claim coverage is only.13. And they tell us this is one of the worst performers here. that why is this happening? They suggest here a system optimized for fluent plausible sounding text is great on the one hand. So it's excellent in synthesizing here story or story line. But if you work in science, if you want to be absolute precise, you want to reduce hallucinating to the max and you want to make sure making logical leaps that are not strictly supported by the cited sources of archive are not part of your document. Now unfortunately this is happening here with open deep research. So for purely scientific work where you have to have absolute precision this could turn out to be a fatal flaw. Now, interestingly, if we look here at the new matter, the deep scholar base as I just showed you and we put here an 03 as the engine into this benchmark, you see, interestingly for the knowledge synthesis, we have the exact same benchmark 857.857. If we look here at the nugget, we even have a better performance here at the sequential tokenization if you want. And and this is now interesting with this new method. Look at the variability from one we jump to 6. So yeah, wow. This is an impressive improvement against the commercial openi deep research performance. So this new method by Stanford and UC Berkeley even if you use it with an openi model you see in one of the indicators they are at least as good as the commercial models but they outperform the commercial model significantly in variability. Isn't this beautiful? And if you ask yourself hey why I have suddenly two LLMs I told you you can have multiple LLMs here in the pipeline. So they decided for the SAM filter and the SAM top K data they go with GBD 4.1 but for the heavyduty SAM the aggregation step they go here with an 03 or a Claude or a Gemini just to make sure we on the same page. So this is the process. So we have the filter process then the top K the top 20 papers then the final aggregation the final section generation and I told you I show you always the prompt and here you have the prompt. So whatever LLM you choose an 03 or a claude OPUS 4 or a Gemini 2.5 Pro this is here the prompt for the aggregation for the final aggregation here of your research. So now we can see the paper here highlights here the deep scholar base can achieve up to 6.3 times higher claim coverage. So one indicator and one dimension six times three better than openi deep research. Nice. So we do have new systems that really outperform the classical commercial blackbox systems. But on the other hand, if you want just a plausible, well-written draft and you are not in science and you are willing, if you are in science, to meticulously fact check every single claim, every single half sentence that EI writes for you, then if you as a human do this verification and correct it, then open EI is really maybe also interesting. However, if you are a scientist and you say, I will go with the AI system that I can trust at least partially, then the deep scholar base here with claude or gemini model as the engine is dramatically superior. If you look at the data from Stanford and UC Berkeley, the paper states also that no method achieves a score greater than.19 when you average across the metric. So this is not an incremental challenge. You know, go from 78% to 82%. No, we at 19%. This is the best model we have. So we have quite a monumental challenge in front of us to make our AI models better, more performance, more power. So there's a lot of work in front of us. And if you say okay, I don't like here the numerical values in a table. I want to see this here in a visualization. Here you have exactly all the different elements here in um yeah to radar visualization. But this is exactly the data here from the numerical table I just showed you. But if you prefer to have it here in the radar, beautiful. Here is it. 
Relevance score: -9.290

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Hello community so great that you are back. We have a new paper by Stanford University and UC Berkeley and it is all about science and a deep scholar AI system that is brand new. Now you know we have open-source research system no that do a research and synthesize known facts and new findings and you are familiar with this no for example here open scholar this is here the paper or if you go for another open source this is here storm or maybe you are familiar with deep researcher from April 2025 so we do have open systems but of course it's interesting how do they compare to the commercial system let's say openi deep research or we We have brand new search EIS where you remember MCP and here multi- aent system. So how do they compare here to multi- aent search EI systems in well let's say you do a research on the internet. Now for those we have now the study where they compare Llama forced to GPD 4.1 and 03 an Opus 4 by Claude and a Gemini 2.5 Pro. And if you ask me why not some other models, those are the set of models Stanford and UC Berkeley decided to perform their tests on. This is it. The framework they used here is here open deep search. The search with open-source reasoning agents and you remember we have here multi- aent system. We have an open reasoning agent here itself that interprets here the given human task and completes it here by orchestrating a sequence of action and one of those action might include here calling tools and one of those tools that is available to this agent is here the open search tool to connect to the internet and everything. So a multi- aent search utility and now we have a new paper, we have a new method, we have some new research benchmark results and some new ideas. So what is the task that we are looking at? Let's say you're a researcher and you have your title and maybe you have already an idea from a technical abstract. Yeah. And then you want to ask AI to write here the related work section of your publication for you. In the old times we call this a literature review. Have a look who published what in your particular specific domain, when it was published, what were the topics, how was it related to your work. Now everything should be done by the eye. Now this new system is called the deep scholar benchmark and yes it promises to generate here the deep the related work section the deep research on internet on archive on meta archive for you. So what it needs three capabilities you are looking for the retrieval. So you want to have a vast corpus the live internet or the live archive whatever you have then you want to have a synthesis report. So everything all the data that were replied here from multiple sources weave them together the findings into a coherent long form narrative and then you want and this is the most important constraint absolute verifiability. So whatever you are claiming here and whatever you are citing those must really exist. So the citing sources must be accurate on every claim and must be back traced back to its origin. You must have references. Great. So the authors here Stanford and UC Berkeley built here an automated data pipeline directly and they said you know what we not just for the internet let's go here for science let's go to archive. So we want to have a fresh data set retrieved absolutely fresh here daily or every minute here from archive. So how we do this? So three topics the recency the quality control and diversity and here they also limited theirel to computer science as a topic and they made sure that they have a diverse top with 18 different domains in computer science. So yeah we are restricted to computer science but I think you will see the results they speak for themselves. Here's the paper published today August 27, 2025. Deep scholar bench live benchmark on automated evolation for generative research systems from Stanford University in UC Berkeley. And the beauty is not that they have now a new benchmark, but what the new benchmark shows us. It shows us the true difficulty AI systems have if they do have to do a research synthesis. Here's the result. I would say wait a minute explain here at first the metric and the benchmark methodology. Okay. So they decided to go here for a threedimensional differentiation to say we look for the knowledge synthesis of this new system for the retrieval quality. It's more or less a rack system. No. And then for the verifiability because we are working in science with archive. It must be really that we can site real sources and there's no hallucination. Great. And then you can see even in those three dimension they decided to go with two or three sub indicators. So let's have a look. So for the knowledge synthesis or what I call hey how well is this generated EI text written? Is it informative? They have two indicators. the organization where an LLM as a J performs here a pairwise comparison and then they have here a metric that assesses here the informationational completeness. So what you do you have now from your text here a human written um segment here and this is automatically if you want chucked up here by the eye so you get single sentences for example but a human like red at all introduced here the transformer architecture which relies here on a self attention mechanism now the eye might say hey this is a two long sentence there are too many objects too many subjects I reduce this I chunk it up here in nuggets so the first nugget is redford at introduce the transformer stop. This is it. So simple to verify and then it goes on and you see exactly what's happening. Now you have then this essential nuggets and you want to make sure that those really represent here in the I generated text what is the main content of the human written sentence or paragraph or paper. Then second dimension the retrieval quality. Simple in my words. Did the I system find the right papers on archive for my topic. Here we have three indicators. The relevance rate, the reference coverage or simply hey if the system found here the real important papers and you know importance is just one but we can also measure here how impactful this is with an impact um quote. So we have a document importance metric. So this is a proxy for retrieving impactful word that uses here the citation comes from a specific API. Great. And finally the most important for me we have to have verifiability. So whatever we claim there is a check against some other published work. So are the claims now actually supported here by the citation? And here they go with two indicators. the citation precision where they say hey does the cited source support at least one claim made in the eye sentence just one and then they are say hey let's go full power now now for the term claim coverage we want do the cited sources support all the claims made in the eye sentence so it's really every little detail that the eye writes out here supported here by let's say arrive references Great. Then they decided, you know, so we have now the idea of the benchmark, we have the metric and the subindicator of the benchmark. And they say, you know what, now we build a vehicle. So we can switch now the engine of this vehicle. So we can put in either a GPT 4.1 or we can put in a GPD4 Omni or we go with a clawed model as the engine. And they built here the deep scholar base. Now this was interesting because remember I told you this is your almost rag system. No they went here with a classical rag multi multi- aent structure. So at first you have the query generation. So an LLM generates now a set of diverse search query based on the human input. So whatever I have the AI the LLM tells me hey that was a little bit too complicated. I generate now a set of diver search queries and then I send this up. So those queries now AI generated queries now run against the search API in our example archive and then we have a filtering now a semantic filtering and now we use another LLM is used to filter here to retrieve documents and anything that is irrelevant we just want to ignore this no but we have still a lot of documents like say 200 documents left now after the filtering so we have to do a reranking because we want to go down to the top 20 no so we have another llm based step that now reranks in detail to the only 200 remaining documents and it's a new prioritization here. What are the most relevance only of these 200 and then if we have the top 20 we want to have a semantic aggregation of the content of all 20 papers. No. So we say now the final top 20 documents are fed now as a context to an LLM and again you can choose your LLM that you like with a detailed prompt and then synthesization and the LLM writes it the final related works action. So clearly structured data processing pipeline and you can switch on and off any LLM in any place that you want and they tested here variation of different LLMs and they say so what are now the best performance combination so you see pipeline performing research this is great they run this here with a GPD4 as the default and then they exchange and said hey what happens if I add here let's say an 03 or a cloud model to GPD4.1 one all the prompts for this single steps that you just noticed here you have here in detail in the annex here of the documentation of this new research paper like here I show you the revised ODS react agent prompt here for calling here a tool so a lot of additional data I just want to give you the result this is it again you see one two three dimension and then all the different sub indicator in this dimension. Yeah, I think we can forget about the opensource research system from 2014. 
Relevance score: -9.741

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: 85. So I give you here my summary. OpenI is really here the leader in crafting a well ststructured and seemingly comprehensive narrative. Open deep research really excels in this synthesis and in the retrieval task and we have here the numerical data for the highest organization score 85 and the highest nugget coverage 39. output are well organized and capture really the essential fact better than the competitors can perform. And for the retrieval quality also the winner here. If you look here at the relevance rate 62 covers the most important human-sided reference with a reference coins of8 and finds the most impactful paper.1 but and this is now a big but the performance of open research collapses in the third dimension and this is the most crucial aspect if you have some scientific writing trust and verifiability. If you just write something that is not verifiable. 
Relevance score: -10.005

----------------------------------------------------------------
-------------------- New Query at 2025-09-03T22:52:20.555804 -------------

        ## Task
        The user's query: how deepscholar works?

        Answer the user's query based only on the retrieved context.
        Use chat history if relevant, but never go beyond the given context.
        Write a natural, context-based answer.


        ## Context
        Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Great. So verifiability here open research scores shockingly low. This is what Stanford and UC Berkeler tells us. Citation precision numerical value is only 39. The claim coverage is only.13. And they tell us this is one of the worst performers here. that why is this happening? They suggest here a system optimized for fluent plausible sounding text is great on the one hand. So it's excellent in synthesizing here story or story line. But if you work in science, if you want to be absolute precise, you want to reduce hallucinating to the max and you want to make sure making logical leaps that are not strictly supported by the cited sources of archive are not part of your document. Now unfortunately this is happening here with open deep research. So for purely scientific work where you have to have absolute precision this could turn out to be a fatal flaw. Now, interestingly, if we look here at the new matter, the deep scholar base as I just showed you and we put here an 03 as the engine into this benchmark, you see, interestingly for the knowledge synthesis, we have the exact same benchmark 857.857. If we look here at the nugget, we even have a better performance here at the sequential tokenization if you want. And and this is now interesting with this new method. Look at the variability from one we jump to 6. So yeah, wow. This is an impressive improvement against the commercial openi deep research performance. So this new method by Stanford and UC Berkeley even if you use it with an openi model you see in one of the indicators they are at least as good as the commercial models but they outperform the commercial model significantly in variability. Isn't this beautiful? And if you ask yourself hey why I have suddenly two LLMs I told you you can have multiple LLMs here in the pipeline. So they decided for the SAM filter and the SAM top K data they go with GBD 4.1 but for the heavyduty SAM the aggregation step they go here with an 03 or a Claude or a Gemini just to make sure we on the same page. So this is the process. So we have the filter process then the top K the top 20 papers then the final aggregation the final section generation and I told you I show you always the prompt and here you have the prompt. So whatever LLM you choose an 03 or a claude OPUS 4 or a Gemini 2.5 Pro this is here the prompt for the aggregation for the final aggregation here of your research. So now we can see the paper here highlights here the deep scholar base can achieve up to 6.3 times higher claim coverage. So one indicator and one dimension six times three better than openi deep research. Nice. So we do have new systems that really outperform the classical commercial blackbox systems. But on the other hand, if you want just a plausible, well-written draft and you are not in science and you are willing, if you are in science, to meticulously fact check every single claim, every single half sentence that EI writes for you, then if you as a human do this verification and correct it, then open EI is really maybe also interesting. However, if you are a scientist and you say, I will go with the AI system that I can trust at least partially, then the deep scholar base here with claude or gemini model as the engine is dramatically superior. If you look at the data from Stanford and UC Berkeley, the paper states also that no method achieves a score greater than.19 when you average across the metric. So this is not an incremental challenge. You know, go from 78% to 82%. No, we at 19%. This is the best model we have. So we have quite a monumental challenge in front of us to make our AI models better, more performance, more power. So there's a lot of work in front of us. And if you say okay, I don't like here the numerical values in a table. I want to see this here in a visualization. Here you have exactly all the different elements here in um yeah to radar visualization. But this is exactly the data here from the numerical table I just showed you. But if you prefer to have it here in the radar, beautiful. Here is it. 
Relevance score: -9.290

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Hello community so great that you are back. We have a new paper by Stanford University and UC Berkeley and it is all about science and a deep scholar AI system that is brand new. Now you know we have open-source research system no that do a research and synthesize known facts and new findings and you are familiar with this no for example here open scholar this is here the paper or if you go for another open source this is here storm or maybe you are familiar with deep researcher from April 2025 so we do have open systems but of course it's interesting how do they compare to the commercial system let's say openi deep research or we We have brand new search EIS where you remember MCP and here multi- aent system. So how do they compare here to multi- aent search EI systems in well let's say you do a research on the internet. Now for those we have now the study where they compare Llama forced to GPD 4.1 and 03 an Opus 4 by Claude and a Gemini 2.5 Pro. And if you ask me why not some other models, those are the set of models Stanford and UC Berkeley decided to perform their tests on. This is it. The framework they used here is here open deep search. The search with open-source reasoning agents and you remember we have here multi- aent system. We have an open reasoning agent here itself that interprets here the given human task and completes it here by orchestrating a sequence of action and one of those action might include here calling tools and one of those tools that is available to this agent is here the open search tool to connect to the internet and everything. So a multi- aent search utility and now we have a new paper, we have a new method, we have some new research benchmark results and some new ideas. So what is the task that we are looking at? Let's say you're a researcher and you have your title and maybe you have already an idea from a technical abstract. Yeah. And then you want to ask AI to write here the related work section of your publication for you. In the old times we call this a literature review. Have a look who published what in your particular specific domain, when it was published, what were the topics, how was it related to your work. Now everything should be done by the eye. Now this new system is called the deep scholar benchmark and yes it promises to generate here the deep the related work section the deep research on internet on archive on meta archive for you. So what it needs three capabilities you are looking for the retrieval. So you want to have a vast corpus the live internet or the live archive whatever you have then you want to have a synthesis report. So everything all the data that were replied here from multiple sources weave them together the findings into a coherent long form narrative and then you want and this is the most important constraint absolute verifiability. So whatever you are claiming here and whatever you are citing those must really exist. So the citing sources must be accurate on every claim and must be back traced back to its origin. You must have references. Great. So the authors here Stanford and UC Berkeley built here an automated data pipeline directly and they said you know what we not just for the internet let's go here for science let's go to archive. So we want to have a fresh data set retrieved absolutely fresh here daily or every minute here from archive. So how we do this? So three topics the recency the quality control and diversity and here they also limited theirel to computer science as a topic and they made sure that they have a diverse top with 18 different domains in computer science. So yeah we are restricted to computer science but I think you will see the results they speak for themselves. Here's the paper published today August 27, 2025. Deep scholar bench live benchmark on automated evolation for generative research systems from Stanford University in UC Berkeley. And the beauty is not that they have now a new benchmark, but what the new benchmark shows us. It shows us the true difficulty AI systems have if they do have to do a research synthesis. Here's the result. I would say wait a minute explain here at first the metric and the benchmark methodology. Okay. So they decided to go here for a threedimensional differentiation to say we look for the knowledge synthesis of this new system for the retrieval quality. It's more or less a rack system. No. And then for the verifiability because we are working in science with archive. It must be really that we can site real sources and there's no hallucination. Great. And then you can see even in those three dimension they decided to go with two or three sub indicators. So let's have a look. So for the knowledge synthesis or what I call hey how well is this generated EI text written? Is it informative? They have two indicators. the organization where an LLM as a J performs here a pairwise comparison and then they have here a metric that assesses here the informationational completeness. So what you do you have now from your text here a human written um segment here and this is automatically if you want chucked up here by the eye so you get single sentences for example but a human like red at all introduced here the transformer architecture which relies here on a self attention mechanism now the eye might say hey this is a two long sentence there are too many objects too many subjects I reduce this I chunk it up here in nuggets so the first nugget is redford at introduce the transformer stop. This is it. So simple to verify and then it goes on and you see exactly what's happening. Now you have then this essential nuggets and you want to make sure that those really represent here in the I generated text what is the main content of the human written sentence or paragraph or paper. Then second dimension the retrieval quality. Simple in my words. Did the I system find the right papers on archive for my topic. Here we have three indicators. The relevance rate, the reference coverage or simply hey if the system found here the real important papers and you know importance is just one but we can also measure here how impactful this is with an impact um quote. So we have a document importance metric. So this is a proxy for retrieving impactful word that uses here the citation comes from a specific API. Great. And finally the most important for me we have to have verifiability. So whatever we claim there is a check against some other published work. So are the claims now actually supported here by the citation? And here they go with two indicators. the citation precision where they say hey does the cited source support at least one claim made in the eye sentence just one and then they are say hey let's go full power now now for the term claim coverage we want do the cited sources support all the claims made in the eye sentence so it's really every little detail that the eye writes out here supported here by let's say arrive references Great. Then they decided, you know, so we have now the idea of the benchmark, we have the metric and the subindicator of the benchmark. And they say, you know what, now we build a vehicle. So we can switch now the engine of this vehicle. So we can put in either a GPT 4.1 or we can put in a GPD4 Omni or we go with a clawed model as the engine. And they built here the deep scholar base. Now this was interesting because remember I told you this is your almost rag system. No they went here with a classical rag multi multi- aent structure. So at first you have the query generation. So an LLM generates now a set of diverse search query based on the human input. So whatever I have the AI the LLM tells me hey that was a little bit too complicated. I generate now a set of diver search queries and then I send this up. So those queries now AI generated queries now run against the search API in our example archive and then we have a filtering now a semantic filtering and now we use another LLM is used to filter here to retrieve documents and anything that is irrelevant we just want to ignore this no but we have still a lot of documents like say 200 documents left now after the filtering so we have to do a reranking because we want to go down to the top 20 no so we have another llm based step that now reranks in detail to the only 200 remaining documents and it's a new prioritization here. What are the most relevance only of these 200 and then if we have the top 20 we want to have a semantic aggregation of the content of all 20 papers. No. So we say now the final top 20 documents are fed now as a context to an LLM and again you can choose your LLM that you like with a detailed prompt and then synthesization and the LLM writes it the final related works action. So clearly structured data processing pipeline and you can switch on and off any LLM in any place that you want and they tested here variation of different LLMs and they say so what are now the best performance combination so you see pipeline performing research this is great they run this here with a GPD4 as the default and then they exchange and said hey what happens if I add here let's say an 03 or a cloud model to GPD4.1 one all the prompts for this single steps that you just noticed here you have here in detail in the annex here of the documentation of this new research paper like here I show you the revised ODS react agent prompt here for calling here a tool so a lot of additional data I just want to give you the result this is it again you see one two three dimension and then all the different sub indicator in this dimension. Yeah, I think we can forget about the opensource research system from 2014. 
Relevance score: -9.741

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: 85. So I give you here my summary. OpenI is really here the leader in crafting a well ststructured and seemingly comprehensive narrative. Open deep research really excels in this synthesis and in the retrieval task and we have here the numerical data for the highest organization score 85 and the highest nugget coverage 39. output are well organized and capture really the essential fact better than the competitors can perform. And for the retrieval quality also the winner here. If you look here at the relevance rate 62 covers the most important human-sided reference with a reference coins of8 and finds the most impactful paper.1 but and this is now a big but the performance of open research collapses in the third dimension and this is the most crucial aspect if you have some scientific writing trust and verifiability. If you just write something that is not verifiable. 
Relevance score: -10.005

----------------------------------------------------------------
-------------------- New Query at 2025-09-03T22:53:19.537031 -------------

        ## Task
        The user's query: how deepscholar works?

        Answer the user's query based only on the retrieved context.
        Use chat history if relevant, but never go beyond the given context.
        Write a natural, context-based answer.


        ## Context
        Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Great. So verifiability here open research scores shockingly low. This is what Stanford and UC Berkeler tells us. Citation precision numerical value is only 39. The claim coverage is only.13. And they tell us this is one of the worst performers here. that why is this happening? They suggest here a system optimized for fluent plausible sounding text is great on the one hand. So it's excellent in synthesizing here story or story line. But if you work in science, if you want to be absolute precise, you want to reduce hallucinating to the max and you want to make sure making logical leaps that are not strictly supported by the cited sources of archive are not part of your document. Now unfortunately this is happening here with open deep research. So for purely scientific work where you have to have absolute precision this could turn out to be a fatal flaw. Now, interestingly, if we look here at the new matter, the deep scholar base as I just showed you and we put here an 03 as the engine into this benchmark, you see, interestingly for the knowledge synthesis, we have the exact same benchmark 857.857. If we look here at the nugget, we even have a better performance here at the sequential tokenization if you want. And and this is now interesting with this new method. Look at the variability from one we jump to 6. So yeah, wow. This is an impressive improvement against the commercial openi deep research performance. So this new method by Stanford and UC Berkeley even if you use it with an openi model you see in one of the indicators they are at least as good as the commercial models but they outperform the commercial model significantly in variability. Isn't this beautiful? And if you ask yourself hey why I have suddenly two LLMs I told you you can have multiple LLMs here in the pipeline. So they decided for the SAM filter and the SAM top K data they go with GBD 4.1 but for the heavyduty SAM the aggregation step they go here with an 03 or a Claude or a Gemini just to make sure we on the same page. So this is the process. So we have the filter process then the top K the top 20 papers then the final aggregation the final section generation and I told you I show you always the prompt and here you have the prompt. So whatever LLM you choose an 03 or a claude OPUS 4 or a Gemini 2.5 Pro this is here the prompt for the aggregation for the final aggregation here of your research. So now we can see the paper here highlights here the deep scholar base can achieve up to 6.3 times higher claim coverage. So one indicator and one dimension six times three better than openi deep research. Nice. So we do have new systems that really outperform the classical commercial blackbox systems. But on the other hand, if you want just a plausible, well-written draft and you are not in science and you are willing, if you are in science, to meticulously fact check every single claim, every single half sentence that EI writes for you, then if you as a human do this verification and correct it, then open EI is really maybe also interesting. However, if you are a scientist and you say, I will go with the AI system that I can trust at least partially, then the deep scholar base here with claude or gemini model as the engine is dramatically superior. If you look at the data from Stanford and UC Berkeley, the paper states also that no method achieves a score greater than.19 when you average across the metric. So this is not an incremental challenge. You know, go from 78% to 82%. No, we at 19%. This is the best model we have. So we have quite a monumental challenge in front of us to make our AI models better, more performance, more power. So there's a lot of work in front of us. And if you say okay, I don't like here the numerical values in a table. I want to see this here in a visualization. Here you have exactly all the different elements here in um yeah to radar visualization. But this is exactly the data here from the numerical table I just showed you. But if you prefer to have it here in the radar, beautiful. Here is it. 
Relevance score: -9.290

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Hello community so great that you are back. We have a new paper by Stanford University and UC Berkeley and it is all about science and a deep scholar AI system that is brand new. Now you know we have open-source research system no that do a research and synthesize known facts and new findings and you are familiar with this no for example here open scholar this is here the paper or if you go for another open source this is here storm or maybe you are familiar with deep researcher from April 2025 so we do have open systems but of course it's interesting how do they compare to the commercial system let's say openi deep research or we We have brand new search EIS where you remember MCP and here multi- aent system. So how do they compare here to multi- aent search EI systems in well let's say you do a research on the internet. Now for those we have now the study where they compare Llama forced to GPD 4.1 and 03 an Opus 4 by Claude and a Gemini 2.5 Pro. And if you ask me why not some other models, those are the set of models Stanford and UC Berkeley decided to perform their tests on. This is it. The framework they used here is here open deep search. The search with open-source reasoning agents and you remember we have here multi- aent system. We have an open reasoning agent here itself that interprets here the given human task and completes it here by orchestrating a sequence of action and one of those action might include here calling tools and one of those tools that is available to this agent is here the open search tool to connect to the internet and everything. So a multi- aent search utility and now we have a new paper, we have a new method, we have some new research benchmark results and some new ideas. So what is the task that we are looking at? Let's say you're a researcher and you have your title and maybe you have already an idea from a technical abstract. Yeah. And then you want to ask AI to write here the related work section of your publication for you. In the old times we call this a literature review. Have a look who published what in your particular specific domain, when it was published, what were the topics, how was it related to your work. Now everything should be done by the eye. Now this new system is called the deep scholar benchmark and yes it promises to generate here the deep the related work section the deep research on internet on archive on meta archive for you. So what it needs three capabilities you are looking for the retrieval. So you want to have a vast corpus the live internet or the live archive whatever you have then you want to have a synthesis report. So everything all the data that were replied here from multiple sources weave them together the findings into a coherent long form narrative and then you want and this is the most important constraint absolute verifiability. So whatever you are claiming here and whatever you are citing those must really exist. So the citing sources must be accurate on every claim and must be back traced back to its origin. You must have references. Great. So the authors here Stanford and UC Berkeley built here an automated data pipeline directly and they said you know what we not just for the internet let's go here for science let's go to archive. So we want to have a fresh data set retrieved absolutely fresh here daily or every minute here from archive. So how we do this? So three topics the recency the quality control and diversity and here they also limited theirel to computer science as a topic and they made sure that they have a diverse top with 18 different domains in computer science. So yeah we are restricted to computer science but I think you will see the results they speak for themselves. Here's the paper published today August 27, 2025. Deep scholar bench live benchmark on automated evolation for generative research systems from Stanford University in UC Berkeley. And the beauty is not that they have now a new benchmark, but what the new benchmark shows us. It shows us the true difficulty AI systems have if they do have to do a research synthesis. Here's the result. I would say wait a minute explain here at first the metric and the benchmark methodology. Okay. So they decided to go here for a threedimensional differentiation to say we look for the knowledge synthesis of this new system for the retrieval quality. It's more or less a rack system. No. And then for the verifiability because we are working in science with archive. It must be really that we can site real sources and there's no hallucination. Great. And then you can see even in those three dimension they decided to go with two or three sub indicators. So let's have a look. So for the knowledge synthesis or what I call hey how well is this generated EI text written? Is it informative? They have two indicators. the organization where an LLM as a J performs here a pairwise comparison and then they have here a metric that assesses here the informationational completeness. So what you do you have now from your text here a human written um segment here and this is automatically if you want chucked up here by the eye so you get single sentences for example but a human like red at all introduced here the transformer architecture which relies here on a self attention mechanism now the eye might say hey this is a two long sentence there are too many objects too many subjects I reduce this I chunk it up here in nuggets so the first nugget is redford at introduce the transformer stop. This is it. So simple to verify and then it goes on and you see exactly what's happening. Now you have then this essential nuggets and you want to make sure that those really represent here in the I generated text what is the main content of the human written sentence or paragraph or paper. Then second dimension the retrieval quality. Simple in my words. Did the I system find the right papers on archive for my topic. Here we have three indicators. The relevance rate, the reference coverage or simply hey if the system found here the real important papers and you know importance is just one but we can also measure here how impactful this is with an impact um quote. So we have a document importance metric. So this is a proxy for retrieving impactful word that uses here the citation comes from a specific API. Great. And finally the most important for me we have to have verifiability. So whatever we claim there is a check against some other published work. So are the claims now actually supported here by the citation? And here they go with two indicators. the citation precision where they say hey does the cited source support at least one claim made in the eye sentence just one and then they are say hey let's go full power now now for the term claim coverage we want do the cited sources support all the claims made in the eye sentence so it's really every little detail that the eye writes out here supported here by let's say arrive references Great. Then they decided, you know, so we have now the idea of the benchmark, we have the metric and the subindicator of the benchmark. And they say, you know what, now we build a vehicle. So we can switch now the engine of this vehicle. So we can put in either a GPT 4.1 or we can put in a GPD4 Omni or we go with a clawed model as the engine. And they built here the deep scholar base. Now this was interesting because remember I told you this is your almost rag system. No they went here with a classical rag multi multi- aent structure. So at first you have the query generation. So an LLM generates now a set of diverse search query based on the human input. So whatever I have the AI the LLM tells me hey that was a little bit too complicated. I generate now a set of diver search queries and then I send this up. So those queries now AI generated queries now run against the search API in our example archive and then we have a filtering now a semantic filtering and now we use another LLM is used to filter here to retrieve documents and anything that is irrelevant we just want to ignore this no but we have still a lot of documents like say 200 documents left now after the filtering so we have to do a reranking because we want to go down to the top 20 no so we have another llm based step that now reranks in detail to the only 200 remaining documents and it's a new prioritization here. What are the most relevance only of these 200 and then if we have the top 20 we want to have a semantic aggregation of the content of all 20 papers. No. So we say now the final top 20 documents are fed now as a context to an LLM and again you can choose your LLM that you like with a detailed prompt and then synthesization and the LLM writes it the final related works action. So clearly structured data processing pipeline and you can switch on and off any LLM in any place that you want and they tested here variation of different LLMs and they say so what are now the best performance combination so you see pipeline performing research this is great they run this here with a GPD4 as the default and then they exchange and said hey what happens if I add here let's say an 03 or a cloud model to GPD4.1 one all the prompts for this single steps that you just noticed here you have here in detail in the annex here of the documentation of this new research paper like here I show you the revised ODS react agent prompt here for calling here a tool so a lot of additional data I just want to give you the result this is it again you see one two three dimension and then all the different sub indicator in this dimension. Yeah, I think we can forget about the opensource research system from 2014. 
Relevance score: -9.741

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: 85. So I give you here my summary. OpenI is really here the leader in crafting a well ststructured and seemingly comprehensive narrative. Open deep research really excels in this synthesis and in the retrieval task and we have here the numerical data for the highest organization score 85 and the highest nugget coverage 39. output are well organized and capture really the essential fact better than the competitors can perform. And for the retrieval quality also the winner here. If you look here at the relevance rate 62 covers the most important human-sided reference with a reference coins of8 and finds the most impactful paper.1 but and this is now a big but the performance of open research collapses in the third dimension and this is the most crucial aspect if you have some scientific writing trust and verifiability. If you just write something that is not verifiable. 
Relevance score: -10.005

----------------------------------------------------------------
-------------------- New Query at 2025-09-03T23:02:44.165961 -------------

        ## Task
        The user's query: what is deepscholar

        Answer the user's query based only on the retrieved context.
        Use chat history if relevant, but never go beyond the given context.
        Write a natural, context-based answer.


        ## Context
        Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Great. So verifiability here open research scores shockingly low. This is what Stanford and UC Berkeler tells us. Citation precision numerical value is only 39. The claim coverage is only.13. And they tell us this is one of the worst performers here. that why is this happening? They suggest here a system optimized for fluent plausible sounding text is great on the one hand. So it's excellent in synthesizing here story or story line. But if you work in science, if you want to be absolute precise, you want to reduce hallucinating to the max and you want to make sure making logical leaps that are not strictly supported by the cited sources of archive are not part of your document. Now unfortunately this is happening here with open deep research. So for purely scientific work where you have to have absolute precision this could turn out to be a fatal flaw. Now, interestingly, if we look here at the new matter, the deep scholar base as I just showed you and we put here an 03 as the engine into this benchmark, you see, interestingly for the knowledge synthesis, we have the exact same benchmark 857.857. If we look here at the nugget, we even have a better performance here at the sequential tokenization if you want. And and this is now interesting with this new method. Look at the variability from one we jump to 6. So yeah, wow. This is an impressive improvement against the commercial openi deep research performance. So this new method by Stanford and UC Berkeley even if you use it with an openi model you see in one of the indicators they are at least as good as the commercial models but they outperform the commercial model significantly in variability. Isn't this beautiful? And if you ask yourself hey why I have suddenly two LLMs I told you you can have multiple LLMs here in the pipeline. So they decided for the SAM filter and the SAM top K data they go with GBD 4.1 but for the heavyduty SAM the aggregation step they go here with an 03 or a Claude or a Gemini just to make sure we on the same page. So this is the process. So we have the filter process then the top K the top 20 papers then the final aggregation the final section generation and I told you I show you always the prompt and here you have the prompt. So whatever LLM you choose an 03 or a claude OPUS 4 or a Gemini 2.5 Pro this is here the prompt for the aggregation for the final aggregation here of your research. So now we can see the paper here highlights here the deep scholar base can achieve up to 6.3 times higher claim coverage. So one indicator and one dimension six times three better than openi deep research. Nice. So we do have new systems that really outperform the classical commercial blackbox systems. But on the other hand, if you want just a plausible, well-written draft and you are not in science and you are willing, if you are in science, to meticulously fact check every single claim, every single half sentence that EI writes for you, then if you as a human do this verification and correct it, then open EI is really maybe also interesting. However, if you are a scientist and you say, I will go with the AI system that I can trust at least partially, then the deep scholar base here with claude or gemini model as the engine is dramatically superior. If you look at the data from Stanford and UC Berkeley, the paper states also that no method achieves a score greater than.19 when you average across the metric. So this is not an incremental challenge. You know, go from 78% to 82%. No, we at 19%. This is the best model we have. So we have quite a monumental challenge in front of us to make our AI models better, more performance, more power. So there's a lot of work in front of us. And if you say okay, I don't like here the numerical values in a table. I want to see this here in a visualization. Here you have exactly all the different elements here in um yeah to radar visualization. But this is exactly the data here from the numerical table I just showed you. But if you prefer to have it here in the radar, beautiful. Here is it. 
Relevance score: -10.176

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Hello community so great that you are back. We have a new paper by Stanford University and UC Berkeley and it is all about science and a deep scholar AI system that is brand new. Now you know we have open-source research system no that do a research and synthesize known facts and new findings and you are familiar with this no for example here open scholar this is here the paper or if you go for another open source this is here storm or maybe you are familiar with deep researcher from April 2025 so we do have open systems but of course it's interesting how do they compare to the commercial system let's say openi deep research or we We have brand new search EIS where you remember MCP and here multi- aent system. So how do they compare here to multi- aent search EI systems in well let's say you do a research on the internet. Now for those we have now the study where they compare Llama forced to GPD 4.1 and 03 an Opus 4 by Claude and a Gemini 2.5 Pro. And if you ask me why not some other models, those are the set of models Stanford and UC Berkeley decided to perform their tests on. This is it. The framework they used here is here open deep search. The search with open-source reasoning agents and you remember we have here multi- aent system. We have an open reasoning agent here itself that interprets here the given human task and completes it here by orchestrating a sequence of action and one of those action might include here calling tools and one of those tools that is available to this agent is here the open search tool to connect to the internet and everything. So a multi- aent search utility and now we have a new paper, we have a new method, we have some new research benchmark results and some new ideas. So what is the task that we are looking at? Let's say you're a researcher and you have your title and maybe you have already an idea from a technical abstract. Yeah. And then you want to ask AI to write here the related work section of your publication for you. In the old times we call this a literature review. Have a look who published what in your particular specific domain, when it was published, what were the topics, how was it related to your work. Now everything should be done by the eye. Now this new system is called the deep scholar benchmark and yes it promises to generate here the deep the related work section the deep research on internet on archive on meta archive for you. So what it needs three capabilities you are looking for the retrieval. So you want to have a vast corpus the live internet or the live archive whatever you have then you want to have a synthesis report. So everything all the data that were replied here from multiple sources weave them together the findings into a coherent long form narrative and then you want and this is the most important constraint absolute verifiability. So whatever you are claiming here and whatever you are citing those must really exist. So the citing sources must be accurate on every claim and must be back traced back to its origin. You must have references. Great. So the authors here Stanford and UC Berkeley built here an automated data pipeline directly and they said you know what we not just for the internet let's go here for science let's go to archive. So we want to have a fresh data set retrieved absolutely fresh here daily or every minute here from archive. So how we do this? So three topics the recency the quality control and diversity and here they also limited theirel to computer science as a topic and they made sure that they have a diverse top with 18 different domains in computer science. So yeah we are restricted to computer science but I think you will see the results they speak for themselves. Here's the paper published today August 27, 2025. Deep scholar bench live benchmark on automated evolation for generative research systems from Stanford University in UC Berkeley. And the beauty is not that they have now a new benchmark, but what the new benchmark shows us. It shows us the true difficulty AI systems have if they do have to do a research synthesis. Here's the result. I would say wait a minute explain here at first the metric and the benchmark methodology. Okay. So they decided to go here for a threedimensional differentiation to say we look for the knowledge synthesis of this new system for the retrieval quality. It's more or less a rack system. No. And then for the verifiability because we are working in science with archive. It must be really that we can site real sources and there's no hallucination. Great. And then you can see even in those three dimension they decided to go with two or three sub indicators. So let's have a look. So for the knowledge synthesis or what I call hey how well is this generated EI text written? Is it informative? They have two indicators. the organization where an LLM as a J performs here a pairwise comparison and then they have here a metric that assesses here the informationational completeness. So what you do you have now from your text here a human written um segment here and this is automatically if you want chucked up here by the eye so you get single sentences for example but a human like red at all introduced here the transformer architecture which relies here on a self attention mechanism now the eye might say hey this is a two long sentence there are too many objects too many subjects I reduce this I chunk it up here in nuggets so the first nugget is redford at introduce the transformer stop. This is it. So simple to verify and then it goes on and you see exactly what's happening. Now you have then this essential nuggets and you want to make sure that those really represent here in the I generated text what is the main content of the human written sentence or paragraph or paper. Then second dimension the retrieval quality. Simple in my words. Did the I system find the right papers on archive for my topic. Here we have three indicators. The relevance rate, the reference coverage or simply hey if the system found here the real important papers and you know importance is just one but we can also measure here how impactful this is with an impact um quote. So we have a document importance metric. So this is a proxy for retrieving impactful word that uses here the citation comes from a specific API. Great. And finally the most important for me we have to have verifiability. So whatever we claim there is a check against some other published work. So are the claims now actually supported here by the citation? And here they go with two indicators. the citation precision where they say hey does the cited source support at least one claim made in the eye sentence just one and then they are say hey let's go full power now now for the term claim coverage we want do the cited sources support all the claims made in the eye sentence so it's really every little detail that the eye writes out here supported here by let's say arrive references Great. Then they decided, you know, so we have now the idea of the benchmark, we have the metric and the subindicator of the benchmark. And they say, you know what, now we build a vehicle. So we can switch now the engine of this vehicle. So we can put in either a GPT 4.1 or we can put in a GPD4 Omni or we go with a clawed model as the engine. And they built here the deep scholar base. Now this was interesting because remember I told you this is your almost rag system. No they went here with a classical rag multi multi- aent structure. So at first you have the query generation. So an LLM generates now a set of diverse search query based on the human input. So whatever I have the AI the LLM tells me hey that was a little bit too complicated. I generate now a set of diver search queries and then I send this up. So those queries now AI generated queries now run against the search API in our example archive and then we have a filtering now a semantic filtering and now we use another LLM is used to filter here to retrieve documents and anything that is irrelevant we just want to ignore this no but we have still a lot of documents like say 200 documents left now after the filtering so we have to do a reranking because we want to go down to the top 20 no so we have another llm based step that now reranks in detail to the only 200 remaining documents and it's a new prioritization here. What are the most relevance only of these 200 and then if we have the top 20 we want to have a semantic aggregation of the content of all 20 papers. No. So we say now the final top 20 documents are fed now as a context to an LLM and again you can choose your LLM that you like with a detailed prompt and then synthesization and the LLM writes it the final related works action. So clearly structured data processing pipeline and you can switch on and off any LLM in any place that you want and they tested here variation of different LLMs and they say so what are now the best performance combination so you see pipeline performing research this is great they run this here with a GPD4 as the default and then they exchange and said hey what happens if I add here let's say an 03 or a cloud model to GPD4.1 one all the prompts for this single steps that you just noticed here you have here in detail in the annex here of the documentation of this new research paper like here I show you the revised ODS react agent prompt here for calling here a tool so a lot of additional data I just want to give you the result this is it again you see one two three dimension and then all the different sub indicator in this dimension. Yeah, I think we can forget about the opensource research system from 2014. 
Relevance score: -10.642

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: 85. So I give you here my summary. OpenI is really here the leader in crafting a well ststructured and seemingly comprehensive narrative. Open deep research really excels in this synthesis and in the retrieval task and we have here the numerical data for the highest organization score 85 and the highest nugget coverage 39. output are well organized and capture really the essential fact better than the competitors can perform. And for the retrieval quality also the winner here. If you look here at the relevance rate 62 covers the most important human-sided reference with a reference coins of8 and finds the most impactful paper.1 but and this is now a big but the performance of open research collapses in the third dimension and this is the most crucial aspect if you have some scientific writing trust and verifiability. If you just write something that is not verifiable. 
Relevance score: -10.990

----------------------------------------------------------------
-------------------- New Query at 2025-09-03T23:06:15.894322 -------------

        ## Task
        The user's query: what is deepscholar score

        Answer the user's query based only on the retrieved context.
        Use chat history if relevant, but never go beyond the given context.
        Write a natural, context-based answer.


        ## Context
        Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Great. So verifiability here open research scores shockingly low. This is what Stanford and UC Berkeler tells us. Citation precision numerical value is only 39. The claim coverage is only.13. And they tell us this is one of the worst performers here. that why is this happening? They suggest here a system optimized for fluent plausible sounding text is great on the one hand. So it's excellent in synthesizing here story or story line. But if you work in science, if you want to be absolute precise, you want to reduce hallucinating to the max and you want to make sure making logical leaps that are not strictly supported by the cited sources of archive are not part of your document. Now unfortunately this is happening here with open deep research. So for purely scientific work where you have to have absolute precision this could turn out to be a fatal flaw. Now, interestingly, if we look here at the new matter, the deep scholar base as I just showed you and we put here an 03 as the engine into this benchmark, you see, interestingly for the knowledge synthesis, we have the exact same benchmark 857.857. If we look here at the nugget, we even have a better performance here at the sequential tokenization if you want. And and this is now interesting with this new method. Look at the variability from one we jump to 6. So yeah, wow. This is an impressive improvement against the commercial openi deep research performance. So this new method by Stanford and UC Berkeley even if you use it with an openi model you see in one of the indicators they are at least as good as the commercial models but they outperform the commercial model significantly in variability. Isn't this beautiful? And if you ask yourself hey why I have suddenly two LLMs I told you you can have multiple LLMs here in the pipeline. So they decided for the SAM filter and the SAM top K data they go with GBD 4.1 but for the heavyduty SAM the aggregation step they go here with an 03 or a Claude or a Gemini just to make sure we on the same page. So this is the process. So we have the filter process then the top K the top 20 papers then the final aggregation the final section generation and I told you I show you always the prompt and here you have the prompt. So whatever LLM you choose an 03 or a claude OPUS 4 or a Gemini 2.5 Pro this is here the prompt for the aggregation for the final aggregation here of your research. So now we can see the paper here highlights here the deep scholar base can achieve up to 6.3 times higher claim coverage. So one indicator and one dimension six times three better than openi deep research. Nice. So we do have new systems that really outperform the classical commercial blackbox systems. But on the other hand, if you want just a plausible, well-written draft and you are not in science and you are willing, if you are in science, to meticulously fact check every single claim, every single half sentence that EI writes for you, then if you as a human do this verification and correct it, then open EI is really maybe also interesting. However, if you are a scientist and you say, I will go with the AI system that I can trust at least partially, then the deep scholar base here with claude or gemini model as the engine is dramatically superior. If you look at the data from Stanford and UC Berkeley, the paper states also that no method achieves a score greater than.19 when you average across the metric. So this is not an incremental challenge. You know, go from 78% to 82%. No, we at 19%. This is the best model we have. So we have quite a monumental challenge in front of us to make our AI models better, more performance, more power. So there's a lot of work in front of us. And if you say okay, I don't like here the numerical values in a table. I want to see this here in a visualization. Here you have exactly all the different elements here in um yeah to radar visualization. But this is exactly the data here from the numerical table I just showed you. But if you prefer to have it here in the radar, beautiful. Here is it. 
Relevance score: -8.813

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: 85. So I give you here my summary. OpenI is really here the leader in crafting a well ststructured and seemingly comprehensive narrative. Open deep research really excels in this synthesis and in the retrieval task and we have here the numerical data for the highest organization score 85 and the highest nugget coverage 39. output are well organized and capture really the essential fact better than the competitors can perform. And for the retrieval quality also the winner here. If you look here at the relevance rate 62 covers the most important human-sided reference with a reference coins of8 and finds the most impactful paper.1 but and this is now a big but the performance of open research collapses in the third dimension and this is the most crucial aspect if you have some scientific writing trust and verifiability. If you just write something that is not verifiable. 
Relevance score: -9.136

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Hello community so great that you are back. We have a new paper by Stanford University and UC Berkeley and it is all about science and a deep scholar AI system that is brand new. Now you know we have open-source research system no that do a research and synthesize known facts and new findings and you are familiar with this no for example here open scholar this is here the paper or if you go for another open source this is here storm or maybe you are familiar with deep researcher from April 2025 so we do have open systems but of course it's interesting how do they compare to the commercial system let's say openi deep research or we We have brand new search EIS where you remember MCP and here multi- aent system. So how do they compare here to multi- aent search EI systems in well let's say you do a research on the internet. Now for those we have now the study where they compare Llama forced to GPD 4.1 and 03 an Opus 4 by Claude and a Gemini 2.5 Pro. And if you ask me why not some other models, those are the set of models Stanford and UC Berkeley decided to perform their tests on. This is it. The framework they used here is here open deep search. The search with open-source reasoning agents and you remember we have here multi- aent system. We have an open reasoning agent here itself that interprets here the given human task and completes it here by orchestrating a sequence of action and one of those action might include here calling tools and one of those tools that is available to this agent is here the open search tool to connect to the internet and everything. So a multi- aent search utility and now we have a new paper, we have a new method, we have some new research benchmark results and some new ideas. So what is the task that we are looking at? Let's say you're a researcher and you have your title and maybe you have already an idea from a technical abstract. Yeah. And then you want to ask AI to write here the related work section of your publication for you. In the old times we call this a literature review. Have a look who published what in your particular specific domain, when it was published, what were the topics, how was it related to your work. Now everything should be done by the eye. Now this new system is called the deep scholar benchmark and yes it promises to generate here the deep the related work section the deep research on internet on archive on meta archive for you. So what it needs three capabilities you are looking for the retrieval. So you want to have a vast corpus the live internet or the live archive whatever you have then you want to have a synthesis report. So everything all the data that were replied here from multiple sources weave them together the findings into a coherent long form narrative and then you want and this is the most important constraint absolute verifiability. So whatever you are claiming here and whatever you are citing those must really exist. So the citing sources must be accurate on every claim and must be back traced back to its origin. You must have references. Great. So the authors here Stanford and UC Berkeley built here an automated data pipeline directly and they said you know what we not just for the internet let's go here for science let's go to archive. So we want to have a fresh data set retrieved absolutely fresh here daily or every minute here from archive. So how we do this? So three topics the recency the quality control and diversity and here they also limited theirel to computer science as a topic and they made sure that they have a diverse top with 18 different domains in computer science. So yeah we are restricted to computer science but I think you will see the results they speak for themselves. Here's the paper published today August 27, 2025. Deep scholar bench live benchmark on automated evolation for generative research systems from Stanford University in UC Berkeley. And the beauty is not that they have now a new benchmark, but what the new benchmark shows us. It shows us the true difficulty AI systems have if they do have to do a research synthesis. Here's the result. I would say wait a minute explain here at first the metric and the benchmark methodology. Okay. So they decided to go here for a threedimensional differentiation to say we look for the knowledge synthesis of this new system for the retrieval quality. It's more or less a rack system. No. And then for the verifiability because we are working in science with archive. It must be really that we can site real sources and there's no hallucination. Great. And then you can see even in those three dimension they decided to go with two or three sub indicators. So let's have a look. So for the knowledge synthesis or what I call hey how well is this generated EI text written? Is it informative? They have two indicators. the organization where an LLM as a J performs here a pairwise comparison and then they have here a metric that assesses here the informationational completeness. So what you do you have now from your text here a human written um segment here and this is automatically if you want chucked up here by the eye so you get single sentences for example but a human like red at all introduced here the transformer architecture which relies here on a self attention mechanism now the eye might say hey this is a two long sentence there are too many objects too many subjects I reduce this I chunk it up here in nuggets so the first nugget is redford at introduce the transformer stop. This is it. So simple to verify and then it goes on and you see exactly what's happening. Now you have then this essential nuggets and you want to make sure that those really represent here in the I generated text what is the main content of the human written sentence or paragraph or paper. Then second dimension the retrieval quality. Simple in my words. Did the I system find the right papers on archive for my topic. Here we have three indicators. The relevance rate, the reference coverage or simply hey if the system found here the real important papers and you know importance is just one but we can also measure here how impactful this is with an impact um quote. So we have a document importance metric. So this is a proxy for retrieving impactful word that uses here the citation comes from a specific API. Great. And finally the most important for me we have to have verifiability. So whatever we claim there is a check against some other published work. So are the claims now actually supported here by the citation? And here they go with two indicators. the citation precision where they say hey does the cited source support at least one claim made in the eye sentence just one and then they are say hey let's go full power now now for the term claim coverage we want do the cited sources support all the claims made in the eye sentence so it's really every little detail that the eye writes out here supported here by let's say arrive references Great. Then they decided, you know, so we have now the idea of the benchmark, we have the metric and the subindicator of the benchmark. And they say, you know what, now we build a vehicle. So we can switch now the engine of this vehicle. So we can put in either a GPT 4.1 or we can put in a GPD4 Omni or we go with a clawed model as the engine. And they built here the deep scholar base. Now this was interesting because remember I told you this is your almost rag system. No they went here with a classical rag multi multi- aent structure. So at first you have the query generation. So an LLM generates now a set of diverse search query based on the human input. So whatever I have the AI the LLM tells me hey that was a little bit too complicated. I generate now a set of diver search queries and then I send this up. So those queries now AI generated queries now run against the search API in our example archive and then we have a filtering now a semantic filtering and now we use another LLM is used to filter here to retrieve documents and anything that is irrelevant we just want to ignore this no but we have still a lot of documents like say 200 documents left now after the filtering so we have to do a reranking because we want to go down to the top 20 no so we have another llm based step that now reranks in detail to the only 200 remaining documents and it's a new prioritization here. What are the most relevance only of these 200 and then if we have the top 20 we want to have a semantic aggregation of the content of all 20 papers. No. So we say now the final top 20 documents are fed now as a context to an LLM and again you can choose your LLM that you like with a detailed prompt and then synthesization and the LLM writes it the final related works action. So clearly structured data processing pipeline and you can switch on and off any LLM in any place that you want and they tested here variation of different LLMs and they say so what are now the best performance combination so you see pipeline performing research this is great they run this here with a GPD4 as the default and then they exchange and said hey what happens if I add here let's say an 03 or a cloud model to GPD4.1 one all the prompts for this single steps that you just noticed here you have here in detail in the annex here of the documentation of this new research paper like here I show you the revised ODS react agent prompt here for calling here a tool so a lot of additional data I just want to give you the result this is it again you see one two three dimension and then all the different sub indicator in this dimension. Yeah, I think we can forget about the opensource research system from 2014. 
Relevance score: -9.849

----------------------------------------------------------------
-------------------- New Query at 2025-09-03T23:10:58.887209 -------------

        ## Task
        The user's query: score of scholar?

        Answer the user's query based only on the retrieved context.
        Use chat history if relevant, but never go beyond the given context.
        Write a natural, context-based answer.


        ## Context
        Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Great. So verifiability here open research scores shockingly low. This is what Stanford and UC Berkeler tells us. Citation precision numerical value is only 39. The claim coverage is only.13. And they tell us this is one of the worst performers here. that why is this happening? They suggest here a system optimized for fluent plausible sounding text is great on the one hand. So it's excellent in synthesizing here story or story line. But if you work in science, if you want to be absolute precise, you want to reduce hallucinating to the max and you want to make sure making logical leaps that are not strictly supported by the cited sources of archive are not part of your document. Now unfortunately this is happening here with open deep research. So for purely scientific work where you have to have absolute precision this could turn out to be a fatal flaw. Now, interestingly, if we look here at the new matter, the deep scholar base as I just showed you and we put here an 03 as the engine into this benchmark, you see, interestingly for the knowledge synthesis, we have the exact same benchmark 857.857. If we look here at the nugget, we even have a better performance here at the sequential tokenization if you want. And and this is now interesting with this new method. Look at the variability from one we jump to 6. So yeah, wow. This is an impressive improvement against the commercial openi deep research performance. So this new method by Stanford and UC Berkeley even if you use it with an openi model you see in one of the indicators they are at least as good as the commercial models but they outperform the commercial model significantly in variability. Isn't this beautiful? And if you ask yourself hey why I have suddenly two LLMs I told you you can have multiple LLMs here in the pipeline. So they decided for the SAM filter and the SAM top K data they go with GBD 4.1 but for the heavyduty SAM the aggregation step they go here with an 03 or a Claude or a Gemini just to make sure we on the same page. So this is the process. So we have the filter process then the top K the top 20 papers then the final aggregation the final section generation and I told you I show you always the prompt and here you have the prompt. So whatever LLM you choose an 03 or a claude OPUS 4 or a Gemini 2.5 Pro this is here the prompt for the aggregation for the final aggregation here of your research. So now we can see the paper here highlights here the deep scholar base can achieve up to 6.3 times higher claim coverage. So one indicator and one dimension six times three better than openi deep research. Nice. So we do have new systems that really outperform the classical commercial blackbox systems. But on the other hand, if you want just a plausible, well-written draft and you are not in science and you are willing, if you are in science, to meticulously fact check every single claim, every single half sentence that EI writes for you, then if you as a human do this verification and correct it, then open EI is really maybe also interesting. However, if you are a scientist and you say, I will go with the AI system that I can trust at least partially, then the deep scholar base here with claude or gemini model as the engine is dramatically superior. If you look at the data from Stanford and UC Berkeley, the paper states also that no method achieves a score greater than.19 when you average across the metric. So this is not an incremental challenge. You know, go from 78% to 82%. No, we at 19%. This is the best model we have. So we have quite a monumental challenge in front of us to make our AI models better, more performance, more power. So there's a lot of work in front of us. And if you say okay, I don't like here the numerical values in a table. I want to see this here in a visualization. Here you have exactly all the different elements here in um yeah to radar visualization. But this is exactly the data here from the numerical table I just showed you. But if you prefer to have it here in the radar, beautiful. Here is it. 
Relevance score: -7.670

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Hello community so great that you are back. We have a new paper by Stanford University and UC Berkeley and it is all about science and a deep scholar AI system that is brand new. Now you know we have open-source research system no that do a research and synthesize known facts and new findings and you are familiar with this no for example here open scholar this is here the paper or if you go for another open source this is here storm or maybe you are familiar with deep researcher from April 2025 so we do have open systems but of course it's interesting how do they compare to the commercial system let's say openi deep research or we We have brand new search EIS where you remember MCP and here multi- aent system. So how do they compare here to multi- aent search EI systems in well let's say you do a research on the internet. Now for those we have now the study where they compare Llama forced to GPD 4.1 and 03 an Opus 4 by Claude and a Gemini 2.5 Pro. And if you ask me why not some other models, those are the set of models Stanford and UC Berkeley decided to perform their tests on. This is it. The framework they used here is here open deep search. The search with open-source reasoning agents and you remember we have here multi- aent system. We have an open reasoning agent here itself that interprets here the given human task and completes it here by orchestrating a sequence of action and one of those action might include here calling tools and one of those tools that is available to this agent is here the open search tool to connect to the internet and everything. So a multi- aent search utility and now we have a new paper, we have a new method, we have some new research benchmark results and some new ideas. So what is the task that we are looking at? Let's say you're a researcher and you have your title and maybe you have already an idea from a technical abstract. Yeah. And then you want to ask AI to write here the related work section of your publication for you. In the old times we call this a literature review. Have a look who published what in your particular specific domain, when it was published, what were the topics, how was it related to your work. Now everything should be done by the eye. Now this new system is called the deep scholar benchmark and yes it promises to generate here the deep the related work section the deep research on internet on archive on meta archive for you. So what it needs three capabilities you are looking for the retrieval. So you want to have a vast corpus the live internet or the live archive whatever you have then you want to have a synthesis report. So everything all the data that were replied here from multiple sources weave them together the findings into a coherent long form narrative and then you want and this is the most important constraint absolute verifiability. So whatever you are claiming here and whatever you are citing those must really exist. So the citing sources must be accurate on every claim and must be back traced back to its origin. You must have references. Great. So the authors here Stanford and UC Berkeley built here an automated data pipeline directly and they said you know what we not just for the internet let's go here for science let's go to archive. So we want to have a fresh data set retrieved absolutely fresh here daily or every minute here from archive. So how we do this? So three topics the recency the quality control and diversity and here they also limited theirel to computer science as a topic and they made sure that they have a diverse top with 18 different domains in computer science. So yeah we are restricted to computer science but I think you will see the results they speak for themselves. Here's the paper published today August 27, 2025. Deep scholar bench live benchmark on automated evolation for generative research systems from Stanford University in UC Berkeley. And the beauty is not that they have now a new benchmark, but what the new benchmark shows us. It shows us the true difficulty AI systems have if they do have to do a research synthesis. Here's the result. I would say wait a minute explain here at first the metric and the benchmark methodology. Okay. So they decided to go here for a threedimensional differentiation to say we look for the knowledge synthesis of this new system for the retrieval quality. It's more or less a rack system. No. And then for the verifiability because we are working in science with archive. It must be really that we can site real sources and there's no hallucination. Great. And then you can see even in those three dimension they decided to go with two or three sub indicators. So let's have a look. So for the knowledge synthesis or what I call hey how well is this generated EI text written? Is it informative? They have two indicators. the organization where an LLM as a J performs here a pairwise comparison and then they have here a metric that assesses here the informationational completeness. So what you do you have now from your text here a human written um segment here and this is automatically if you want chucked up here by the eye so you get single sentences for example but a human like red at all introduced here the transformer architecture which relies here on a self attention mechanism now the eye might say hey this is a two long sentence there are too many objects too many subjects I reduce this I chunk it up here in nuggets so the first nugget is redford at introduce the transformer stop. This is it. So simple to verify and then it goes on and you see exactly what's happening. Now you have then this essential nuggets and you want to make sure that those really represent here in the I generated text what is the main content of the human written sentence or paragraph or paper. Then second dimension the retrieval quality. Simple in my words. Did the I system find the right papers on archive for my topic. Here we have three indicators. The relevance rate, the reference coverage or simply hey if the system found here the real important papers and you know importance is just one but we can also measure here how impactful this is with an impact um quote. So we have a document importance metric. So this is a proxy for retrieving impactful word that uses here the citation comes from a specific API. Great. And finally the most important for me we have to have verifiability. So whatever we claim there is a check against some other published work. So are the claims now actually supported here by the citation? And here they go with two indicators. the citation precision where they say hey does the cited source support at least one claim made in the eye sentence just one and then they are say hey let's go full power now now for the term claim coverage we want do the cited sources support all the claims made in the eye sentence so it's really every little detail that the eye writes out here supported here by let's say arrive references Great. Then they decided, you know, so we have now the idea of the benchmark, we have the metric and the subindicator of the benchmark. And they say, you know what, now we build a vehicle. So we can switch now the engine of this vehicle. So we can put in either a GPT 4.1 or we can put in a GPD4 Omni or we go with a clawed model as the engine. And they built here the deep scholar base. Now this was interesting because remember I told you this is your almost rag system. No they went here with a classical rag multi multi- aent structure. So at first you have the query generation. So an LLM generates now a set of diverse search query based on the human input. So whatever I have the AI the LLM tells me hey that was a little bit too complicated. I generate now a set of diver search queries and then I send this up. So those queries now AI generated queries now run against the search API in our example archive and then we have a filtering now a semantic filtering and now we use another LLM is used to filter here to retrieve documents and anything that is irrelevant we just want to ignore this no but we have still a lot of documents like say 200 documents left now after the filtering so we have to do a reranking because we want to go down to the top 20 no so we have another llm based step that now reranks in detail to the only 200 remaining documents and it's a new prioritization here. What are the most relevance only of these 200 and then if we have the top 20 we want to have a semantic aggregation of the content of all 20 papers. No. So we say now the final top 20 documents are fed now as a context to an LLM and again you can choose your LLM that you like with a detailed prompt and then synthesization and the LLM writes it the final related works action. So clearly structured data processing pipeline and you can switch on and off any LLM in any place that you want and they tested here variation of different LLMs and they say so what are now the best performance combination so you see pipeline performing research this is great they run this here with a GPD4 as the default and then they exchange and said hey what happens if I add here let's say an 03 or a cloud model to GPD4.1 one all the prompts for this single steps that you just noticed here you have here in detail in the annex here of the documentation of this new research paper like here I show you the revised ODS react agent prompt here for calling here a tool so a lot of additional data I just want to give you the result this is it again you see one two three dimension and then all the different sub indicator in this dimension. Yeah, I think we can forget about the opensource research system from 2014. 
Relevance score: -9.264

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: 85. So I give you here my summary. OpenI is really here the leader in crafting a well ststructured and seemingly comprehensive narrative. Open deep research really excels in this synthesis and in the retrieval task and we have here the numerical data for the highest organization score 85 and the highest nugget coverage 39. output are well organized and capture really the essential fact better than the competitors can perform. And for the retrieval quality also the winner here. If you look here at the relevance rate 62 covers the most important human-sided reference with a reference coins of8 and finds the most impactful paper.1 but and this is now a big but the performance of open research collapses in the third dimension and this is the most crucial aspect if you have some scientific writing trust and verifiability. If you just write something that is not verifiable. 
Relevance score: -9.271

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: And there we have it. A new research paper by Stanford University UC Berkeley about AI doing here the research the literature review here on brand new topics live in a multi- aent network deep scholar to research and synthesize knowledge. And if you are in science this could be a real benefit, a real help. 
Relevance score: -10.025

----------------------------------------------------------------
---------- Query at 2025-09-04T00:46:23.337821 ----------

        ## Task
        The user's query: what is deepscholar AI?

        Answer the user's query based only on the retrieved context.
        Use chat history if relevant, but never go beyond the given context.
        Write a natural, context-based answer.


        ## Context
        Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: And there we have it. A new research paper by Stanford University UC Berkeley about AI doing here the research the literature review here on brand new topics live in a multi- aent network deep scholar to research and synthesize knowledge. And if you are in science this could be a real benefit, a real help. 
Relevance score: -9.896

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Hello community so great that you are back. We have a new paper by Stanford University and UC Berkeley and it is all about science and a deep scholar AI system that is brand new. Now you know we have open-source research system no that do a research and synthesize known facts and new findings and you are familiar with this no for example here open scholar this is here the paper or if you go for another open source this is here storm or maybe you are familiar with deep researcher from April 2025 so we do have open systems but of course it's interesting how do they compare to the commercial system let's say openi deep research or we We have brand new search EIS where you remember MCP and here multi- aent system. So how do they compare here to multi- aent search EI systems in well let's say you do a research on the internet. Now for those we have now the study where they compare Llama forced to GPD 4.1 and 03 an Opus 4 by Claude and a Gemini 2.5 Pro. And if you ask me why not some other models, those are the set of models Stanford and UC Berkeley decided to perform their tests on. This is it. The framework they used here is here open deep search. The search with open-source reasoning agents and you remember we have here multi- aent system. We have an open reasoning agent here itself that interprets here the given human task and completes it here by orchestrating a sequence of action and one of those action might include here calling tools and one of those tools that is available to this agent is here the open search tool to connect to the internet and everything. So a multi- aent search utility and now we have a new paper, we have a new method, we have some new research benchmark results and some new ideas. So what is the task that we are looking at? Let's say you're a researcher and you have your title and maybe you have already an idea from a technical abstract. Yeah. And then you want to ask AI to write here the related work section of your publication for you. In the old times we call this a literature review. Have a look who published what in your particular specific domain, when it was published, what were the topics, how was it related to your work. Now everything should be done by the eye. Now this new system is called the deep scholar benchmark and yes it promises to generate here the deep the related work section the deep research on internet on archive on meta archive for you. So what it needs three capabilities you are looking for the retrieval. So you want to have a vast corpus the live internet or the live archive whatever you have then you want to have a synthesis report. So everything all the data that were replied here from multiple sources weave them together the findings into a coherent long form narrative and then you want and this is the most important constraint absolute verifiability. So whatever you are claiming here and whatever you are citing those must really exist. So the citing sources must be accurate on every claim and must be back traced back to its origin. You must have references. Great. So the authors here Stanford and UC Berkeley built here an automated data pipeline directly and they said you know what we not just for the internet let's go here for science let's go to archive. So we want to have a fresh data set retrieved absolutely fresh here daily or every minute here from archive. So how we do this? So three topics the recency the quality control and diversity and here they also limited theirel to computer science as a topic and they made sure that they have a diverse top with 18 different domains in computer science. So yeah we are restricted to computer science but I think you will see the results they speak for themselves. Here's the paper published today August 27, 2025. Deep scholar bench live benchmark on automated evolation for generative research systems from Stanford University in UC Berkeley. And the beauty is not that they have now a new benchmark, but what the new benchmark shows us. It shows us the true difficulty AI systems have if they do have to do a research synthesis. Here's the result. I would say wait a minute explain here at first the metric and the benchmark methodology. Okay. So they decided to go here for a threedimensional differentiation to say we look for the knowledge synthesis of this new system for the retrieval quality. It's more or less a rack system. No. And then for the verifiability because we are working in science with archive. It must be really that we can site real sources and there's no hallucination. Great. And then you can see even in those three dimension they decided to go with two or three sub indicators. So let's have a look. So for the knowledge synthesis or what I call hey how well is this generated EI text written? Is it informative? They have two indicators. the organization where an LLM as a J performs here a pairwise comparison and then they have here a metric that assesses here the informationational completeness. So what you do you have now from your text here a human written um segment here and this is automatically if you want chucked up here by the eye so you get single sentences for example but a human like red at all introduced here the transformer architecture which relies here on a self attention mechanism now the eye might say hey this is a two long sentence there are too many objects too many subjects I reduce this I chunk it up here in nuggets so the first nugget is redford at introduce the transformer stop. This is it. So simple to verify and then it goes on and you see exactly what's happening. Now you have then this essential nuggets and you want to make sure that those really represent here in the I generated text what is the main content of the human written sentence or paragraph or paper. Then second dimension the retrieval quality. Simple in my words. Did the I system find the right papers on archive for my topic. Here we have three indicators. The relevance rate, the reference coverage or simply hey if the system found here the real important papers and you know importance is just one but we can also measure here how impactful this is with an impact um quote. So we have a document importance metric. So this is a proxy for retrieving impactful word that uses here the citation comes from a specific API. Great. And finally the most important for me we have to have verifiability. So whatever we claim there is a check against some other published work. So are the claims now actually supported here by the citation? And here they go with two indicators. the citation precision where they say hey does the cited source support at least one claim made in the eye sentence just one and then they are say hey let's go full power now now for the term claim coverage we want do the cited sources support all the claims made in the eye sentence so it's really every little detail that the eye writes out here supported here by let's say arrive references Great. Then they decided, you know, so we have now the idea of the benchmark, we have the metric and the subindicator of the benchmark. And they say, you know what, now we build a vehicle. So we can switch now the engine of this vehicle. So we can put in either a GPT 4.1 or we can put in a GPD4 Omni or we go with a clawed model as the engine. And they built here the deep scholar base. Now this was interesting because remember I told you this is your almost rag system. No they went here with a classical rag multi multi- aent structure. So at first you have the query generation. So an LLM generates now a set of diverse search query based on the human input. So whatever I have the AI the LLM tells me hey that was a little bit too complicated. I generate now a set of diver search queries and then I send this up. So those queries now AI generated queries now run against the search API in our example archive and then we have a filtering now a semantic filtering and now we use another LLM is used to filter here to retrieve documents and anything that is irrelevant we just want to ignore this no but we have still a lot of documents like say 200 documents left now after the filtering so we have to do a reranking because we want to go down to the top 20 no so we have another llm based step that now reranks in detail to the only 200 remaining documents and it's a new prioritization here. What are the most relevance only of these 200 and then if we have the top 20 we want to have a semantic aggregation of the content of all 20 papers. No. So we say now the final top 20 documents are fed now as a context to an LLM and again you can choose your LLM that you like with a detailed prompt and then synthesization and the LLM writes it the final related works action. So clearly structured data processing pipeline and you can switch on and off any LLM in any place that you want and they tested here variation of different LLMs and they say so what are now the best performance combination so you see pipeline performing research this is great they run this here with a GPD4 as the default and then they exchange and said hey what happens if I add here let's say an 03 or a cloud model to GPD4.1 one all the prompts for this single steps that you just noticed here you have here in detail in the annex here of the documentation of this new research paper like here I show you the revised ODS react agent prompt here for calling here a tool so a lot of additional data I just want to give you the result this is it again you see one two three dimension and then all the different sub indicator in this dimension. Yeah, I think we can forget about the opensource research system from 2014. 
Relevance score: -10.105

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Great. So verifiability here open research scores shockingly low. This is what Stanford and UC Berkeler tells us. Citation precision numerical value is only 39. The claim coverage is only.13. And they tell us this is one of the worst performers here. that why is this happening? They suggest here a system optimized for fluent plausible sounding text is great on the one hand. So it's excellent in synthesizing here story or story line. But if you work in science, if you want to be absolute precise, you want to reduce hallucinating to the max and you want to make sure making logical leaps that are not strictly supported by the cited sources of archive are not part of your document. Now unfortunately this is happening here with open deep research. So for purely scientific work where you have to have absolute precision this could turn out to be a fatal flaw. Now, interestingly, if we look here at the new matter, the deep scholar base as I just showed you and we put here an 03 as the engine into this benchmark, you see, interestingly for the knowledge synthesis, we have the exact same benchmark 857.857. If we look here at the nugget, we even have a better performance here at the sequential tokenization if you want. And and this is now interesting with this new method. Look at the variability from one we jump to 6. So yeah, wow. This is an impressive improvement against the commercial openi deep research performance. So this new method by Stanford and UC Berkeley even if you use it with an openi model you see in one of the indicators they are at least as good as the commercial models but they outperform the commercial model significantly in variability. Isn't this beautiful? And if you ask yourself hey why I have suddenly two LLMs I told you you can have multiple LLMs here in the pipeline. So they decided for the SAM filter and the SAM top K data they go with GBD 4.1 but for the heavyduty SAM the aggregation step they go here with an 03 or a Claude or a Gemini just to make sure we on the same page. So this is the process. So we have the filter process then the top K the top 20 papers then the final aggregation the final section generation and I told you I show you always the prompt and here you have the prompt. So whatever LLM you choose an 03 or a claude OPUS 4 or a Gemini 2.5 Pro this is here the prompt for the aggregation for the final aggregation here of your research. So now we can see the paper here highlights here the deep scholar base can achieve up to 6.3 times higher claim coverage. So one indicator and one dimension six times three better than openi deep research. Nice. So we do have new systems that really outperform the classical commercial blackbox systems. But on the other hand, if you want just a plausible, well-written draft and you are not in science and you are willing, if you are in science, to meticulously fact check every single claim, every single half sentence that EI writes for you, then if you as a human do this verification and correct it, then open EI is really maybe also interesting. However, if you are a scientist and you say, I will go with the AI system that I can trust at least partially, then the deep scholar base here with claude or gemini model as the engine is dramatically superior. If you look at the data from Stanford and UC Berkeley, the paper states also that no method achieves a score greater than.19 when you average across the metric. So this is not an incremental challenge. You know, go from 78% to 82%. No, we at 19%. This is the best model we have. So we have quite a monumental challenge in front of us to make our AI models better, more performance, more power. So there's a lot of work in front of us. And if you say okay, I don't like here the numerical values in a table. I want to see this here in a visualization. Here you have exactly all the different elements here in um yeah to radar visualization. But this is exactly the data here from the numerical table I just showed you. But if you prefer to have it here in the radar, beautiful. Here is it. 
Relevance score: -10.633

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Not up to challenge anymore. Let's look at a commercial system before we go to the pure search. AI commercial system is they tested here one. They said this is kind of one of the best OpenIP research. And if you look at the knowledge synthesis, wow, this is really impressive here. 
Relevance score: -10.567

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: 85. So I give you here my summary. OpenI is really here the leader in crafting a well ststructured and seemingly comprehensive narrative. Open deep research really excels in this synthesis and in the retrieval task and we have here the numerical data for the highest organization score 85 and the highest nugget coverage 39. output are well organized and capture really the essential fact better than the competitors can perform. And for the retrieval quality also the winner here. If you look here at the relevance rate 62 covers the most important human-sided reference with a reference coins of8 and finds the most impactful paper.1 but and this is now a big but the performance of open research collapses in the third dimension and this is the most crucial aspect if you have some scientific writing trust and verifiability. If you just write something that is not verifiable. 
Relevance score: -10.590

------------------------------------------------------------
---------- Query at 2025-09-04T00:52:30.032666 ----------

        ## Task
        The user's query: how to increase retrieval quality

        Answer the user's query based only on the retrieved context.
        Use chat history if relevant, but never go beyond the given context.
        Write a natural, context-based answer.


        ## Context
        Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: 85. So I give you here my summary. OpenI is really here the leader in crafting a well ststructured and seemingly comprehensive narrative. Open deep research really excels in this synthesis and in the retrieval task and we have here the numerical data for the highest organization score 85 and the highest nugget coverage 39. output are well organized and capture really the essential fact better than the competitors can perform. And for the retrieval quality also the winner here. If you look here at the relevance rate 62 covers the most important human-sided reference with a reference coins of8 and finds the most impactful paper.1 but and this is now a big but the performance of open research collapses in the third dimension and this is the most crucial aspect if you have some scientific writing trust and verifiability. If you just write something that is not verifiable. 
Relevance score: -9.356

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Great. So verifiability here open research scores shockingly low. This is what Stanford and UC Berkeler tells us. Citation precision numerical value is only 39. The claim coverage is only.13. And they tell us this is one of the worst performers here. that why is this happening? They suggest here a system optimized for fluent plausible sounding text is great on the one hand. So it's excellent in synthesizing here story or story line. But if you work in science, if you want to be absolute precise, you want to reduce hallucinating to the max and you want to make sure making logical leaps that are not strictly supported by the cited sources of archive are not part of your document. Now unfortunately this is happening here with open deep research. So for purely scientific work where you have to have absolute precision this could turn out to be a fatal flaw. Now, interestingly, if we look here at the new matter, the deep scholar base as I just showed you and we put here an 03 as the engine into this benchmark, you see, interestingly for the knowledge synthesis, we have the exact same benchmark 857.857. If we look here at the nugget, we even have a better performance here at the sequential tokenization if you want. And and this is now interesting with this new method. Look at the variability from one we jump to 6. So yeah, wow. This is an impressive improvement against the commercial openi deep research performance. So this new method by Stanford and UC Berkeley even if you use it with an openi model you see in one of the indicators they are at least as good as the commercial models but they outperform the commercial model significantly in variability. Isn't this beautiful? And if you ask yourself hey why I have suddenly two LLMs I told you you can have multiple LLMs here in the pipeline. So they decided for the SAM filter and the SAM top K data they go with GBD 4.1 but for the heavyduty SAM the aggregation step they go here with an 03 or a Claude or a Gemini just to make sure we on the same page. So this is the process. So we have the filter process then the top K the top 20 papers then the final aggregation the final section generation and I told you I show you always the prompt and here you have the prompt. So whatever LLM you choose an 03 or a claude OPUS 4 or a Gemini 2.5 Pro this is here the prompt for the aggregation for the final aggregation here of your research. So now we can see the paper here highlights here the deep scholar base can achieve up to 6.3 times higher claim coverage. So one indicator and one dimension six times three better than openi deep research. Nice. So we do have new systems that really outperform the classical commercial blackbox systems. But on the other hand, if you want just a plausible, well-written draft and you are not in science and you are willing, if you are in science, to meticulously fact check every single claim, every single half sentence that EI writes for you, then if you as a human do this verification and correct it, then open EI is really maybe also interesting. However, if you are a scientist and you say, I will go with the AI system that I can trust at least partially, then the deep scholar base here with claude or gemini model as the engine is dramatically superior. If you look at the data from Stanford and UC Berkeley, the paper states also that no method achieves a score greater than.19 when you average across the metric. So this is not an incremental challenge. You know, go from 78% to 82%. No, we at 19%. This is the best model we have. So we have quite a monumental challenge in front of us to make our AI models better, more performance, more power. So there's a lot of work in front of us. And if you say okay, I don't like here the numerical values in a table. I want to see this here in a visualization. Here you have exactly all the different elements here in um yeah to radar visualization. But this is exactly the data here from the numerical table I just showed you. But if you prefer to have it here in the radar, beautiful. Here is it. 
Relevance score: -11.048

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Hello community so great that you are back. We have a new paper by Stanford University and UC Berkeley and it is all about science and a deep scholar AI system that is brand new. Now you know we have open-source research system no that do a research and synthesize known facts and new findings and you are familiar with this no for example here open scholar this is here the paper or if you go for another open source this is here storm or maybe you are familiar with deep researcher from April 2025 so we do have open systems but of course it's interesting how do they compare to the commercial system let's say openi deep research or we We have brand new search EIS where you remember MCP and here multi- aent system. So how do they compare here to multi- aent search EI systems in well let's say you do a research on the internet. Now for those we have now the study where they compare Llama forced to GPD 4.1 and 03 an Opus 4 by Claude and a Gemini 2.5 Pro. And if you ask me why not some other models, those are the set of models Stanford and UC Berkeley decided to perform their tests on. This is it. The framework they used here is here open deep search. The search with open-source reasoning agents and you remember we have here multi- aent system. We have an open reasoning agent here itself that interprets here the given human task and completes it here by orchestrating a sequence of action and one of those action might include here calling tools and one of those tools that is available to this agent is here the open search tool to connect to the internet and everything. So a multi- aent search utility and now we have a new paper, we have a new method, we have some new research benchmark results and some new ideas. So what is the task that we are looking at? Let's say you're a researcher and you have your title and maybe you have already an idea from a technical abstract. Yeah. And then you want to ask AI to write here the related work section of your publication for you. In the old times we call this a literature review. Have a look who published what in your particular specific domain, when it was published, what were the topics, how was it related to your work. Now everything should be done by the eye. Now this new system is called the deep scholar benchmark and yes it promises to generate here the deep the related work section the deep research on internet on archive on meta archive for you. So what it needs three capabilities you are looking for the retrieval. So you want to have a vast corpus the live internet or the live archive whatever you have then you want to have a synthesis report. So everything all the data that were replied here from multiple sources weave them together the findings into a coherent long form narrative and then you want and this is the most important constraint absolute verifiability. So whatever you are claiming here and whatever you are citing those must really exist. So the citing sources must be accurate on every claim and must be back traced back to its origin. You must have references. Great. So the authors here Stanford and UC Berkeley built here an automated data pipeline directly and they said you know what we not just for the internet let's go here for science let's go to archive. So we want to have a fresh data set retrieved absolutely fresh here daily or every minute here from archive. So how we do this? So three topics the recency the quality control and diversity and here they also limited theirel to computer science as a topic and they made sure that they have a diverse top with 18 different domains in computer science. So yeah we are restricted to computer science but I think you will see the results they speak for themselves. Here's the paper published today August 27, 2025. Deep scholar bench live benchmark on automated evolation for generative research systems from Stanford University in UC Berkeley. And the beauty is not that they have now a new benchmark, but what the new benchmark shows us. It shows us the true difficulty AI systems have if they do have to do a research synthesis. Here's the result. I would say wait a minute explain here at first the metric and the benchmark methodology. Okay. So they decided to go here for a threedimensional differentiation to say we look for the knowledge synthesis of this new system for the retrieval quality. It's more or less a rack system. No. And then for the verifiability because we are working in science with archive. It must be really that we can site real sources and there's no hallucination. Great. And then you can see even in those three dimension they decided to go with two or three sub indicators. So let's have a look. So for the knowledge synthesis or what I call hey how well is this generated EI text written? Is it informative? They have two indicators. the organization where an LLM as a J performs here a pairwise comparison and then they have here a metric that assesses here the informationational completeness. So what you do you have now from your text here a human written um segment here and this is automatically if you want chucked up here by the eye so you get single sentences for example but a human like red at all introduced here the transformer architecture which relies here on a self attention mechanism now the eye might say hey this is a two long sentence there are too many objects too many subjects I reduce this I chunk it up here in nuggets so the first nugget is redford at introduce the transformer stop. This is it. So simple to verify and then it goes on and you see exactly what's happening. Now you have then this essential nuggets and you want to make sure that those really represent here in the I generated text what is the main content of the human written sentence or paragraph or paper. Then second dimension the retrieval quality. Simple in my words. Did the I system find the right papers on archive for my topic. Here we have three indicators. The relevance rate, the reference coverage or simply hey if the system found here the real important papers and you know importance is just one but we can also measure here how impactful this is with an impact um quote. So we have a document importance metric. So this is a proxy for retrieving impactful word that uses here the citation comes from a specific API. Great. And finally the most important for me we have to have verifiability. So whatever we claim there is a check against some other published work. So are the claims now actually supported here by the citation? And here they go with two indicators. the citation precision where they say hey does the cited source support at least one claim made in the eye sentence just one and then they are say hey let's go full power now now for the term claim coverage we want do the cited sources support all the claims made in the eye sentence so it's really every little detail that the eye writes out here supported here by let's say arrive references Great. Then they decided, you know, so we have now the idea of the benchmark, we have the metric and the subindicator of the benchmark. And they say, you know what, now we build a vehicle. So we can switch now the engine of this vehicle. So we can put in either a GPT 4.1 or we can put in a GPD4 Omni or we go with a clawed model as the engine. And they built here the deep scholar base. Now this was interesting because remember I told you this is your almost rag system. No they went here with a classical rag multi multi- aent structure. So at first you have the query generation. So an LLM generates now a set of diverse search query based on the human input. So whatever I have the AI the LLM tells me hey that was a little bit too complicated. I generate now a set of diver search queries and then I send this up. So those queries now AI generated queries now run against the search API in our example archive and then we have a filtering now a semantic filtering and now we use another LLM is used to filter here to retrieve documents and anything that is irrelevant we just want to ignore this no but we have still a lot of documents like say 200 documents left now after the filtering so we have to do a reranking because we want to go down to the top 20 no so we have another llm based step that now reranks in detail to the only 200 remaining documents and it's a new prioritization here. What are the most relevance only of these 200 and then if we have the top 20 we want to have a semantic aggregation of the content of all 20 papers. No. So we say now the final top 20 documents are fed now as a context to an LLM and again you can choose your LLM that you like with a detailed prompt and then synthesization and the LLM writes it the final related works action. So clearly structured data processing pipeline and you can switch on and off any LLM in any place that you want and they tested here variation of different LLMs and they say so what are now the best performance combination so you see pipeline performing research this is great they run this here with a GPD4 as the default and then they exchange and said hey what happens if I add here let's say an 03 or a cloud model to GPD4.1 one all the prompts for this single steps that you just noticed here you have here in detail in the annex here of the documentation of this new research paper like here I show you the revised ODS react agent prompt here for calling here a tool so a lot of additional data I just want to give you the result this is it again you see one two three dimension and then all the different sub indicator in this dimension. Yeah, I think we can forget about the opensource research system from 2014. 
Relevance score: -11.318

------------------------------------------------------------
---------- Query at 2025-09-04T00:58:19.736162 ----------

        ## Task
        The user's query: what is score

        Answer the user's query based only on the retrieved context.
        Use chat history if relevant, but never go beyond the given context.
        Write a natural, context-based answer.


        ## Context
        Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Great. So verifiability here open research scores shockingly low. This is what Stanford and UC Berkeler tells us. Citation precision numerical value is only 39. The claim coverage is only.13. And they tell us this is one of the worst performers here. that why is this happening? They suggest here a system optimized for fluent plausible sounding text is great on the one hand. So it's excellent in synthesizing here story or story line. But if you work in science, if you want to be absolute precise, you want to reduce hallucinating to the max and you want to make sure making logical leaps that are not strictly supported by the cited sources of archive are not part of your document. Now unfortunately this is happening here with open deep research. So for purely scientific work where you have to have absolute precision this could turn out to be a fatal flaw. Now, interestingly, if we look here at the new matter, the deep scholar base as I just showed you and we put here an 03 as the engine into this benchmark, you see, interestingly for the knowledge synthesis, we have the exact same benchmark 857.857. If we look here at the nugget, we even have a better performance here at the sequential tokenization if you want. And and this is now interesting with this new method. Look at the variability from one we jump to 6. So yeah, wow. This is an impressive improvement against the commercial openi deep research performance. So this new method by Stanford and UC Berkeley even if you use it with an openi model you see in one of the indicators they are at least as good as the commercial models but they outperform the commercial model significantly in variability. Isn't this beautiful? And if you ask yourself hey why I have suddenly two LLMs I told you you can have multiple LLMs here in the pipeline. So they decided for the SAM filter and the SAM top K data they go with GBD 4.1 but for the heavyduty SAM the aggregation step they go here with an 03 or a Claude or a Gemini just to make sure we on the same page. So this is the process. So we have the filter process then the top K the top 20 papers then the final aggregation the final section generation and I told you I show you always the prompt and here you have the prompt. So whatever LLM you choose an 03 or a claude OPUS 4 or a Gemini 2.5 Pro this is here the prompt for the aggregation for the final aggregation here of your research. So now we can see the paper here highlights here the deep scholar base can achieve up to 6.3 times higher claim coverage. So one indicator and one dimension six times three better than openi deep research. Nice. So we do have new systems that really outperform the classical commercial blackbox systems. But on the other hand, if you want just a plausible, well-written draft and you are not in science and you are willing, if you are in science, to meticulously fact check every single claim, every single half sentence that EI writes for you, then if you as a human do this verification and correct it, then open EI is really maybe also interesting. However, if you are a scientist and you say, I will go with the AI system that I can trust at least partially, then the deep scholar base here with claude or gemini model as the engine is dramatically superior. If you look at the data from Stanford and UC Berkeley, the paper states also that no method achieves a score greater than.19 when you average across the metric. So this is not an incremental challenge. You know, go from 78% to 82%. No, we at 19%. This is the best model we have. So we have quite a monumental challenge in front of us to make our AI models better, more performance, more power. So there's a lot of work in front of us. And if you say okay, I don't like here the numerical values in a table. I want to see this here in a visualization. Here you have exactly all the different elements here in um yeah to radar visualization. But this is exactly the data here from the numerical table I just showed you. But if you prefer to have it here in the radar, beautiful. Here is it. 
Relevance score: -8.488

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: 85. So I give you here my summary. OpenI is really here the leader in crafting a well ststructured and seemingly comprehensive narrative. Open deep research really excels in this synthesis and in the retrieval task and we have here the numerical data for the highest organization score 85 and the highest nugget coverage 39. output are well organized and capture really the essential fact better than the competitors can perform. And for the retrieval quality also the winner here. If you look here at the relevance rate 62 covers the most important human-sided reference with a reference coins of8 and finds the most impactful paper.1 but and this is now a big but the performance of open research collapses in the third dimension and this is the most crucial aspect if you have some scientific writing trust and verifiability. If you just write something that is not verifiable. 
Relevance score: -8.673

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Hello community so great that you are back. We have a new paper by Stanford University and UC Berkeley and it is all about science and a deep scholar AI system that is brand new. Now you know we have open-source research system no that do a research and synthesize known facts and new findings and you are familiar with this no for example here open scholar this is here the paper or if you go for another open source this is here storm or maybe you are familiar with deep researcher from April 2025 so we do have open systems but of course it's interesting how do they compare to the commercial system let's say openi deep research or we We have brand new search EIS where you remember MCP and here multi- aent system. So how do they compare here to multi- aent search EI systems in well let's say you do a research on the internet. Now for those we have now the study where they compare Llama forced to GPD 4.1 and 03 an Opus 4 by Claude and a Gemini 2.5 Pro. And if you ask me why not some other models, those are the set of models Stanford and UC Berkeley decided to perform their tests on. This is it. The framework they used here is here open deep search. The search with open-source reasoning agents and you remember we have here multi- aent system. We have an open reasoning agent here itself that interprets here the given human task and completes it here by orchestrating a sequence of action and one of those action might include here calling tools and one of those tools that is available to this agent is here the open search tool to connect to the internet and everything. So a multi- aent search utility and now we have a new paper, we have a new method, we have some new research benchmark results and some new ideas. So what is the task that we are looking at? Let's say you're a researcher and you have your title and maybe you have already an idea from a technical abstract. Yeah. And then you want to ask AI to write here the related work section of your publication for you. In the old times we call this a literature review. Have a look who published what in your particular specific domain, when it was published, what were the topics, how was it related to your work. Now everything should be done by the eye. Now this new system is called the deep scholar benchmark and yes it promises to generate here the deep the related work section the deep research on internet on archive on meta archive for you. So what it needs three capabilities you are looking for the retrieval. So you want to have a vast corpus the live internet or the live archive whatever you have then you want to have a synthesis report. So everything all the data that were replied here from multiple sources weave them together the findings into a coherent long form narrative and then you want and this is the most important constraint absolute verifiability. So whatever you are claiming here and whatever you are citing those must really exist. So the citing sources must be accurate on every claim and must be back traced back to its origin. You must have references. Great. So the authors here Stanford and UC Berkeley built here an automated data pipeline directly and they said you know what we not just for the internet let's go here for science let's go to archive. So we want to have a fresh data set retrieved absolutely fresh here daily or every minute here from archive. So how we do this? So three topics the recency the quality control and diversity and here they also limited theirel to computer science as a topic and they made sure that they have a diverse top with 18 different domains in computer science. So yeah we are restricted to computer science but I think you will see the results they speak for themselves. Here's the paper published today August 27, 2025. Deep scholar bench live benchmark on automated evolation for generative research systems from Stanford University in UC Berkeley. And the beauty is not that they have now a new benchmark, but what the new benchmark shows us. It shows us the true difficulty AI systems have if they do have to do a research synthesis. Here's the result. I would say wait a minute explain here at first the metric and the benchmark methodology. Okay. So they decided to go here for a threedimensional differentiation to say we look for the knowledge synthesis of this new system for the retrieval quality. It's more or less a rack system. No. And then for the verifiability because we are working in science with archive. It must be really that we can site real sources and there's no hallucination. Great. And then you can see even in those three dimension they decided to go with two or three sub indicators. So let's have a look. So for the knowledge synthesis or what I call hey how well is this generated EI text written? Is it informative? They have two indicators. the organization where an LLM as a J performs here a pairwise comparison and then they have here a metric that assesses here the informationational completeness. So what you do you have now from your text here a human written um segment here and this is automatically if you want chucked up here by the eye so you get single sentences for example but a human like red at all introduced here the transformer architecture which relies here on a self attention mechanism now the eye might say hey this is a two long sentence there are too many objects too many subjects I reduce this I chunk it up here in nuggets so the first nugget is redford at introduce the transformer stop. This is it. So simple to verify and then it goes on and you see exactly what's happening. Now you have then this essential nuggets and you want to make sure that those really represent here in the I generated text what is the main content of the human written sentence or paragraph or paper. Then second dimension the retrieval quality. Simple in my words. Did the I system find the right papers on archive for my topic. Here we have three indicators. The relevance rate, the reference coverage or simply hey if the system found here the real important papers and you know importance is just one but we can also measure here how impactful this is with an impact um quote. So we have a document importance metric. So this is a proxy for retrieving impactful word that uses here the citation comes from a specific API. Great. And finally the most important for me we have to have verifiability. So whatever we claim there is a check against some other published work. So are the claims now actually supported here by the citation? And here they go with two indicators. the citation precision where they say hey does the cited source support at least one claim made in the eye sentence just one and then they are say hey let's go full power now now for the term claim coverage we want do the cited sources support all the claims made in the eye sentence so it's really every little detail that the eye writes out here supported here by let's say arrive references Great. Then they decided, you know, so we have now the idea of the benchmark, we have the metric and the subindicator of the benchmark. And they say, you know what, now we build a vehicle. So we can switch now the engine of this vehicle. So we can put in either a GPT 4.1 or we can put in a GPD4 Omni or we go with a clawed model as the engine. And they built here the deep scholar base. Now this was interesting because remember I told you this is your almost rag system. No they went here with a classical rag multi multi- aent structure. So at first you have the query generation. So an LLM generates now a set of diverse search query based on the human input. So whatever I have the AI the LLM tells me hey that was a little bit too complicated. I generate now a set of diver search queries and then I send this up. So those queries now AI generated queries now run against the search API in our example archive and then we have a filtering now a semantic filtering and now we use another LLM is used to filter here to retrieve documents and anything that is irrelevant we just want to ignore this no but we have still a lot of documents like say 200 documents left now after the filtering so we have to do a reranking because we want to go down to the top 20 no so we have another llm based step that now reranks in detail to the only 200 remaining documents and it's a new prioritization here. What are the most relevance only of these 200 and then if we have the top 20 we want to have a semantic aggregation of the content of all 20 papers. No. So we say now the final top 20 documents are fed now as a context to an LLM and again you can choose your LLM that you like with a detailed prompt and then synthesization and the LLM writes it the final related works action. So clearly structured data processing pipeline and you can switch on and off any LLM in any place that you want and they tested here variation of different LLMs and they say so what are now the best performance combination so you see pipeline performing research this is great they run this here with a GPD4 as the default and then they exchange and said hey what happens if I add here let's say an 03 or a cloud model to GPD4.1 one all the prompts for this single steps that you just noticed here you have here in detail in the annex here of the documentation of this new research paper like here I show you the revised ODS react agent prompt here for calling here a tool so a lot of additional data I just want to give you the result this is it again you see one two three dimension and then all the different sub indicator in this dimension. Yeah, I think we can forget about the opensource research system from 2014. 
Relevance score: -9.934

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: But just a warning, you always have to check your resources. There's always a possibility, even with the best current systems, that the system is hallucinating. It is referencing to archive preprints that are non-existent or it is coming up with an explanation that is logically incorrect. But otherwise, enjoy AI, subscribe to the channel, and I see you in the next video.
Relevance score: -10.838

------------------------------------------------------------
---------- Query at 2025-09-04T01:43:36.397323 ----------

        ## Task
        The user's query: how deepcsholar ai works

        Answer the user's query based only on the retrieved context.
        Use chat history if relevant, but never go beyond the given context.
        Write a natural, context-based answer.


        ## Context
        Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Hello community so great that you are back. We have a new paper by Stanford University and UC Berkeley and it is all about science and a deep scholar AI system that is brand new. Now you know we have open-source research system no that do a research and synthesize known facts and new findings and you are familiar with this no for example here open scholar this is here the paper or if you go for another open source this is here storm or maybe you are familiar with deep researcher from April 2025 so we do have open systems but of course it's interesting how do they compare to the commercial system let's say openi deep research or we We have brand new search EIS where you remember MCP and here multi- aent system. So how do they compare here to multi- aent search EI systems in well let's say you do a research on the internet. Now for those we have now the study where they compare Llama forced to GPD 4.1 and 03 an Opus 4 by Claude and a Gemini 2.5 Pro. And if you ask me why not some other models, those are the set of models Stanford and UC Berkeley decided to perform their tests on. This is it. The framework they used here is here open deep search. The search with open-source reasoning agents and you remember we have here multi- aent system. We have an open reasoning agent here itself that interprets here the given human task and completes it here by orchestrating a sequence of action and one of those action might include here calling tools and one of those tools that is available to this agent is here the open search tool to connect to the internet and everything. So a multi- aent search utility and now we have a new paper, we have a new method, we have some new research benchmark results and some new ideas. So what is the task that we are looking at? Let's say you're a researcher and you have your title and maybe you have already an idea from a technical abstract. Yeah. And then you want to ask AI to write here the related work section of your publication for you. In the old times we call this a literature review. Have a look who published what in your particular specific domain, when it was published, what were the topics, how was it related to your work. Now everything should be done by the eye. Now this new system is called the deep scholar benchmark and yes it promises to generate here the deep the related work section the deep research on internet on archive on meta archive for you. So what it needs three capabilities you are looking for the retrieval. So you want to have a vast corpus the live internet or the live archive whatever you have then you want to have a synthesis report. So everything all the data that were replied here from multiple sources weave them together the findings into a coherent long form narrative and then you want and this is the most important constraint absolute verifiability. So whatever you are claiming here and whatever you are citing those must really exist. So the citing sources must be accurate on every claim and must be back traced back to its origin. You must have references. Great. So the authors here Stanford and UC Berkeley built here an automated data pipeline directly and they said you know what we not just for the internet let's go here for science let's go to archive. So we want to have a fresh data set retrieved absolutely fresh here daily or every minute here from archive. So how we do this? So three topics the recency the quality control and diversity and here they also limited theirel to computer science as a topic and they made sure that they have a diverse top with 18 different domains in computer science. So yeah we are restricted to computer science but I think you will see the results they speak for themselves. Here's the paper published today August 27, 2025. Deep scholar bench live benchmark on automated evolation for generative research systems from Stanford University in UC Berkeley. And the beauty is not that they have now a new benchmark, but what the new benchmark shows us. It shows us the true difficulty AI systems have if they do have to do a research synthesis. Here's the result. I would say wait a minute explain here at first the metric and the benchmark methodology. Okay. So they decided to go here for a threedimensional differentiation to say we look for the knowledge synthesis of this new system for the retrieval quality. It's more or less a rack system. No. And then for the verifiability because we are working in science with archive. It must be really that we can site real sources and there's no hallucination. Great. And then you can see even in those three dimension they decided to go with two or three sub indicators. So let's have a look. So for the knowledge synthesis or what I call hey how well is this generated EI text written? Is it informative? They have two indicators. the organization where an LLM as a J performs here a pairwise comparison and then they have here a metric that assesses here the informationational completeness. So what you do you have now from your text here a human written um segment here and this is automatically if you want chucked up here by the eye so you get single sentences for example but a human like red at all introduced here the transformer architecture which relies here on a self attention mechanism now the eye might say hey this is a two long sentence there are too many objects too many subjects I reduce this I chunk it up here in nuggets so the first nugget is redford at introduce the transformer stop. This is it. So simple to verify and then it goes on and you see exactly what's happening. Now you have then this essential nuggets and you want to make sure that those really represent here in the I generated text what is the main content of the human written sentence or paragraph or paper. Then second dimension the retrieval quality. Simple in my words. Did the I system find the right papers on archive for my topic. Here we have three indicators. The relevance rate, the reference coverage or simply hey if the system found here the real important papers and you know importance is just one but we can also measure here how impactful this is with an impact um quote. So we have a document importance metric. So this is a proxy for retrieving impactful word that uses here the citation comes from a specific API. Great. And finally the most important for me we have to have verifiability. So whatever we claim there is a check against some other published work. So are the claims now actually supported here by the citation? And here they go with two indicators. the citation precision where they say hey does the cited source support at least one claim made in the eye sentence just one and then they are say hey let's go full power now now for the term claim coverage we want do the cited sources support all the claims made in the eye sentence so it's really every little detail that the eye writes out here supported here by let's say arrive references Great. Then they decided, you know, so we have now the idea of the benchmark, we have the metric and the subindicator of the benchmark. And they say, you know what, now we build a vehicle. So we can switch now the engine of this vehicle. So we can put in either a GPT 4.1 or we can put in a GPD4 Omni or we go with a clawed model as the engine. And they built here the deep scholar base. Now this was interesting because remember I told you this is your almost rag system. No they went here with a classical rag multi multi- aent structure. So at first you have the query generation. So an LLM generates now a set of diverse search query based on the human input. So whatever I have the AI the LLM tells me hey that was a little bit too complicated. I generate now a set of diver search queries and then I send this up. So those queries now AI generated queries now run against the search API in our example archive and then we have a filtering now a semantic filtering and now we use another LLM is used to filter here to retrieve documents and anything that is irrelevant we just want to ignore this no but we have still a lot of documents like say 200 documents left now after the filtering so we have to do a reranking because we want to go down to the top 20 no so we have another llm based step that now reranks in detail to the only 200 remaining documents and it's a new prioritization here. What are the most relevance only of these 200 and then if we have the top 20 we want to have a semantic aggregation of the content of all 20 papers. No. So we say now the final top 20 documents are fed now as a context to an LLM and again you can choose your LLM that you like with a detailed prompt and then synthesization and the LLM writes it the final related works action. So clearly structured data processing pipeline and you can switch on and off any LLM in any place that you want and they tested here variation of different LLMs and they say so what are now the best performance combination so you see pipeline performing research this is great they run this here with a GPD4 as the default and then they exchange and said hey what happens if I add here let's say an 03 or a cloud model to GPD4.1 one all the prompts for this single steps that you just noticed here you have here in detail in the annex here of the documentation of this new research paper like here I show you the revised ODS react agent prompt here for calling here a tool so a lot of additional data I just want to give you the result this is it again you see one two three dimension and then all the different sub indicator in this dimension. Yeah, I think we can forget about the opensource research system from 2014. 
Relevance score: -6.871

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: And there we have it. A new research paper by Stanford University UC Berkeley about AI doing here the research the literature review here on brand new topics live in a multi- aent network deep scholar to research and synthesize knowledge. And if you are in science this could be a real benefit, a real help. 
Relevance score: -6.898

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Great. So verifiability here open research scores shockingly low. This is what Stanford and UC Berkeler tells us. Citation precision numerical value is only 39. The claim coverage is only.13. And they tell us this is one of the worst performers here. that why is this happening? They suggest here a system optimized for fluent plausible sounding text is great on the one hand. So it's excellent in synthesizing here story or story line. But if you work in science, if you want to be absolute precise, you want to reduce hallucinating to the max and you want to make sure making logical leaps that are not strictly supported by the cited sources of archive are not part of your document. Now unfortunately this is happening here with open deep research. So for purely scientific work where you have to have absolute precision this could turn out to be a fatal flaw. Now, interestingly, if we look here at the new matter, the deep scholar base as I just showed you and we put here an 03 as the engine into this benchmark, you see, interestingly for the knowledge synthesis, we have the exact same benchmark 857.857. If we look here at the nugget, we even have a better performance here at the sequential tokenization if you want. And and this is now interesting with this new method. Look at the variability from one we jump to 6. So yeah, wow. This is an impressive improvement against the commercial openi deep research performance. So this new method by Stanford and UC Berkeley even if you use it with an openi model you see in one of the indicators they are at least as good as the commercial models but they outperform the commercial model significantly in variability. Isn't this beautiful? And if you ask yourself hey why I have suddenly two LLMs I told you you can have multiple LLMs here in the pipeline. So they decided for the SAM filter and the SAM top K data they go with GBD 4.1 but for the heavyduty SAM the aggregation step they go here with an 03 or a Claude or a Gemini just to make sure we on the same page. So this is the process. So we have the filter process then the top K the top 20 papers then the final aggregation the final section generation and I told you I show you always the prompt and here you have the prompt. So whatever LLM you choose an 03 or a claude OPUS 4 or a Gemini 2.5 Pro this is here the prompt for the aggregation for the final aggregation here of your research. So now we can see the paper here highlights here the deep scholar base can achieve up to 6.3 times higher claim coverage. So one indicator and one dimension six times three better than openi deep research. Nice. So we do have new systems that really outperform the classical commercial blackbox systems. But on the other hand, if you want just a plausible, well-written draft and you are not in science and you are willing, if you are in science, to meticulously fact check every single claim, every single half sentence that EI writes for you, then if you as a human do this verification and correct it, then open EI is really maybe also interesting. However, if you are a scientist and you say, I will go with the AI system that I can trust at least partially, then the deep scholar base here with claude or gemini model as the engine is dramatically superior. If you look at the data from Stanford and UC Berkeley, the paper states also that no method achieves a score greater than.19 when you average across the metric. So this is not an incremental challenge. You know, go from 78% to 82%. No, we at 19%. This is the best model we have. So we have quite a monumental challenge in front of us to make our AI models better, more performance, more power. So there's a lot of work in front of us. And if you say okay, I don't like here the numerical values in a table. I want to see this here in a visualization. Here you have exactly all the different elements here in um yeah to radar visualization. But this is exactly the data here from the numerical table I just showed you. But if you prefer to have it here in the radar, beautiful. Here is it. 
Relevance score: -8.125

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: 85. So I give you here my summary. OpenI is really here the leader in crafting a well ststructured and seemingly comprehensive narrative. Open deep research really excels in this synthesis and in the retrieval task and we have here the numerical data for the highest organization score 85 and the highest nugget coverage 39. output are well organized and capture really the essential fact better than the competitors can perform. And for the retrieval quality also the winner here. If you look here at the relevance rate 62 covers the most important human-sided reference with a reference coins of8 and finds the most impactful paper.1 but and this is now a big but the performance of open research collapses in the third dimension and this is the most crucial aspect if you have some scientific writing trust and verifiability. If you just write something that is not verifiable. 
Relevance score: -9.603

------------------------------------------------------------
---------- Query at 2025-09-04T01:44:50.797534 ----------

        ## Task
        The user's query: what is methodology of scholar ai

        Answer the user's query based only on the retrieved context.
        Use chat history if relevant, but never go beyond the given context.
        Write a natural, context-based answer.


        ## Context
        Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Hello community so great that you are back. We have a new paper by Stanford University and UC Berkeley and it is all about science and a deep scholar AI system that is brand new. Now you know we have open-source research system no that do a research and synthesize known facts and new findings and you are familiar with this no for example here open scholar this is here the paper or if you go for another open source this is here storm or maybe you are familiar with deep researcher from April 2025 so we do have open systems but of course it's interesting how do they compare to the commercial system let's say openi deep research or we We have brand new search EIS where you remember MCP and here multi- aent system. So how do they compare here to multi- aent search EI systems in well let's say you do a research on the internet. Now for those we have now the study where they compare Llama forced to GPD 4.1 and 03 an Opus 4 by Claude and a Gemini 2.5 Pro. And if you ask me why not some other models, those are the set of models Stanford and UC Berkeley decided to perform their tests on. This is it. The framework they used here is here open deep search. The search with open-source reasoning agents and you remember we have here multi- aent system. We have an open reasoning agent here itself that interprets here the given human task and completes it here by orchestrating a sequence of action and one of those action might include here calling tools and one of those tools that is available to this agent is here the open search tool to connect to the internet and everything. So a multi- aent search utility and now we have a new paper, we have a new method, we have some new research benchmark results and some new ideas. So what is the task that we are looking at? Let's say you're a researcher and you have your title and maybe you have already an idea from a technical abstract. Yeah. And then you want to ask AI to write here the related work section of your publication for you. In the old times we call this a literature review. Have a look who published what in your particular specific domain, when it was published, what were the topics, how was it related to your work. Now everything should be done by the eye. Now this new system is called the deep scholar benchmark and yes it promises to generate here the deep the related work section the deep research on internet on archive on meta archive for you. So what it needs three capabilities you are looking for the retrieval. So you want to have a vast corpus the live internet or the live archive whatever you have then you want to have a synthesis report. So everything all the data that were replied here from multiple sources weave them together the findings into a coherent long form narrative and then you want and this is the most important constraint absolute verifiability. So whatever you are claiming here and whatever you are citing those must really exist. So the citing sources must be accurate on every claim and must be back traced back to its origin. You must have references. Great. So the authors here Stanford and UC Berkeley built here an automated data pipeline directly and they said you know what we not just for the internet let's go here for science let's go to archive. So we want to have a fresh data set retrieved absolutely fresh here daily or every minute here from archive. So how we do this? So three topics the recency the quality control and diversity and here they also limited theirel to computer science as a topic and they made sure that they have a diverse top with 18 different domains in computer science. So yeah we are restricted to computer science but I think you will see the results they speak for themselves. Here's the paper published today August 27, 2025. Deep scholar bench live benchmark on automated evolation for generative research systems from Stanford University in UC Berkeley. And the beauty is not that they have now a new benchmark, but what the new benchmark shows us. It shows us the true difficulty AI systems have if they do have to do a research synthesis. Here's the result. I would say wait a minute explain here at first the metric and the benchmark methodology. Okay. So they decided to go here for a threedimensional differentiation to say we look for the knowledge synthesis of this new system for the retrieval quality. It's more or less a rack system. No. And then for the verifiability because we are working in science with archive. It must be really that we can site real sources and there's no hallucination. Great. And then you can see even in those three dimension they decided to go with two or three sub indicators. So let's have a look. So for the knowledge synthesis or what I call hey how well is this generated EI text written? Is it informative? They have two indicators. the organization where an LLM as a J performs here a pairwise comparison and then they have here a metric that assesses here the informationational completeness. So what you do you have now from your text here a human written um segment here and this is automatically if you want chucked up here by the eye so you get single sentences for example but a human like red at all introduced here the transformer architecture which relies here on a self attention mechanism now the eye might say hey this is a two long sentence there are too many objects too many subjects I reduce this I chunk it up here in nuggets so the first nugget is redford at introduce the transformer stop. This is it. So simple to verify and then it goes on and you see exactly what's happening. Now you have then this essential nuggets and you want to make sure that those really represent here in the I generated text what is the main content of the human written sentence or paragraph or paper. Then second dimension the retrieval quality. Simple in my words. Did the I system find the right papers on archive for my topic. Here we have three indicators. The relevance rate, the reference coverage or simply hey if the system found here the real important papers and you know importance is just one but we can also measure here how impactful this is with an impact um quote. So we have a document importance metric. So this is a proxy for retrieving impactful word that uses here the citation comes from a specific API. Great. And finally the most important for me we have to have verifiability. So whatever we claim there is a check against some other published work. So are the claims now actually supported here by the citation? And here they go with two indicators. the citation precision where they say hey does the cited source support at least one claim made in the eye sentence just one and then they are say hey let's go full power now now for the term claim coverage we want do the cited sources support all the claims made in the eye sentence so it's really every little detail that the eye writes out here supported here by let's say arrive references Great. Then they decided, you know, so we have now the idea of the benchmark, we have the metric and the subindicator of the benchmark. And they say, you know what, now we build a vehicle. So we can switch now the engine of this vehicle. So we can put in either a GPT 4.1 or we can put in a GPD4 Omni or we go with a clawed model as the engine. And they built here the deep scholar base. Now this was interesting because remember I told you this is your almost rag system. No they went here with a classical rag multi multi- aent structure. So at first you have the query generation. So an LLM generates now a set of diverse search query based on the human input. So whatever I have the AI the LLM tells me hey that was a little bit too complicated. I generate now a set of diver search queries and then I send this up. So those queries now AI generated queries now run against the search API in our example archive and then we have a filtering now a semantic filtering and now we use another LLM is used to filter here to retrieve documents and anything that is irrelevant we just want to ignore this no but we have still a lot of documents like say 200 documents left now after the filtering so we have to do a reranking because we want to go down to the top 20 no so we have another llm based step that now reranks in detail to the only 200 remaining documents and it's a new prioritization here. What are the most relevance only of these 200 and then if we have the top 20 we want to have a semantic aggregation of the content of all 20 papers. No. So we say now the final top 20 documents are fed now as a context to an LLM and again you can choose your LLM that you like with a detailed prompt and then synthesization and the LLM writes it the final related works action. So clearly structured data processing pipeline and you can switch on and off any LLM in any place that you want and they tested here variation of different LLMs and they say so what are now the best performance combination so you see pipeline performing research this is great they run this here with a GPD4 as the default and then they exchange and said hey what happens if I add here let's say an 03 or a cloud model to GPD4.1 one all the prompts for this single steps that you just noticed here you have here in detail in the annex here of the documentation of this new research paper like here I show you the revised ODS react agent prompt here for calling here a tool so a lot of additional data I just want to give you the result this is it again you see one two three dimension and then all the different sub indicator in this dimension. Yeah, I think we can forget about the opensource research system from 2014. 
Relevance score: -8.510

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Great. So verifiability here open research scores shockingly low. This is what Stanford and UC Berkeler tells us. Citation precision numerical value is only 39. The claim coverage is only.13. And they tell us this is one of the worst performers here. that why is this happening? They suggest here a system optimized for fluent plausible sounding text is great on the one hand. So it's excellent in synthesizing here story or story line. But if you work in science, if you want to be absolute precise, you want to reduce hallucinating to the max and you want to make sure making logical leaps that are not strictly supported by the cited sources of archive are not part of your document. Now unfortunately this is happening here with open deep research. So for purely scientific work where you have to have absolute precision this could turn out to be a fatal flaw. Now, interestingly, if we look here at the new matter, the deep scholar base as I just showed you and we put here an 03 as the engine into this benchmark, you see, interestingly for the knowledge synthesis, we have the exact same benchmark 857.857. If we look here at the nugget, we even have a better performance here at the sequential tokenization if you want. And and this is now interesting with this new method. Look at the variability from one we jump to 6. So yeah, wow. This is an impressive improvement against the commercial openi deep research performance. So this new method by Stanford and UC Berkeley even if you use it with an openi model you see in one of the indicators they are at least as good as the commercial models but they outperform the commercial model significantly in variability. Isn't this beautiful? And if you ask yourself hey why I have suddenly two LLMs I told you you can have multiple LLMs here in the pipeline. So they decided for the SAM filter and the SAM top K data they go with GBD 4.1 but for the heavyduty SAM the aggregation step they go here with an 03 or a Claude or a Gemini just to make sure we on the same page. So this is the process. So we have the filter process then the top K the top 20 papers then the final aggregation the final section generation and I told you I show you always the prompt and here you have the prompt. So whatever LLM you choose an 03 or a claude OPUS 4 or a Gemini 2.5 Pro this is here the prompt for the aggregation for the final aggregation here of your research. So now we can see the paper here highlights here the deep scholar base can achieve up to 6.3 times higher claim coverage. So one indicator and one dimension six times three better than openi deep research. Nice. So we do have new systems that really outperform the classical commercial blackbox systems. But on the other hand, if you want just a plausible, well-written draft and you are not in science and you are willing, if you are in science, to meticulously fact check every single claim, every single half sentence that EI writes for you, then if you as a human do this verification and correct it, then open EI is really maybe also interesting. However, if you are a scientist and you say, I will go with the AI system that I can trust at least partially, then the deep scholar base here with claude or gemini model as the engine is dramatically superior. If you look at the data from Stanford and UC Berkeley, the paper states also that no method achieves a score greater than.19 when you average across the metric. So this is not an incremental challenge. You know, go from 78% to 82%. No, we at 19%. This is the best model we have. So we have quite a monumental challenge in front of us to make our AI models better, more performance, more power. So there's a lot of work in front of us. And if you say okay, I don't like here the numerical values in a table. I want to see this here in a visualization. Here you have exactly all the different elements here in um yeah to radar visualization. But this is exactly the data here from the numerical table I just showed you. But if you prefer to have it here in the radar, beautiful. Here is it. 
Relevance score: -10.349

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: And there we have it. A new research paper by Stanford University UC Berkeley about AI doing here the research the literature review here on brand new topics live in a multi- aent network deep scholar to research and synthesize knowledge. And if you are in science this could be a real benefit, a real help. 
Relevance score: -10.456

------------------------------------------------------------
---------- Query at 2025-09-04T01:49:58.870903 ----------

        ## Task
        The user's query: what is bencmark score of scholar ai

        Answer the user's query based only on the retrieved context.
        Use chat history if relevant, but never go beyond the given context.
        Write a natural, context-based answer.


        ## Context
        Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: And there we have it. A new research paper by Stanford University UC Berkeley about AI doing here the research the literature review here on brand new topics live in a multi- aent network deep scholar to research and synthesize knowledge. And if you are in science this could be a real benefit, a real help. 
Relevance score: -4.229

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: But just a warning, you always have to check your resources. There's always a possibility, even with the best current systems, that the system is hallucinating. It is referencing to archive preprints that are non-existent or it is coming up with an explanation that is logically incorrect. But otherwise, enjoy AI, subscribe to the channel, and I see you in the next video.
Relevance score: -5.891

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Hello community so great that you are back. We have a new paper by Stanford University and UC Berkeley and it is all about science and a deep scholar AI system that is brand new. Now you know we have open-source research system no that do a research and synthesize known facts and new findings and you are familiar with this no for example here open scholar this is here the paper or if you go for another open source this is here storm or maybe you are familiar with deep researcher from April 2025 so we do have open systems but of course it's interesting how do they compare to the commercial system let's say openi deep research or we We have brand new search EIS where you remember MCP and here multi- aent system. So how do they compare here to multi- aent search EI systems in well let's say you do a research on the internet. Now for those we have now the study where they compare Llama forced to GPD 4.1 and 03 an Opus 4 by Claude and a Gemini 2.5 Pro. And if you ask me why not some other models, those are the set of models Stanford and UC Berkeley decided to perform their tests on. This is it. The framework they used here is here open deep search. The search with open-source reasoning agents and you remember we have here multi- aent system. We have an open reasoning agent here itself that interprets here the given human task and completes it here by orchestrating a sequence of action and one of those action might include here calling tools and one of those tools that is available to this agent is here the open search tool to connect to the internet and everything. So a multi- aent search utility and now we have a new paper, we have a new method, we have some new research benchmark results and some new ideas. So what is the task that we are looking at? Let's say you're a researcher and you have your title and maybe you have already an idea from a technical abstract. Yeah. And then you want to ask AI to write here the related work section of your publication for you. In the old times we call this a literature review. Have a look who published what in your particular specific domain, when it was published, what were the topics, how was it related to your work. Now everything should be done by the eye. Now this new system is called the deep scholar benchmark and yes it promises to generate here the deep the related work section the deep research on internet on archive on meta archive for you. So what it needs three capabilities you are looking for the retrieval. So you want to have a vast corpus the live internet or the live archive whatever you have then you want to have a synthesis report. So everything all the data that were replied here from multiple sources weave them together the findings into a coherent long form narrative and then you want and this is the most important constraint absolute verifiability. So whatever you are claiming here and whatever you are citing those must really exist. So the citing sources must be accurate on every claim and must be back traced back to its origin. You must have references. Great. So the authors here Stanford and UC Berkeley built here an automated data pipeline directly and they said you know what we not just for the internet let's go here for science let's go to archive. So we want to have a fresh data set retrieved absolutely fresh here daily or every minute here from archive. So how we do this? So three topics the recency the quality control and diversity and here they also limited theirel to computer science as a topic and they made sure that they have a diverse top with 18 different domains in computer science. So yeah we are restricted to computer science but I think you will see the results they speak for themselves. Here's the paper published today August 27, 2025. Deep scholar bench live benchmark on automated evolation for generative research systems from Stanford University in UC Berkeley. And the beauty is not that they have now a new benchmark, but what the new benchmark shows us. It shows us the true difficulty AI systems have if they do have to do a research synthesis. Here's the result. I would say wait a minute explain here at first the metric and the benchmark methodology. Okay. So they decided to go here for a threedimensional differentiation to say we look for the knowledge synthesis of this new system for the retrieval quality. It's more or less a rack system. No. And then for the verifiability because we are working in science with archive. It must be really that we can site real sources and there's no hallucination. Great. And then you can see even in those three dimension they decided to go with two or three sub indicators. So let's have a look. So for the knowledge synthesis or what I call hey how well is this generated EI text written? Is it informative? They have two indicators. the organization where an LLM as a J performs here a pairwise comparison and then they have here a metric that assesses here the informationational completeness. So what you do you have now from your text here a human written um segment here and this is automatically if you want chucked up here by the eye so you get single sentences for example but a human like red at all introduced here the transformer architecture which relies here on a self attention mechanism now the eye might say hey this is a two long sentence there are too many objects too many subjects I reduce this I chunk it up here in nuggets so the first nugget is redford at introduce the transformer stop. This is it. So simple to verify and then it goes on and you see exactly what's happening. Now you have then this essential nuggets and you want to make sure that those really represent here in the I generated text what is the main content of the human written sentence or paragraph or paper. Then second dimension the retrieval quality. Simple in my words. Did the I system find the right papers on archive for my topic. Here we have three indicators. The relevance rate, the reference coverage or simply hey if the system found here the real important papers and you know importance is just one but we can also measure here how impactful this is with an impact um quote. So we have a document importance metric. So this is a proxy for retrieving impactful word that uses here the citation comes from a specific API. Great. And finally the most important for me we have to have verifiability. So whatever we claim there is a check against some other published work. So are the claims now actually supported here by the citation? And here they go with two indicators. the citation precision where they say hey does the cited source support at least one claim made in the eye sentence just one and then they are say hey let's go full power now now for the term claim coverage we want do the cited sources support all the claims made in the eye sentence so it's really every little detail that the eye writes out here supported here by let's say arrive references Great. Then they decided, you know, so we have now the idea of the benchmark, we have the metric and the subindicator of the benchmark. And they say, you know what, now we build a vehicle. So we can switch now the engine of this vehicle. So we can put in either a GPT 4.1 or we can put in a GPD4 Omni or we go with a clawed model as the engine. And they built here the deep scholar base. Now this was interesting because remember I told you this is your almost rag system. No they went here with a classical rag multi multi- aent structure. So at first you have the query generation. So an LLM generates now a set of diverse search query based on the human input. So whatever I have the AI the LLM tells me hey that was a little bit too complicated. I generate now a set of diver search queries and then I send this up. So those queries now AI generated queries now run against the search API in our example archive and then we have a filtering now a semantic filtering and now we use another LLM is used to filter here to retrieve documents and anything that is irrelevant we just want to ignore this no but we have still a lot of documents like say 200 documents left now after the filtering so we have to do a reranking because we want to go down to the top 20 no so we have another llm based step that now reranks in detail to the only 200 remaining documents and it's a new prioritization here. What are the most relevance only of these 200 and then if we have the top 20 we want to have a semantic aggregation of the content of all 20 papers. No. So we say now the final top 20 documents are fed now as a context to an LLM and again you can choose your LLM that you like with a detailed prompt and then synthesization and the LLM writes it the final related works action. So clearly structured data processing pipeline and you can switch on and off any LLM in any place that you want and they tested here variation of different LLMs and they say so what are now the best performance combination so you see pipeline performing research this is great they run this here with a GPD4 as the default and then they exchange and said hey what happens if I add here let's say an 03 or a cloud model to GPD4.1 one all the prompts for this single steps that you just noticed here you have here in detail in the annex here of the documentation of this new research paper like here I show you the revised ODS react agent prompt here for calling here a tool so a lot of additional data I just want to give you the result this is it again you see one two three dimension and then all the different sub indicator in this dimension. Yeah, I think we can forget about the opensource research system from 2014. 
Relevance score: -5.923

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Not up to challenge anymore. Let's look at a commercial system before we go to the pure search. AI commercial system is they tested here one. They said this is kind of one of the best OpenIP research. And if you look at the knowledge synthesis, wow, this is really impressive here. 
Relevance score: -8.693

------------------------------------------------------------
---------- Query at 2025-09-04T02:01:30.876488 ----------

        ## Task
        The user's query: what is bencmark score of scholar ai

        Answer the user's query based only on the retrieved context.
        Use chat history if relevant, but never go beyond the given context.
        Write a natural, context-based answer.


        ## Context
        Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: And there we have it. A new research paper by Stanford University UC Berkeley about AI doing here the research the literature review here on brand new topics live in a multi- aent network deep scholar to research and synthesize knowledge. And if you are in science this could be a real benefit, a real help. 
Relevance score: -4.229

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: But just a warning, you always have to check your resources. There's always a possibility, even with the best current systems, that the system is hallucinating. It is referencing to archive preprints that are non-existent or it is coming up with an explanation that is logically incorrect. But otherwise, enjoy AI, subscribe to the channel, and I see you in the next video.
Relevance score: -5.891

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Hello community so great that you are back. We have a new paper by Stanford University and UC Berkeley and it is all about science and a deep scholar AI system that is brand new. Now you know we have open-source research system no that do a research and synthesize known facts and new findings and you are familiar with this no for example here open scholar this is here the paper or if you go for another open source this is here storm or maybe you are familiar with deep researcher from April 2025 so we do have open systems but of course it's interesting how do they compare to the commercial system let's say openi deep research or we We have brand new search EIS where you remember MCP and here multi- aent system. So how do they compare here to multi- aent search EI systems in well let's say you do a research on the internet. Now for those we have now the study where they compare Llama forced to GPD 4.1 and 03 an Opus 4 by Claude and a Gemini 2.5 Pro. And if you ask me why not some other models, those are the set of models Stanford and UC Berkeley decided to perform their tests on. This is it. The framework they used here is here open deep search. The search with open-source reasoning agents and you remember we have here multi- aent system. We have an open reasoning agent here itself that interprets here the given human task and completes it here by orchestrating a sequence of action and one of those action might include here calling tools and one of those tools that is available to this agent is here the open search tool to connect to the internet and everything. So a multi- aent search utility and now we have a new paper, we have a new method, we have some new research benchmark results and some new ideas. So what is the task that we are looking at? Let's say you're a researcher and you have your title and maybe you have already an idea from a technical abstract. Yeah. And then you want to ask AI to write here the related work section of your publication for you. In the old times we call this a literature review. Have a look who published what in your particular specific domain, when it was published, what were the topics, how was it related to your work. Now everything should be done by the eye. Now this new system is called the deep scholar benchmark and yes it promises to generate here the deep the related work section the deep research on internet on archive on meta archive for you. So what it needs three capabilities you are looking for the retrieval. So you want to have a vast corpus the live internet or the live archive whatever you have then you want to have a synthesis report. So everything all the data that were replied here from multiple sources weave them together the findings into a coherent long form narrative and then you want and this is the most important constraint absolute verifiability. So whatever you are claiming here and whatever you are citing those must really exist. So the citing sources must be accurate on every claim and must be back traced back to its origin. You must have references. Great. So the authors here Stanford and UC Berkeley built here an automated data pipeline directly and they said you know what we not just for the internet let's go here for science let's go to archive. So we want to have a fresh data set retrieved absolutely fresh here daily or every minute here from archive. So how we do this? So three topics the recency the quality control and diversity and here they also limited theirel to computer science as a topic and they made sure that they have a diverse top with 18 different domains in computer science. So yeah we are restricted to computer science but I think you will see the results they speak for themselves. Here's the paper published today August 27, 2025. Deep scholar bench live benchmark on automated evolation for generative research systems from Stanford University in UC Berkeley. And the beauty is not that they have now a new benchmark, but what the new benchmark shows us. It shows us the true difficulty AI systems have if they do have to do a research synthesis. Here's the result. I would say wait a minute explain here at first the metric and the benchmark methodology. Okay. So they decided to go here for a threedimensional differentiation to say we look for the knowledge synthesis of this new system for the retrieval quality. It's more or less a rack system. No. And then for the verifiability because we are working in science with archive. It must be really that we can site real sources and there's no hallucination. Great. And then you can see even in those three dimension they decided to go with two or three sub indicators. So let's have a look. So for the knowledge synthesis or what I call hey how well is this generated EI text written? Is it informative? They have two indicators. the organization where an LLM as a J performs here a pairwise comparison and then they have here a metric that assesses here the informationational completeness. So what you do you have now from your text here a human written um segment here and this is automatically if you want chucked up here by the eye so you get single sentences for example but a human like red at all introduced here the transformer architecture which relies here on a self attention mechanism now the eye might say hey this is a two long sentence there are too many objects too many subjects I reduce this I chunk it up here in nuggets so the first nugget is redford at introduce the transformer stop. This is it. So simple to verify and then it goes on and you see exactly what's happening. Now you have then this essential nuggets and you want to make sure that those really represent here in the I generated text what is the main content of the human written sentence or paragraph or paper. Then second dimension the retrieval quality. Simple in my words. Did the I system find the right papers on archive for my topic. Here we have three indicators. The relevance rate, the reference coverage or simply hey if the system found here the real important papers and you know importance is just one but we can also measure here how impactful this is with an impact um quote. So we have a document importance metric. So this is a proxy for retrieving impactful word that uses here the citation comes from a specific API. Great. And finally the most important for me we have to have verifiability. So whatever we claim there is a check against some other published work. So are the claims now actually supported here by the citation? And here they go with two indicators. the citation precision where they say hey does the cited source support at least one claim made in the eye sentence just one and then they are say hey let's go full power now now for the term claim coverage we want do the cited sources support all the claims made in the eye sentence so it's really every little detail that the eye writes out here supported here by let's say arrive references Great. Then they decided, you know, so we have now the idea of the benchmark, we have the metric and the subindicator of the benchmark. And they say, you know what, now we build a vehicle. So we can switch now the engine of this vehicle. So we can put in either a GPT 4.1 or we can put in a GPD4 Omni or we go with a clawed model as the engine. And they built here the deep scholar base. Now this was interesting because remember I told you this is your almost rag system. No they went here with a classical rag multi multi- aent structure. So at first you have the query generation. So an LLM generates now a set of diverse search query based on the human input. So whatever I have the AI the LLM tells me hey that was a little bit too complicated. I generate now a set of diver search queries and then I send this up. So those queries now AI generated queries now run against the search API in our example archive and then we have a filtering now a semantic filtering and now we use another LLM is used to filter here to retrieve documents and anything that is irrelevant we just want to ignore this no but we have still a lot of documents like say 200 documents left now after the filtering so we have to do a reranking because we want to go down to the top 20 no so we have another llm based step that now reranks in detail to the only 200 remaining documents and it's a new prioritization here. What are the most relevance only of these 200 and then if we have the top 20 we want to have a semantic aggregation of the content of all 20 papers. No. So we say now the final top 20 documents are fed now as a context to an LLM and again you can choose your LLM that you like with a detailed prompt and then synthesization and the LLM writes it the final related works action. So clearly structured data processing pipeline and you can switch on and off any LLM in any place that you want and they tested here variation of different LLMs and they say so what are now the best performance combination so you see pipeline performing research this is great they run this here with a GPD4 as the default and then they exchange and said hey what happens if I add here let's say an 03 or a cloud model to GPD4.1 one all the prompts for this single steps that you just noticed here you have here in detail in the annex here of the documentation of this new research paper like here I show you the revised ODS react agent prompt here for calling here a tool so a lot of additional data I just want to give you the result this is it again you see one two three dimension and then all the different sub indicator in this dimension. Yeah, I think we can forget about the opensource research system from 2014. 
Relevance score: -5.923

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Not up to challenge anymore. Let's look at a commercial system before we go to the pure search. AI commercial system is they tested here one. They said this is kind of one of the best OpenIP research. And if you look at the knowledge synthesis, wow, this is really impressive here. 
Relevance score: -8.693

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Great. So verifiability here open research scores shockingly low. This is what Stanford and UC Berkeler tells us. Citation precision numerical value is only 39. The claim coverage is only.13. And they tell us this is one of the worst performers here. that why is this happening? They suggest here a system optimized for fluent plausible sounding text is great on the one hand. So it's excellent in synthesizing here story or story line. But if you work in science, if you want to be absolute precise, you want to reduce hallucinating to the max and you want to make sure making logical leaps that are not strictly supported by the cited sources of archive are not part of your document. Now unfortunately this is happening here with open deep research. So for purely scientific work where you have to have absolute precision this could turn out to be a fatal flaw. Now, interestingly, if we look here at the new matter, the deep scholar base as I just showed you and we put here an 03 as the engine into this benchmark, you see, interestingly for the knowledge synthesis, we have the exact same benchmark 857.857. If we look here at the nugget, we even have a better performance here at the sequential tokenization if you want. And and this is now interesting with this new method. Look at the variability from one we jump to 6. So yeah, wow. This is an impressive improvement against the commercial openi deep research performance. So this new method by Stanford and UC Berkeley even if you use it with an openi model you see in one of the indicators they are at least as good as the commercial models but they outperform the commercial model significantly in variability. Isn't this beautiful? And if you ask yourself hey why I have suddenly two LLMs I told you you can have multiple LLMs here in the pipeline. So they decided for the SAM filter and the SAM top K data they go with GBD 4.1 but for the heavyduty SAM the aggregation step they go here with an 03 or a Claude or a Gemini just to make sure we on the same page. So this is the process. So we have the filter process then the top K the top 20 papers then the final aggregation the final section generation and I told you I show you always the prompt and here you have the prompt. So whatever LLM you choose an 03 or a claude OPUS 4 or a Gemini 2.5 Pro this is here the prompt for the aggregation for the final aggregation here of your research. So now we can see the paper here highlights here the deep scholar base can achieve up to 6.3 times higher claim coverage. So one indicator and one dimension six times three better than openi deep research. Nice. So we do have new systems that really outperform the classical commercial blackbox systems. But on the other hand, if you want just a plausible, well-written draft and you are not in science and you are willing, if you are in science, to meticulously fact check every single claim, every single half sentence that EI writes for you, then if you as a human do this verification and correct it, then open EI is really maybe also interesting. However, if you are a scientist and you say, I will go with the AI system that I can trust at least partially, then the deep scholar base here with claude or gemini model as the engine is dramatically superior. If you look at the data from Stanford and UC Berkeley, the paper states also that no method achieves a score greater than.19 when you average across the metric. So this is not an incremental challenge. You know, go from 78% to 82%. No, we at 19%. This is the best model we have. So we have quite a monumental challenge in front of us to make our AI models better, more performance, more power. So there's a lot of work in front of us. And if you say okay, I don't like here the numerical values in a table. I want to see this here in a visualization. Here you have exactly all the different elements here in um yeah to radar visualization. But this is exactly the data here from the numerical table I just showed you. But if you prefer to have it here in the radar, beautiful. Here is it. 
Relevance score: -8.271

------------------------------------------------------------
---------- Query at 2025-09-04T02:12:38.985335 ----------

        Answer the user's query based on the retrieved context.
        The context is derived from youtube vidoe transcripts which person is telling about the user query.

        ## User Query: 
        what is scholar ai methodology?

        ## Context:
        Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Hello community so great that you are back. We have a new paper by Stanford University and UC Berkeley and it is all about science and a deep scholar AI system that is brand new. Now you know we have open-source research system no that do a research and synthesize known facts and new findings and you are familiar with this no for example here open scholar this is here the paper or if you go for another open source this is here storm or maybe you are familiar with deep researcher from April 2025 so we do have open systems but of course it's interesting how do they compare to the commercial system let's say openi deep research or we We have brand new search EIS where you remember MCP and here multi- aent system. So how do they compare here to multi- aent search EI systems in well let's say you do a research on the internet. Now for those we have now the study where they compare Llama forced to GPD 4.1 and 03 an Opus 4 by Claude and a Gemini 2.5 Pro. And if you ask me why not some other models, those are the set of models Stanford and UC Berkeley decided to perform their tests on. This is it. The framework they used here is here open deep search. The search with open-source reasoning agents and you remember we have here multi- aent system. We have an open reasoning agent here itself that interprets here the given human task and completes it here by orchestrating a sequence of action and one of those action might include here calling tools and one of those tools that is available to this agent is here the open search tool to connect to the internet and everything. So a multi- aent search utility and now we have a new paper, we have a new method, we have some new research benchmark results and some new ideas. So what is the task that we are looking at? Let's say you're a researcher and you have your title and maybe you have already an idea from a technical abstract. Yeah. And then you want to ask AI to write here the related work section of your publication for you. In the old times we call this a literature review. Have a look who published what in your particular specific domain, when it was published, what were the topics, how was it related to your work. Now everything should be done by the eye. Now this new system is called the deep scholar benchmark and yes it promises to generate here the deep the related work section the deep research on internet on archive on meta archive for you. So what it needs three capabilities you are looking for the retrieval. So you want to have a vast corpus the live internet or the live archive whatever you have then you want to have a synthesis report. So everything all the data that were replied here from multiple sources weave them together the findings into a coherent long form narrative and then you want and this is the most important constraint absolute verifiability. So whatever you are claiming here and whatever you are citing those must really exist. So the citing sources must be accurate on every claim and must be back traced back to its origin. You must have references. Great. So the authors here Stanford and UC Berkeley built here an automated data pipeline directly and they said you know what we not just for the internet let's go here for science let's go to archive. So we want to have a fresh data set retrieved absolutely fresh here daily or every minute here from archive. So how we do this? So three topics the recency the quality control and diversity and here they also limited theirel to computer science as a topic and they made sure that they have a diverse top with 18 different domains in computer science. So yeah we are restricted to computer science but I think you will see the results they speak for themselves. Here's the paper published today August 27, 2025. Deep scholar bench live benchmark on automated evolation for generative research systems from Stanford University in UC Berkeley. And the beauty is not that they have now a new benchmark, but what the new benchmark shows us. It shows us the true difficulty AI systems have if they do have to do a research synthesis. Here's the result. I would say wait a minute explain here at first the metric and the benchmark methodology. Okay. So they decided to go here for a threedimensional differentiation to say we look for the knowledge synthesis of this new system for the retrieval quality. It's more or less a rack system. No. And then for the verifiability because we are working in science with archive. It must be really that we can site real sources and there's no hallucination. Great. And then you can see even in those three dimension they decided to go with two or three sub indicators. So let's have a look. So for the knowledge synthesis or what I call hey how well is this generated EI text written? Is it informative? They have two indicators. the organization where an LLM as a J performs here a pairwise comparison and then they have here a metric that assesses here the informationational completeness. So what you do you have now from your text here a human written um segment here and this is automatically if you want chucked up here by the eye so you get single sentences for example but a human like red at all introduced here the transformer architecture which relies here on a self attention mechanism now the eye might say hey this is a two long sentence there are too many objects too many subjects I reduce this I chunk it up here in nuggets so the first nugget is redford at introduce the transformer stop. This is it. So simple to verify and then it goes on and you see exactly what's happening. Now you have then this essential nuggets and you want to make sure that those really represent here in the I generated text what is the main content of the human written sentence or paragraph or paper. Then second dimension the retrieval quality. Simple in my words. Did the I system find the right papers on archive for my topic. Here we have three indicators. The relevance rate, the reference coverage or simply hey if the system found here the real important papers and you know importance is just one but we can also measure here how impactful this is with an impact um quote. So we have a document importance metric. So this is a proxy for retrieving impactful word that uses here the citation comes from a specific API. Great. And finally the most important for me we have to have verifiability. So whatever we claim there is a check against some other published work. So are the claims now actually supported here by the citation? And here they go with two indicators. the citation precision where they say hey does the cited source support at least one claim made in the eye sentence just one and then they are say hey let's go full power now now for the term claim coverage we want do the cited sources support all the claims made in the eye sentence so it's really every little detail that the eye writes out here supported here by let's say arrive references Great. Then they decided, you know, so we have now the idea of the benchmark, we have the metric and the subindicator of the benchmark. And they say, you know what, now we build a vehicle. So we can switch now the engine of this vehicle. So we can put in either a GPT 4.1 or we can put in a GPD4 Omni or we go with a clawed model as the engine. And they built here the deep scholar base. Now this was interesting because remember I told you this is your almost rag system. No they went here with a classical rag multi multi- aent structure. So at first you have the query generation. So an LLM generates now a set of diverse search query based on the human input. So whatever I have the AI the LLM tells me hey that was a little bit too complicated. I generate now a set of diver search queries and then I send this up. So those queries now AI generated queries now run against the search API in our example archive and then we have a filtering now a semantic filtering and now we use another LLM is used to filter here to retrieve documents and anything that is irrelevant we just want to ignore this no but we have still a lot of documents like say 200 documents left now after the filtering so we have to do a reranking because we want to go down to the top 20 no so we have another llm based step that now reranks in detail to the only 200 remaining documents and it's a new prioritization here. What are the most relevance only of these 200 and then if we have the top 20 we want to have a semantic aggregation of the content of all 20 papers. No. So we say now the final top 20 documents are fed now as a context to an LLM and again you can choose your LLM that you like with a detailed prompt and then synthesization and the LLM writes it the final related works action. So clearly structured data processing pipeline and you can switch on and off any LLM in any place that you want and they tested here variation of different LLMs and they say so what are now the best performance combination so you see pipeline performing research this is great they run this here with a GPD4 as the default and then they exchange and said hey what happens if I add here let's say an 03 or a cloud model to GPD4.1 one all the prompts for this single steps that you just noticed here you have here in detail in the annex here of the documentation of this new research paper like here I show you the revised ODS react agent prompt here for calling here a tool so a lot of additional data I just want to give you the result this is it again you see one two three dimension and then all the different sub indicator in this dimension. Yeah, I think we can forget about the opensource research system from 2014. 
Relevance score: -8.474

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: And there we have it. A new research paper by Stanford University UC Berkeley about AI doing here the research the literature review here on brand new topics live in a multi- aent network deep scholar to research and synthesize knowledge. And if you are in science this could be a real benefit, a real help. 
Relevance score: -10.120

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Great. So verifiability here open research scores shockingly low. This is what Stanford and UC Berkeler tells us. Citation precision numerical value is only 39. The claim coverage is only.13. And they tell us this is one of the worst performers here. that why is this happening? They suggest here a system optimized for fluent plausible sounding text is great on the one hand. So it's excellent in synthesizing here story or story line. But if you work in science, if you want to be absolute precise, you want to reduce hallucinating to the max and you want to make sure making logical leaps that are not strictly supported by the cited sources of archive are not part of your document. Now unfortunately this is happening here with open deep research. So for purely scientific work where you have to have absolute precision this could turn out to be a fatal flaw. Now, interestingly, if we look here at the new matter, the deep scholar base as I just showed you and we put here an 03 as the engine into this benchmark, you see, interestingly for the knowledge synthesis, we have the exact same benchmark 857.857. If we look here at the nugget, we even have a better performance here at the sequential tokenization if you want. And and this is now interesting with this new method. Look at the variability from one we jump to 6. So yeah, wow. This is an impressive improvement against the commercial openi deep research performance. So this new method by Stanford and UC Berkeley even if you use it with an openi model you see in one of the indicators they are at least as good as the commercial models but they outperform the commercial model significantly in variability. Isn't this beautiful? And if you ask yourself hey why I have suddenly two LLMs I told you you can have multiple LLMs here in the pipeline. So they decided for the SAM filter and the SAM top K data they go with GBD 4.1 but for the heavyduty SAM the aggregation step they go here with an 03 or a Claude or a Gemini just to make sure we on the same page. So this is the process. So we have the filter process then the top K the top 20 papers then the final aggregation the final section generation and I told you I show you always the prompt and here you have the prompt. So whatever LLM you choose an 03 or a claude OPUS 4 or a Gemini 2.5 Pro this is here the prompt for the aggregation for the final aggregation here of your research. So now we can see the paper here highlights here the deep scholar base can achieve up to 6.3 times higher claim coverage. So one indicator and one dimension six times three better than openi deep research. Nice. So we do have new systems that really outperform the classical commercial blackbox systems. But on the other hand, if you want just a plausible, well-written draft and you are not in science and you are willing, if you are in science, to meticulously fact check every single claim, every single half sentence that EI writes for you, then if you as a human do this verification and correct it, then open EI is really maybe also interesting. However, if you are a scientist and you say, I will go with the AI system that I can trust at least partially, then the deep scholar base here with claude or gemini model as the engine is dramatically superior. If you look at the data from Stanford and UC Berkeley, the paper states also that no method achieves a score greater than.19 when you average across the metric. So this is not an incremental challenge. You know, go from 78% to 82%. No, we at 19%. This is the best model we have. So we have quite a monumental challenge in front of us to make our AI models better, more performance, more power. So there's a lot of work in front of us. And if you say okay, I don't like here the numerical values in a table. I want to see this here in a visualization. Here you have exactly all the different elements here in um yeah to radar visualization. But this is exactly the data here from the numerical table I just showed you. But if you prefer to have it here in the radar, beautiful. Here is it. 
Relevance score: -10.235

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: 85. So I give you here my summary. OpenI is really here the leader in crafting a well ststructured and seemingly comprehensive narrative. Open deep research really excels in this synthesis and in the retrieval task and we have here the numerical data for the highest organization score 85 and the highest nugget coverage 39. output are well organized and capture really the essential fact better than the competitors can perform. And for the retrieval quality also the winner here. If you look here at the relevance rate 62 covers the most important human-sided reference with a reference coins of8 and finds the most impactful paper.1 but and this is now a big but the performance of open research collapses in the third dimension and this is the most crucial aspect if you have some scientific writing trust and verifiability. If you just write something that is not verifiable. 
Relevance score: -10.775
    
------------------------------------------------------------
---------- Query at 2025-09-04T02:14:19.099913 ----------

        Answer the user's query based on the retrieved context.
        The context is derived from youtube vidoe transcripts which person is telling about the user query.

        ## User Query: 
        what is multi-ent search system

        ## Context:
        Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Hello community so great that you are back. We have a new paper by Stanford University and UC Berkeley and it is all about science and a deep scholar AI system that is brand new. Now you know we have open-source research system no that do a research and synthesize known facts and new findings and you are familiar with this no for example here open scholar this is here the paper or if you go for another open source this is here storm or maybe you are familiar with deep researcher from April 2025 so we do have open systems but of course it's interesting how do they compare to the commercial system let's say openi deep research or we We have brand new search EIS where you remember MCP and here multi- aent system. So how do they compare here to multi- aent search EI systems in well let's say you do a research on the internet. Now for those we have now the study where they compare Llama forced to GPD 4.1 and 03 an Opus 4 by Claude and a Gemini 2.5 Pro. And if you ask me why not some other models, those are the set of models Stanford and UC Berkeley decided to perform their tests on. This is it. The framework they used here is here open deep search. The search with open-source reasoning agents and you remember we have here multi- aent system. We have an open reasoning agent here itself that interprets here the given human task and completes it here by orchestrating a sequence of action and one of those action might include here calling tools and one of those tools that is available to this agent is here the open search tool to connect to the internet and everything. So a multi- aent search utility and now we have a new paper, we have a new method, we have some new research benchmark results and some new ideas. So what is the task that we are looking at? Let's say you're a researcher and you have your title and maybe you have already an idea from a technical abstract. Yeah. And then you want to ask AI to write here the related work section of your publication for you. In the old times we call this a literature review. Have a look who published what in your particular specific domain, when it was published, what were the topics, how was it related to your work. Now everything should be done by the eye. Now this new system is called the deep scholar benchmark and yes it promises to generate here the deep the related work section the deep research on internet on archive on meta archive for you. So what it needs three capabilities you are looking for the retrieval. So you want to have a vast corpus the live internet or the live archive whatever you have then you want to have a synthesis report. So everything all the data that were replied here from multiple sources weave them together the findings into a coherent long form narrative and then you want and this is the most important constraint absolute verifiability. So whatever you are claiming here and whatever you are citing those must really exist. So the citing sources must be accurate on every claim and must be back traced back to its origin. You must have references. Great. So the authors here Stanford and UC Berkeley built here an automated data pipeline directly and they said you know what we not just for the internet let's go here for science let's go to archive. So we want to have a fresh data set retrieved absolutely fresh here daily or every minute here from archive. So how we do this? So three topics the recency the quality control and diversity and here they also limited theirel to computer science as a topic and they made sure that they have a diverse top with 18 different domains in computer science. So yeah we are restricted to computer science but I think you will see the results they speak for themselves. Here's the paper published today August 27, 2025. Deep scholar bench live benchmark on automated evolation for generative research systems from Stanford University in UC Berkeley. And the beauty is not that they have now a new benchmark, but what the new benchmark shows us. It shows us the true difficulty AI systems have if they do have to do a research synthesis. Here's the result. I would say wait a minute explain here at first the metric and the benchmark methodology. Okay. So they decided to go here for a threedimensional differentiation to say we look for the knowledge synthesis of this new system for the retrieval quality. It's more or less a rack system. No. And then for the verifiability because we are working in science with archive. It must be really that we can site real sources and there's no hallucination. Great. And then you can see even in those three dimension they decided to go with two or three sub indicators. So let's have a look. So for the knowledge synthesis or what I call hey how well is this generated EI text written? Is it informative? They have two indicators. the organization where an LLM as a J performs here a pairwise comparison and then they have here a metric that assesses here the informationational completeness. So what you do you have now from your text here a human written um segment here and this is automatically if you want chucked up here by the eye so you get single sentences for example but a human like red at all introduced here the transformer architecture which relies here on a self attention mechanism now the eye might say hey this is a two long sentence there are too many objects too many subjects I reduce this I chunk it up here in nuggets so the first nugget is redford at introduce the transformer stop. This is it. So simple to verify and then it goes on and you see exactly what's happening. Now you have then this essential nuggets and you want to make sure that those really represent here in the I generated text what is the main content of the human written sentence or paragraph or paper. Then second dimension the retrieval quality. Simple in my words. Did the I system find the right papers on archive for my topic. Here we have three indicators. The relevance rate, the reference coverage or simply hey if the system found here the real important papers and you know importance is just one but we can also measure here how impactful this is with an impact um quote. So we have a document importance metric. So this is a proxy for retrieving impactful word that uses here the citation comes from a specific API. Great. And finally the most important for me we have to have verifiability. So whatever we claim there is a check against some other published work. So are the claims now actually supported here by the citation? And here they go with two indicators. the citation precision where they say hey does the cited source support at least one claim made in the eye sentence just one and then they are say hey let's go full power now now for the term claim coverage we want do the cited sources support all the claims made in the eye sentence so it's really every little detail that the eye writes out here supported here by let's say arrive references Great. Then they decided, you know, so we have now the idea of the benchmark, we have the metric and the subindicator of the benchmark. And they say, you know what, now we build a vehicle. So we can switch now the engine of this vehicle. So we can put in either a GPT 4.1 or we can put in a GPD4 Omni or we go with a clawed model as the engine. And they built here the deep scholar base. Now this was interesting because remember I told you this is your almost rag system. No they went here with a classical rag multi multi- aent structure. So at first you have the query generation. So an LLM generates now a set of diverse search query based on the human input. So whatever I have the AI the LLM tells me hey that was a little bit too complicated. I generate now a set of diver search queries and then I send this up. So those queries now AI generated queries now run against the search API in our example archive and then we have a filtering now a semantic filtering and now we use another LLM is used to filter here to retrieve documents and anything that is irrelevant we just want to ignore this no but we have still a lot of documents like say 200 documents left now after the filtering so we have to do a reranking because we want to go down to the top 20 no so we have another llm based step that now reranks in detail to the only 200 remaining documents and it's a new prioritization here. What are the most relevance only of these 200 and then if we have the top 20 we want to have a semantic aggregation of the content of all 20 papers. No. So we say now the final top 20 documents are fed now as a context to an LLM and again you can choose your LLM that you like with a detailed prompt and then synthesization and the LLM writes it the final related works action. So clearly structured data processing pipeline and you can switch on and off any LLM in any place that you want and they tested here variation of different LLMs and they say so what are now the best performance combination so you see pipeline performing research this is great they run this here with a GPD4 as the default and then they exchange and said hey what happens if I add here let's say an 03 or a cloud model to GPD4.1 one all the prompts for this single steps that you just noticed here you have here in detail in the annex here of the documentation of this new research paper like here I show you the revised ODS react agent prompt here for calling here a tool so a lot of additional data I just want to give you the result this is it again you see one two three dimension and then all the different sub indicator in this dimension. Yeah, I think we can forget about the opensource research system from 2014. 
Relevance score: -6.327

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: But just a warning, you always have to check your resources. There's always a possibility, even with the best current systems, that the system is hallucinating. It is referencing to archive preprints that are non-existent or it is coming up with an explanation that is logically incorrect. But otherwise, enjoy AI, subscribe to the channel, and I see you in the next video.
Relevance score: -10.779

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Not up to challenge anymore. Let's look at a commercial system before we go to the pure search. AI commercial system is they tested here one. They said this is kind of one of the best OpenIP research. And if you look at the knowledge synthesis, wow, this is really impressive here. 
Relevance score: -10.926

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Great. So verifiability here open research scores shockingly low. This is what Stanford and UC Berkeler tells us. Citation precision numerical value is only 39. The claim coverage is only.13. And they tell us this is one of the worst performers here. that why is this happening? They suggest here a system optimized for fluent plausible sounding text is great on the one hand. So it's excellent in synthesizing here story or story line. But if you work in science, if you want to be absolute precise, you want to reduce hallucinating to the max and you want to make sure making logical leaps that are not strictly supported by the cited sources of archive are not part of your document. Now unfortunately this is happening here with open deep research. So for purely scientific work where you have to have absolute precision this could turn out to be a fatal flaw. Now, interestingly, if we look here at the new matter, the deep scholar base as I just showed you and we put here an 03 as the engine into this benchmark, you see, interestingly for the knowledge synthesis, we have the exact same benchmark 857.857. If we look here at the nugget, we even have a better performance here at the sequential tokenization if you want. And and this is now interesting with this new method. Look at the variability from one we jump to 6. So yeah, wow. This is an impressive improvement against the commercial openi deep research performance. So this new method by Stanford and UC Berkeley even if you use it with an openi model you see in one of the indicators they are at least as good as the commercial models but they outperform the commercial model significantly in variability. Isn't this beautiful? And if you ask yourself hey why I have suddenly two LLMs I told you you can have multiple LLMs here in the pipeline. So they decided for the SAM filter and the SAM top K data they go with GBD 4.1 but for the heavyduty SAM the aggregation step they go here with an 03 or a Claude or a Gemini just to make sure we on the same page. So this is the process. So we have the filter process then the top K the top 20 papers then the final aggregation the final section generation and I told you I show you always the prompt and here you have the prompt. So whatever LLM you choose an 03 or a claude OPUS 4 or a Gemini 2.5 Pro this is here the prompt for the aggregation for the final aggregation here of your research. So now we can see the paper here highlights here the deep scholar base can achieve up to 6.3 times higher claim coverage. So one indicator and one dimension six times three better than openi deep research. Nice. So we do have new systems that really outperform the classical commercial blackbox systems. But on the other hand, if you want just a plausible, well-written draft and you are not in science and you are willing, if you are in science, to meticulously fact check every single claim, every single half sentence that EI writes for you, then if you as a human do this verification and correct it, then open EI is really maybe also interesting. However, if you are a scientist and you say, I will go with the AI system that I can trust at least partially, then the deep scholar base here with claude or gemini model as the engine is dramatically superior. If you look at the data from Stanford and UC Berkeley, the paper states also that no method achieves a score greater than.19 when you average across the metric. So this is not an incremental challenge. You know, go from 78% to 82%. No, we at 19%. This is the best model we have. So we have quite a monumental challenge in front of us to make our AI models better, more performance, more power. So there's a lot of work in front of us. And if you say okay, I don't like here the numerical values in a table. I want to see this here in a visualization. Here you have exactly all the different elements here in um yeah to radar visualization. But this is exactly the data here from the numerical table I just showed you. But if you prefer to have it here in the radar, beautiful. Here is it. 
Relevance score: -10.751

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: 85. So I give you here my summary. OpenI is really here the leader in crafting a well ststructured and seemingly comprehensive narrative. Open deep research really excels in this synthesis and in the retrieval task and we have here the numerical data for the highest organization score 85 and the highest nugget coverage 39. output are well organized and capture really the essential fact better than the competitors can perform. And for the retrieval quality also the winner here. If you look here at the relevance rate 62 covers the most important human-sided reference with a reference coins of8 and finds the most impactful paper.1 but and this is now a big but the performance of open research collapses in the third dimension and this is the most crucial aspect if you have some scientific writing trust and verifiability. If you just write something that is not verifiable. 
Relevance score: -10.775
    
------------------------------------------------------------
---------- Query at 2025-09-04T02:33:32.091909 ----------

        Answer the user's query based on the retrieved context.
        The context is derived from youtube vidoe transcripts which person is telling about the user query.

        ## User Query: 
        how to work

        ## Context:
        Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Great. So verifiability here open research scores shockingly low. This is what Stanford and UC Berkeler tells us. Citation precision numerical value is only 39. The claim coverage is only.13. And they tell us this is one of the worst performers here. that why is this happening? They suggest here a system optimized for fluent plausible sounding text is great on the one hand. So it's excellent in synthesizing here story or story line. But if you work in science, if you want to be absolute precise, you want to reduce hallucinating to the max and you want to make sure making logical leaps that are not strictly supported by the cited sources of archive are not part of your document. Now unfortunately this is happening here with open deep research. So for purely scientific work where you have to have absolute precision this could turn out to be a fatal flaw. Now, interestingly, if we look here at the new matter, the deep scholar base as I just showed you and we put here an 03 as the engine into this benchmark, you see, interestingly for the knowledge synthesis, we have the exact same benchmark 857.857. If we look here at the nugget, we even have a better performance here at the sequential tokenization if you want. And and this is now interesting with this new method. Look at the variability from one we jump to 6. So yeah, wow. This is an impressive improvement against the commercial openi deep research performance. So this new method by Stanford and UC Berkeley even if you use it with an openi model you see in one of the indicators they are at least as good as the commercial models but they outperform the commercial model significantly in variability. Isn't this beautiful? And if you ask yourself hey why I have suddenly two LLMs I told you you can have multiple LLMs here in the pipeline. So they decided for the SAM filter and the SAM top K data they go with GBD 4.1 but for the heavyduty SAM the aggregation step they go here with an 03 or a Claude or a Gemini just to make sure we on the same page. So this is the process. So we have the filter process then the top K the top 20 papers then the final aggregation the final section generation and I told you I show you always the prompt and here you have the prompt. So whatever LLM you choose an 03 or a claude OPUS 4 or a Gemini 2.5 Pro this is here the prompt for the aggregation for the final aggregation here of your research. So now we can see the paper here highlights here the deep scholar base can achieve up to 6.3 times higher claim coverage. So one indicator and one dimension six times three better than openi deep research. Nice. So we do have new systems that really outperform the classical commercial blackbox systems. But on the other hand, if you want just a plausible, well-written draft and you are not in science and you are willing, if you are in science, to meticulously fact check every single claim, every single half sentence that EI writes for you, then if you as a human do this verification and correct it, then open EI is really maybe also interesting. However, if you are a scientist and you say, I will go with the AI system that I can trust at least partially, then the deep scholar base here with claude or gemini model as the engine is dramatically superior. If you look at the data from Stanford and UC Berkeley, the paper states also that no method achieves a score greater than.19 when you average across the metric. So this is not an incremental challenge. You know, go from 78% to 82%. No, we at 19%. This is the best model we have. So we have quite a monumental challenge in front of us to make our AI models better, more performance, more power. So there's a lot of work in front of us. And if you say okay, I don't like here the numerical values in a table. I want to see this here in a visualization. Here you have exactly all the different elements here in um yeah to radar visualization. But this is exactly the data here from the numerical table I just showed you. But if you prefer to have it here in the radar, beautiful. Here is it. 
Relevance score: -9.199

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Hello community so great that you are back. We have a new paper by Stanford University and UC Berkeley and it is all about science and a deep scholar AI system that is brand new. Now you know we have open-source research system no that do a research and synthesize known facts and new findings and you are familiar with this no for example here open scholar this is here the paper or if you go for another open source this is here storm or maybe you are familiar with deep researcher from April 2025 so we do have open systems but of course it's interesting how do they compare to the commercial system let's say openi deep research or we We have brand new search EIS where you remember MCP and here multi- aent system. So how do they compare here to multi- aent search EI systems in well let's say you do a research on the internet. Now for those we have now the study where they compare Llama forced to GPD 4.1 and 03 an Opus 4 by Claude and a Gemini 2.5 Pro. And if you ask me why not some other models, those are the set of models Stanford and UC Berkeley decided to perform their tests on. This is it. The framework they used here is here open deep search. The search with open-source reasoning agents and you remember we have here multi- aent system. We have an open reasoning agent here itself that interprets here the given human task and completes it here by orchestrating a sequence of action and one of those action might include here calling tools and one of those tools that is available to this agent is here the open search tool to connect to the internet and everything. So a multi- aent search utility and now we have a new paper, we have a new method, we have some new research benchmark results and some new ideas. So what is the task that we are looking at? Let's say you're a researcher and you have your title and maybe you have already an idea from a technical abstract. Yeah. And then you want to ask AI to write here the related work section of your publication for you. In the old times we call this a literature review. Have a look who published what in your particular specific domain, when it was published, what were the topics, how was it related to your work. Now everything should be done by the eye. Now this new system is called the deep scholar benchmark and yes it promises to generate here the deep the related work section the deep research on internet on archive on meta archive for you. So what it needs three capabilities you are looking for the retrieval. So you want to have a vast corpus the live internet or the live archive whatever you have then you want to have a synthesis report. So everything all the data that were replied here from multiple sources weave them together the findings into a coherent long form narrative and then you want and this is the most important constraint absolute verifiability. So whatever you are claiming here and whatever you are citing those must really exist. So the citing sources must be accurate on every claim and must be back traced back to its origin. You must have references. Great. So the authors here Stanford and UC Berkeley built here an automated data pipeline directly and they said you know what we not just for the internet let's go here for science let's go to archive. So we want to have a fresh data set retrieved absolutely fresh here daily or every minute here from archive. So how we do this? So three topics the recency the quality control and diversity and here they also limited theirel to computer science as a topic and they made sure that they have a diverse top with 18 different domains in computer science. So yeah we are restricted to computer science but I think you will see the results they speak for themselves. Here's the paper published today August 27, 2025. Deep scholar bench live benchmark on automated evolation for generative research systems from Stanford University in UC Berkeley. And the beauty is not that they have now a new benchmark, but what the new benchmark shows us. It shows us the true difficulty AI systems have if they do have to do a research synthesis. Here's the result. I would say wait a minute explain here at first the metric and the benchmark methodology. Okay. So they decided to go here for a threedimensional differentiation to say we look for the knowledge synthesis of this new system for the retrieval quality. It's more or less a rack system. No. And then for the verifiability because we are working in science with archive. It must be really that we can site real sources and there's no hallucination. Great. And then you can see even in those three dimension they decided to go with two or three sub indicators. So let's have a look. So for the knowledge synthesis or what I call hey how well is this generated EI text written? Is it informative? They have two indicators. the organization where an LLM as a J performs here a pairwise comparison and then they have here a metric that assesses here the informationational completeness. So what you do you have now from your text here a human written um segment here and this is automatically if you want chucked up here by the eye so you get single sentences for example but a human like red at all introduced here the transformer architecture which relies here on a self attention mechanism now the eye might say hey this is a two long sentence there are too many objects too many subjects I reduce this I chunk it up here in nuggets so the first nugget is redford at introduce the transformer stop. This is it. So simple to verify and then it goes on and you see exactly what's happening. Now you have then this essential nuggets and you want to make sure that those really represent here in the I generated text what is the main content of the human written sentence or paragraph or paper. Then second dimension the retrieval quality. Simple in my words. Did the I system find the right papers on archive for my topic. Here we have three indicators. The relevance rate, the reference coverage or simply hey if the system found here the real important papers and you know importance is just one but we can also measure here how impactful this is with an impact um quote. So we have a document importance metric. So this is a proxy for retrieving impactful word that uses here the citation comes from a specific API. Great. And finally the most important for me we have to have verifiability. So whatever we claim there is a check against some other published work. So are the claims now actually supported here by the citation? And here they go with two indicators. the citation precision where they say hey does the cited source support at least one claim made in the eye sentence just one and then they are say hey let's go full power now now for the term claim coverage we want do the cited sources support all the claims made in the eye sentence so it's really every little detail that the eye writes out here supported here by let's say arrive references Great. Then they decided, you know, so we have now the idea of the benchmark, we have the metric and the subindicator of the benchmark. And they say, you know what, now we build a vehicle. So we can switch now the engine of this vehicle. So we can put in either a GPT 4.1 or we can put in a GPD4 Omni or we go with a clawed model as the engine. And they built here the deep scholar base. Now this was interesting because remember I told you this is your almost rag system. No they went here with a classical rag multi multi- aent structure. So at first you have the query generation. So an LLM generates now a set of diverse search query based on the human input. So whatever I have the AI the LLM tells me hey that was a little bit too complicated. I generate now a set of diver search queries and then I send this up. So those queries now AI generated queries now run against the search API in our example archive and then we have a filtering now a semantic filtering and now we use another LLM is used to filter here to retrieve documents and anything that is irrelevant we just want to ignore this no but we have still a lot of documents like say 200 documents left now after the filtering so we have to do a reranking because we want to go down to the top 20 no so we have another llm based step that now reranks in detail to the only 200 remaining documents and it's a new prioritization here. What are the most relevance only of these 200 and then if we have the top 20 we want to have a semantic aggregation of the content of all 20 papers. No. So we say now the final top 20 documents are fed now as a context to an LLM and again you can choose your LLM that you like with a detailed prompt and then synthesization and the LLM writes it the final related works action. So clearly structured data processing pipeline and you can switch on and off any LLM in any place that you want and they tested here variation of different LLMs and they say so what are now the best performance combination so you see pipeline performing research this is great they run this here with a GPD4 as the default and then they exchange and said hey what happens if I add here let's say an 03 or a cloud model to GPD4.1 one all the prompts for this single steps that you just noticed here you have here in detail in the annex here of the documentation of this new research paper like here I show you the revised ODS react agent prompt here for calling here a tool so a lot of additional data I just want to give you the result this is it again you see one two three dimension and then all the different sub indicator in this dimension. Yeah, I think we can forget about the opensource research system from 2014. 
Relevance score: -9.991

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: 85. So I give you here my summary. OpenI is really here the leader in crafting a well ststructured and seemingly comprehensive narrative. Open deep research really excels in this synthesis and in the retrieval task and we have here the numerical data for the highest organization score 85 and the highest nugget coverage 39. output are well organized and capture really the essential fact better than the competitors can perform. And for the retrieval quality also the winner here. If you look here at the relevance rate 62 covers the most important human-sided reference with a reference coins of8 and finds the most impactful paper.1 but and this is now a big but the performance of open research collapses in the third dimension and this is the most crucial aspect if you have some scientific writing trust and verifiability. If you just write something that is not verifiable. 
Relevance score: -10.074
    
------------------------------------------------------------
---------- Query at 2025-09-04T02:35:00.049483 ----------

        Answer the user's query based on the retrieved context.
        The context is derived from youtube vidoe transcripts which person is telling about the user query.

        ## User Query: 
        search architecture

        ## Context:
        Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Hello community so great that you are back. We have a new paper by Stanford University and UC Berkeley and it is all about science and a deep scholar AI system that is brand new. Now you know we have open-source research system no that do a research and synthesize known facts and new findings and you are familiar with this no for example here open scholar this is here the paper or if you go for another open source this is here storm or maybe you are familiar with deep researcher from April 2025 so we do have open systems but of course it's interesting how do they compare to the commercial system let's say openi deep research or we We have brand new search EIS where you remember MCP and here multi- aent system. So how do they compare here to multi- aent search EI systems in well let's say you do a research on the internet. Now for those we have now the study where they compare Llama forced to GPD 4.1 and 03 an Opus 4 by Claude and a Gemini 2.5 Pro. And if you ask me why not some other models, those are the set of models Stanford and UC Berkeley decided to perform their tests on. This is it. The framework they used here is here open deep search. The search with open-source reasoning agents and you remember we have here multi- aent system. We have an open reasoning agent here itself that interprets here the given human task and completes it here by orchestrating a sequence of action and one of those action might include here calling tools and one of those tools that is available to this agent is here the open search tool to connect to the internet and everything. So a multi- aent search utility and now we have a new paper, we have a new method, we have some new research benchmark results and some new ideas. So what is the task that we are looking at? Let's say you're a researcher and you have your title and maybe you have already an idea from a technical abstract. Yeah. And then you want to ask AI to write here the related work section of your publication for you. In the old times we call this a literature review. Have a look who published what in your particular specific domain, when it was published, what were the topics, how was it related to your work. Now everything should be done by the eye. Now this new system is called the deep scholar benchmark and yes it promises to generate here the deep the related work section the deep research on internet on archive on meta archive for you. So what it needs three capabilities you are looking for the retrieval. So you want to have a vast corpus the live internet or the live archive whatever you have then you want to have a synthesis report. So everything all the data that were replied here from multiple sources weave them together the findings into a coherent long form narrative and then you want and this is the most important constraint absolute verifiability. So whatever you are claiming here and whatever you are citing those must really exist. So the citing sources must be accurate on every claim and must be back traced back to its origin. You must have references. Great. So the authors here Stanford and UC Berkeley built here an automated data pipeline directly and they said you know what we not just for the internet let's go here for science let's go to archive. So we want to have a fresh data set retrieved absolutely fresh here daily or every minute here from archive. So how we do this? So three topics the recency the quality control and diversity and here they also limited theirel to computer science as a topic and they made sure that they have a diverse top with 18 different domains in computer science. So yeah we are restricted to computer science but I think you will see the results they speak for themselves. Here's the paper published today August 27, 2025. Deep scholar bench live benchmark on automated evolation for generative research systems from Stanford University in UC Berkeley. And the beauty is not that they have now a new benchmark, but what the new benchmark shows us. It shows us the true difficulty AI systems have if they do have to do a research synthesis. Here's the result. I would say wait a minute explain here at first the metric and the benchmark methodology. Okay. So they decided to go here for a threedimensional differentiation to say we look for the knowledge synthesis of this new system for the retrieval quality. It's more or less a rack system. No. And then for the verifiability because we are working in science with archive. It must be really that we can site real sources and there's no hallucination. Great. And then you can see even in those three dimension they decided to go with two or three sub indicators. So let's have a look. So for the knowledge synthesis or what I call hey how well is this generated EI text written? Is it informative? They have two indicators. the organization where an LLM as a J performs here a pairwise comparison and then they have here a metric that assesses here the informationational completeness. So what you do you have now from your text here a human written um segment here and this is automatically if you want chucked up here by the eye so you get single sentences for example but a human like red at all introduced here the transformer architecture which relies here on a self attention mechanism now the eye might say hey this is a two long sentence there are too many objects too many subjects I reduce this I chunk it up here in nuggets so the first nugget is redford at introduce the transformer stop. This is it. So simple to verify and then it goes on and you see exactly what's happening. Now you have then this essential nuggets and you want to make sure that those really represent here in the I generated text what is the main content of the human written sentence or paragraph or paper. Then second dimension the retrieval quality. Simple in my words. Did the I system find the right papers on archive for my topic. Here we have three indicators. The relevance rate, the reference coverage or simply hey if the system found here the real important papers and you know importance is just one but we can also measure here how impactful this is with an impact um quote. So we have a document importance metric. So this is a proxy for retrieving impactful word that uses here the citation comes from a specific API. Great. And finally the most important for me we have to have verifiability. So whatever we claim there is a check against some other published work. So are the claims now actually supported here by the citation? And here they go with two indicators. the citation precision where they say hey does the cited source support at least one claim made in the eye sentence just one and then they are say hey let's go full power now now for the term claim coverage we want do the cited sources support all the claims made in the eye sentence so it's really every little detail that the eye writes out here supported here by let's say arrive references Great. Then they decided, you know, so we have now the idea of the benchmark, we have the metric and the subindicator of the benchmark. And they say, you know what, now we build a vehicle. So we can switch now the engine of this vehicle. So we can put in either a GPT 4.1 or we can put in a GPD4 Omni or we go with a clawed model as the engine. And they built here the deep scholar base. Now this was interesting because remember I told you this is your almost rag system. No they went here with a classical rag multi multi- aent structure. So at first you have the query generation. So an LLM generates now a set of diverse search query based on the human input. So whatever I have the AI the LLM tells me hey that was a little bit too complicated. I generate now a set of diver search queries and then I send this up. So those queries now AI generated queries now run against the search API in our example archive and then we have a filtering now a semantic filtering and now we use another LLM is used to filter here to retrieve documents and anything that is irrelevant we just want to ignore this no but we have still a lot of documents like say 200 documents left now after the filtering so we have to do a reranking because we want to go down to the top 20 no so we have another llm based step that now reranks in detail to the only 200 remaining documents and it's a new prioritization here. What are the most relevance only of these 200 and then if we have the top 20 we want to have a semantic aggregation of the content of all 20 papers. No. So we say now the final top 20 documents are fed now as a context to an LLM and again you can choose your LLM that you like with a detailed prompt and then synthesization and the LLM writes it the final related works action. So clearly structured data processing pipeline and you can switch on and off any LLM in any place that you want and they tested here variation of different LLMs and they say so what are now the best performance combination so you see pipeline performing research this is great they run this here with a GPD4 as the default and then they exchange and said hey what happens if I add here let's say an 03 or a cloud model to GPD4.1 one all the prompts for this single steps that you just noticed here you have here in detail in the annex here of the documentation of this new research paper like here I show you the revised ODS react agent prompt here for calling here a tool so a lot of additional data I just want to give you the result this is it again you see one two three dimension and then all the different sub indicator in this dimension. Yeah, I think we can forget about the opensource research system from 2014. 
Relevance score: -10.051

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Not up to challenge anymore. Let's look at a commercial system before we go to the pure search. AI commercial system is they tested here one. They said this is kind of one of the best OpenIP research. And if you look at the knowledge synthesis, wow, this is really impressive here. 
Relevance score: -11.255

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: 85. So I give you here my summary. OpenI is really here the leader in crafting a well ststructured and seemingly comprehensive narrative. Open deep research really excels in this synthesis and in the retrieval task and we have here the numerical data for the highest organization score 85 and the highest nugget coverage 39. output are well organized and capture really the essential fact better than the competitors can perform. And for the retrieval quality also the winner here. If you look here at the relevance rate 62 covers the most important human-sided reference with a reference coins of8 and finds the most impactful paper.1 but and this is now a big but the performance of open research collapses in the third dimension and this is the most crucial aspect if you have some scientific writing trust and verifiability. If you just write something that is not verifiable. 
Relevance score: -11.297

Source: https://www.youtube.com/watch?v=255n7yJcmdU
Content: Great. So verifiability here open research scores shockingly low. This is what Stanford and UC Berkeler tells us. Citation precision numerical value is only 39. The claim coverage is only.13. And they tell us this is one of the worst performers here. that why is this happening? They suggest here a system optimized for fluent plausible sounding text is great on the one hand. So it's excellent in synthesizing here story or story line. But if you work in science, if you want to be absolute precise, you want to reduce hallucinating to the max and you want to make sure making logical leaps that are not strictly supported by the cited sources of archive are not part of your document. Now unfortunately this is happening here with open deep research. So for purely scientific work where you have to have absolute precision this could turn out to be a fatal flaw. Now, interestingly, if we look here at the new matter, the deep scholar base as I just showed you and we put here an 03 as the engine into this benchmark, you see, interestingly for the knowledge synthesis, we have the exact same benchmark 857.857. If we look here at the nugget, we even have a better performance here at the sequential tokenization if you want. And and this is now interesting with this new method. Look at the variability from one we jump to 6. So yeah, wow. This is an impressive improvement against the commercial openi deep research performance. So this new method by Stanford and UC Berkeley even if you use it with an openi model you see in one of the indicators they are at least as good as the commercial models but they outperform the commercial model significantly in variability. Isn't this beautiful? And if you ask yourself hey why I have suddenly two LLMs I told you you can have multiple LLMs here in the pipeline. So they decided for the SAM filter and the SAM top K data they go with GBD 4.1 but for the heavyduty SAM the aggregation step they go here with an 03 or a Claude or a Gemini just to make sure we on the same page. So this is the process. So we have the filter process then the top K the top 20 papers then the final aggregation the final section generation and I told you I show you always the prompt and here you have the prompt. So whatever LLM you choose an 03 or a claude OPUS 4 or a Gemini 2.5 Pro this is here the prompt for the aggregation for the final aggregation here of your research. So now we can see the paper here highlights here the deep scholar base can achieve up to 6.3 times higher claim coverage. So one indicator and one dimension six times three better than openi deep research. Nice. So we do have new systems that really outperform the classical commercial blackbox systems. But on the other hand, if you want just a plausible, well-written draft and you are not in science and you are willing, if you are in science, to meticulously fact check every single claim, every single half sentence that EI writes for you, then if you as a human do this verification and correct it, then open EI is really maybe also interesting. However, if you are a scientist and you say, I will go with the AI system that I can trust at least partially, then the deep scholar base here with claude or gemini model as the engine is dramatically superior. If you look at the data from Stanford and UC Berkeley, the paper states also that no method achieves a score greater than.19 when you average across the metric. So this is not an incremental challenge. You know, go from 78% to 82%. No, we at 19%. This is the best model we have. So we have quite a monumental challenge in front of us to make our AI models better, more performance, more power. So there's a lot of work in front of us. And if you say okay, I don't like here the numerical values in a table. I want to see this here in a visualization. Here you have exactly all the different elements here in um yeah to radar visualization. But this is exactly the data here from the numerical table I just showed you. But if you prefer to have it here in the radar, beautiful. Here is it. 
Relevance score: -11.200
    
------------------------------------------------------------
---------- Query at 2025-09-04T17:53:51.270713 ----------

        Summarize the context which are parts of youtube video transcripts.
        Summarize considering the user query to give insight about context.

        ## User Query: 
        deepscholar methodology

        ## Context:
        Great. So verifiability here open research scores shockingly low. This is what Stanford and UC Berkeler tells us. Citation precision numerical value is only 39. The claim coverage is only.13. And they tell us this is one of the worst performers here. that why is this happening? They suggest here a system optimized for fluent plausible sounding text is great on the one hand. So it's excellent in synthesizing here story or story line. But if you work in science, if you want to be absolute precise, you want to reduce hallucinating to the max and you want to make sure making logical leaps that are not strictly supported by the cited sources of archive are not part of your document. Now unfortunately this is happening here with open deep research. So for purely scientific work where you have to have absolute precision this could turn out to be a fatal flaw. Now, interestingly, if we look here at the new matter, the deep scholar base as I just showed you and we put here an 03 as the engine into this benchmark, you see, interestingly for the knowledge synthesis, we have the exact same benchmark 857.857. If we look here at the nugget, we even have a better performance here at the sequential tokenization if you want. And and this is now interesting with this new method. Look at the variability from one we jump to 6. So yeah, wow. This is an impressive improvement against the commercial openi deep research performance. So this new method by Stanford and UC Berkeley even if you use it with an openi model you see in one of the indicators they are at least as good as the commercial models but they outperform the commercial model significantly in variability. Isn't this beautiful? And if you ask yourself hey why I have suddenly two LLMs I told you you can have multiple LLMs here in the pipeline. So they decided for the SAM filter and the SAM top K data they go with GBD 4.1 but for the heavyduty SAM the aggregation step they go here with an 03 or a Claude or a Gemini just to make sure we on the same page. So this is the process. So we have the filter process then the top K the top 20 papers then the final aggregation the final section generation and I told you I show you always the prompt and here you have the prompt. So whatever LLM you choose an 03 or a claude OPUS 4 or a Gemini 2.5 Pro this is here the prompt for the aggregation for the final aggregation here of your research. So now we can see the paper here highlights here the deep scholar base can achieve up to 6.3 times higher claim coverage. So one indicator and one dimension six times three better than openi deep research. Nice. So we do have new systems that really outperform the classical commercial blackbox systems. But on the other hand, if you want just a plausible, well-written draft and you are not in science and you are willing, if you are in science, to meticulously fact check every single claim, every single half sentence that EI writes for you, then if you as a human do this verification and correct it, then open EI is really maybe also interesting. However, if you are a scientist and you say, I will go with the AI system that I can trust at least partially, then the deep scholar base here with claude or gemini model as the engine is dramatically superior. If you look at the data from Stanford and UC Berkeley, the paper states also that no method achieves a score greater than.19 when you average across the metric. So this is not an incremental challenge. You know, go from 78% to 82%. No, we at 19%. This is the best model we have. So we have quite a monumental challenge in front of us to make our AI models better, more performance, more power. So there's a lot of work in front of us. And if you say okay, I don't like here the numerical values in a table. I want to see this here in a visualization. Here you have exactly all the different elements here in um yeah to radar visualization. But this is exactly the data here from the numerical table I just showed you. But if you prefer to have it here in the radar, beautiful. Here is it.  

Hello community so great that you are back. We have a new paper by Stanford University and UC Berkeley and it is all about science and a deep scholar AI system that is brand new. Now you know we have open-source research system no that do a research and synthesize known facts and new findings and you are familiar with this no for example here open scholar this is here the paper or if you go for another open source this is here storm or maybe you are familiar with deep researcher from April 2025 so we do have open systems but of course it's interesting how do they compare to the commercial system let's say openi deep research or we We have brand new search EIS where you remember MCP and here multi- aent system. So how do they compare here to multi- aent search EI systems in well let's say you do a research on the internet. Now for those we have now the study where they compare Llama forced to GPD 4.1 and 03 an Opus 4 by Claude and a Gemini 2.5 Pro. And if you ask me why not some other models, those are the set of models Stanford and UC Berkeley decided to perform their tests on. This is it. The framework they used here is here open deep search. The search with open-source reasoning agents and you remember we have here multi- aent system. We have an open reasoning agent here itself that interprets here the given human task and completes it here by orchestrating a sequence of action and one of those action might include here calling tools and one of those tools that is available to this agent is here the open search tool to connect to the internet and everything. So a multi- aent search utility and now we have a new paper, we have a new method, we have some new research benchmark results and some new ideas. So what is the task that we are looking at? Let's say you're a researcher and you have your title and maybe you have already an idea from a technical abstract. Yeah. And then you want to ask AI to write here the related work section of your publication for you. In the old times we call this a literature review. Have a look who published what in your particular specific domain, when it was published, what were the topics, how was it related to your work. Now everything should be done by the eye. Now this new system is called the deep scholar benchmark and yes it promises to generate here the deep the related work section the deep research on internet on archive on meta archive for you. So what it needs three capabilities you are looking for the retrieval. So you want to have a vast corpus the live internet or the live archive whatever you have then you want to have a synthesis report. So everything all the data that were replied here from multiple sources weave them together the findings into a coherent long form narrative and then you want and this is the most important constraint absolute verifiability. So whatever you are claiming here and whatever you are citing those must really exist. So the citing sources must be accurate on every claim and must be back traced back to its origin. You must have references. Great. So the authors here Stanford and UC Berkeley built here an automated data pipeline directly and they said you know what we not just for the internet let's go here for science let's go to archive. So we want to have a fresh data set retrieved absolutely fresh here daily or every minute here from archive. So how we do this? So three topics the recency the quality control and diversity and here they also limited theirel to computer science as a topic and they made sure that they have a diverse top with 18 different domains in computer science. So yeah we are restricted to computer science but I think you will see the results they speak for themselves. Here's the paper published today August 27, 2025. Deep scholar bench live benchmark on automated evolation for generative research systems from Stanford University in UC Berkeley. And the beauty is not that they have now a new benchmark, but what the new benchmark shows us. It shows us the true difficulty AI systems have if they do have to do a research synthesis. Here's the result. I would say wait a minute explain here at first the metric and the benchmark methodology. Okay. So they decided to go here for a threedimensional differentiation to say we look for the knowledge synthesis of this new system for the retrieval quality. It's more or less a rack system. No. And then for the verifiability because we are working in science with archive. It must be really that we can site real sources and there's no hallucination. Great. And then you can see even in those three dimension they decided to go with two or three sub indicators. So let's have a look. So for the knowledge synthesis or what I call hey how well is this generated EI text written? Is it informative? They have two indicators. the organization where an LLM as a J performs here a pairwise comparison and then they have here a metric that assesses here the informationational completeness. So what you do you have now from your text here a human written um segment here and this is automatically if you want chucked up here by the eye so you get single sentences for example but a human like red at all introduced here the transformer architecture which relies here on a self attention mechanism now the eye might say hey this is a two long sentence there are too many objects too many subjects I reduce this I chunk it up here in nuggets so the first nugget is redford at introduce the transformer stop. This is it. So simple to verify and then it goes on and you see exactly what's happening. Now you have then this essential nuggets and you want to make sure that those really represent here in the I generated text what is the main content of the human written sentence or paragraph or paper. Then second dimension the retrieval quality. Simple in my words. Did the I system find the right papers on archive for my topic. Here we have three indicators. The relevance rate, the reference coverage or simply hey if the system found here the real important papers and you know importance is just one but we can also measure here how impactful this is with an impact um quote. So we have a document importance metric. So this is a proxy for retrieving impactful word that uses here the citation comes from a specific API. Great. And finally the most important for me we have to have verifiability. So whatever we claim there is a check against some other published work. So are the claims now actually supported here by the citation? And here they go with two indicators. the citation precision where they say hey does the cited source support at least one claim made in the eye sentence just one and then they are say hey let's go full power now now for the term claim coverage we want do the cited sources support all the claims made in the eye sentence so it's really every little detail that the eye writes out here supported here by let's say arrive references Great. Then they decided, you know, so we have now the idea of the benchmark, we have the metric and the subindicator of the benchmark. And they say, you know what, now we build a vehicle. So we can switch now the engine of this vehicle. So we can put in either a GPT 4.1 or we can put in a GPD4 Omni or we go with a clawed model as the engine. And they built here the deep scholar base. Now this was interesting because remember I told you this is your almost rag system. No they went here with a classical rag multi multi- aent structure. So at first you have the query generation. So an LLM generates now a set of diverse search query based on the human input. So whatever I have the AI the LLM tells me hey that was a little bit too complicated. I generate now a set of diver search queries and then I send this up. So those queries now AI generated queries now run against the search API in our example archive and then we have a filtering now a semantic filtering and now we use another LLM is used to filter here to retrieve documents and anything that is irrelevant we just want to ignore this no but we have still a lot of documents like say 200 documents left now after the filtering so we have to do a reranking because we want to go down to the top 20 no so we have another llm based step that now reranks in detail to the only 200 remaining documents and it's a new prioritization here. What are the most relevance only of these 200 and then if we have the top 20 we want to have a semantic aggregation of the content of all 20 papers. No. So we say now the final top 20 documents are fed now as a context to an LLM and again you can choose your LLM that you like with a detailed prompt and then synthesization and the LLM writes it the final related works action. So clearly structured data processing pipeline and you can switch on and off any LLM in any place that you want and they tested here variation of different LLMs and they say so what are now the best performance combination so you see pipeline performing research this is great they run this here with a GPD4 as the default and then they exchange and said hey what happens if I add here let's say an 03 or a cloud model to GPD4.1 one all the prompts for this single steps that you just noticed here you have here in detail in the annex here of the documentation of this new research paper like here I show you the revised ODS react agent prompt here for calling here a tool so a lot of additional data I just want to give you the result this is it again you see one two three dimension and then all the different sub indicator in this dimension. Yeah, I think we can forget about the opensource research system from 2014.  

And there we have it. A new research paper by Stanford University UC Berkeley about AI doing here the research the literature review here on brand new topics live in a multi- aent network deep scholar to research and synthesize knowledge. And if you are in science this could be a real benefit, a real help.  

85. So I give you here my summary. OpenI is really here the leader in crafting a well ststructured and seemingly comprehensive narrative. Open deep research really excels in this synthesis and in the retrieval task and we have here the numerical data for the highest organization score 85 and the highest nugget coverage 39. output are well organized and capture really the essential fact better than the competitors can perform. And for the retrieval quality also the winner here. If you look here at the relevance rate 62 covers the most important human-sided reference with a reference coins of8 and finds the most impactful paper.1 but and this is now a big but the performance of open research collapses in the third dimension and this is the most crucial aspect if you have some scientific writing trust and verifiability. If you just write something that is not verifiable.  

Not up to challenge anymore. Let's look at a commercial system before we go to the pure search. AI commercial system is they tested here one. They said this is kind of one of the best OpenIP research. And if you look at the knowledge synthesis, wow, this is really impressive here.  
    
------------------------------------------------------------
---------- Query at 2025-09-04T17:59:00.025239 ----------

        Summarize the context which are parts of youtube video transcripts.
        Summarize considering the user query to give insight about context.

        ## User Query: 
        deepscholar methodology

        ## Context:
        Great. So verifiability here open research scores shockingly low. This is what Stanford and UC Berkeler tells us. Citation precision numerical value is only 39. The claim coverage is only.13. And they tell us this is one of the worst performers here. that why is this happening? They suggest here a system optimized for fluent plausible sounding text is great on the one hand. So it's excellent in synthesizing here story or story line. But if you work in science, if you want to be absolute precise, you want to reduce hallucinating to the max and you want to make sure making logical leaps that are not strictly supported by the cited sources of archive are not part of your document. Now unfortunately this is happening here with open deep research. So for purely scientific work where you have to have absolute precision this could turn out to be a fatal flaw. Now, interestingly, if we look here at the new matter, the deep scholar base as I just showed you and we put here an 03 as the engine into this benchmark, you see, interestingly for the knowledge synthesis, we have the exact same benchmark 857.857. If we look here at the nugget, we even have a better performance here at the sequential tokenization if you want. And and this is now interesting with this new method. Look at the variability from one we jump to 6. So yeah, wow. This is an impressive improvement against the commercial openi deep research performance. So this new method by Stanford and UC Berkeley even if you use it with an openi model you see in one of the indicators they are at least as good as the commercial models but they outperform the commercial model significantly in variability. Isn't this beautiful? And if you ask yourself hey why I have suddenly two LLMs I told you you can have multiple LLMs here in the pipeline. So they decided for the SAM filter and the SAM top K data they go with GBD 4.1 but for the heavyduty SAM the aggregation step they go here with an 03 or a Claude or a Gemini just to make sure we on the same page. So this is the process. So we have the filter process then the top K the top 20 papers then the final aggregation the final section generation and I told you I show you always the prompt and here you have the prompt. So whatever LLM you choose an 03 or a claude OPUS 4 or a Gemini 2.5 Pro this is here the prompt for the aggregation for the final aggregation here of your research. So now we can see the paper here highlights here the deep scholar base can achieve up to 6.3 times higher claim coverage. So one indicator and one dimension six times three better than openi deep research. Nice. So we do have new systems that really outperform the classical commercial blackbox systems. But on the other hand, if you want just a plausible, well-written draft and you are not in science and you are willing, if you are in science, to meticulously fact check every single claim, every single half sentence that EI writes for you, then if you as a human do this verification and correct it, then open EI is really maybe also interesting. However, if you are a scientist and you say, I will go with the AI system that I can trust at least partially, then the deep scholar base here with claude or gemini model as the engine is dramatically superior. If you look at the data from Stanford and UC Berkeley, the paper states also that no method achieves a score greater than.19 when you average across the metric. So this is not an incremental challenge. You know, go from 78% to 82%. No, we at 19%. This is the best model we have. So we have quite a monumental challenge in front of us to make our AI models better, more performance, more power. So there's a lot of work in front of us. And if you say okay, I don't like here the numerical values in a table. I want to see this here in a visualization. Here you have exactly all the different elements here in um yeah to radar visualization. But this is exactly the data here from the numerical table I just showed you. But if you prefer to have it here in the radar, beautiful. Here is it.  

Hello community so great that you are back. We have a new paper by Stanford University and UC Berkeley and it is all about science and a deep scholar AI system that is brand new. Now you know we have open-source research system no that do a research and synthesize known facts and new findings and you are familiar with this no for example here open scholar this is here the paper or if you go for another open source this is here storm or maybe you are familiar with deep researcher from April 2025 so we do have open systems but of course it's interesting how do they compare to the commercial system let's say openi deep research or we We have brand new search EIS where you remember MCP and here multi- aent system. So how do they compare here to multi- aent search EI systems in well let's say you do a research on the internet. Now for those we have now the study where they compare Llama forced to GPD 4.1 and 03 an Opus 4 by Claude and a Gemini 2.5 Pro. And if you ask me why not some other models, those are the set of models Stanford and UC Berkeley decided to perform their tests on. This is it. The framework they used here is here open deep search. The search with open-source reasoning agents and you remember we have here multi- aent system. We have an open reasoning agent here itself that interprets here the given human task and completes it here by orchestrating a sequence of action and one of those action might include here calling tools and one of those tools that is available to this agent is here the open search tool to connect to the internet and everything. So a multi- aent search utility and now we have a new paper, we have a new method, we have some new research benchmark results and some new ideas. So what is the task that we are looking at? Let's say you're a researcher and you have your title and maybe you have already an idea from a technical abstract. Yeah. And then you want to ask AI to write here the related work section of your publication for you. In the old times we call this a literature review. Have a look who published what in your particular specific domain, when it was published, what were the topics, how was it related to your work. Now everything should be done by the eye. Now this new system is called the deep scholar benchmark and yes it promises to generate here the deep the related work section the deep research on internet on archive on meta archive for you. So what it needs three capabilities you are looking for the retrieval. So you want to have a vast corpus the live internet or the live archive whatever you have then you want to have a synthesis report. So everything all the data that were replied here from multiple sources weave them together the findings into a coherent long form narrative and then you want and this is the most important constraint absolute verifiability. So whatever you are claiming here and whatever you are citing those must really exist. So the citing sources must be accurate on every claim and must be back traced back to its origin. You must have references. Great. So the authors here Stanford and UC Berkeley built here an automated data pipeline directly and they said you know what we not just for the internet let's go here for science let's go to archive. So we want to have a fresh data set retrieved absolutely fresh here daily or every minute here from archive. So how we do this? So three topics the recency the quality control and diversity and here they also limited theirel to computer science as a topic and they made sure that they have a diverse top with 18 different domains in computer science. So yeah we are restricted to computer science but I think you will see the results they speak for themselves. Here's the paper published today August 27, 2025. Deep scholar bench live benchmark on automated evolation for generative research systems from Stanford University in UC Berkeley. And the beauty is not that they have now a new benchmark, but what the new benchmark shows us. It shows us the true difficulty AI systems have if they do have to do a research synthesis. Here's the result. I would say wait a minute explain here at first the metric and the benchmark methodology. Okay. So they decided to go here for a threedimensional differentiation to say we look for the knowledge synthesis of this new system for the retrieval quality. It's more or less a rack system. No. And then for the verifiability because we are working in science with archive. It must be really that we can site real sources and there's no hallucination. Great. And then you can see even in those three dimension they decided to go with two or three sub indicators. So let's have a look. So for the knowledge synthesis or what I call hey how well is this generated EI text written? Is it informative? They have two indicators. the organization where an LLM as a J performs here a pairwise comparison and then they have here a metric that assesses here the informationational completeness. So what you do you have now from your text here a human written um segment here and this is automatically if you want chucked up here by the eye so you get single sentences for example but a human like red at all introduced here the transformer architecture which relies here on a self attention mechanism now the eye might say hey this is a two long sentence there are too many objects too many subjects I reduce this I chunk it up here in nuggets so the first nugget is redford at introduce the transformer stop. This is it. So simple to verify and then it goes on and you see exactly what's happening. Now you have then this essential nuggets and you want to make sure that those really represent here in the I generated text what is the main content of the human written sentence or paragraph or paper. Then second dimension the retrieval quality. Simple in my words. Did the I system find the right papers on archive for my topic. Here we have three indicators. The relevance rate, the reference coverage or simply hey if the system found here the real important papers and you know importance is just one but we can also measure here how impactful this is with an impact um quote. So we have a document importance metric. So this is a proxy for retrieving impactful word that uses here the citation comes from a specific API. Great. And finally the most important for me we have to have verifiability. So whatever we claim there is a check against some other published work. So are the claims now actually supported here by the citation? And here they go with two indicators. the citation precision where they say hey does the cited source support at least one claim made in the eye sentence just one and then they are say hey let's go full power now now for the term claim coverage we want do the cited sources support all the claims made in the eye sentence so it's really every little detail that the eye writes out here supported here by let's say arrive references Great. Then they decided, you know, so we have now the idea of the benchmark, we have the metric and the subindicator of the benchmark. And they say, you know what, now we build a vehicle. So we can switch now the engine of this vehicle. So we can put in either a GPT 4.1 or we can put in a GPD4 Omni or we go with a clawed model as the engine. And they built here the deep scholar base. Now this was interesting because remember I told you this is your almost rag system. No they went here with a classical rag multi multi- aent structure. So at first you have the query generation. So an LLM generates now a set of diverse search query based on the human input. So whatever I have the AI the LLM tells me hey that was a little bit too complicated. I generate now a set of diver search queries and then I send this up. So those queries now AI generated queries now run against the search API in our example archive and then we have a filtering now a semantic filtering and now we use another LLM is used to filter here to retrieve documents and anything that is irrelevant we just want to ignore this no but we have still a lot of documents like say 200 documents left now after the filtering so we have to do a reranking because we want to go down to the top 20 no so we have another llm based step that now reranks in detail to the only 200 remaining documents and it's a new prioritization here. What are the most relevance only of these 200 and then if we have the top 20 we want to have a semantic aggregation of the content of all 20 papers. No. So we say now the final top 20 documents are fed now as a context to an LLM and again you can choose your LLM that you like with a detailed prompt and then synthesization and the LLM writes it the final related works action. So clearly structured data processing pipeline and you can switch on and off any LLM in any place that you want and they tested here variation of different LLMs and they say so what are now the best performance combination so you see pipeline performing research this is great they run this here with a GPD4 as the default and then they exchange and said hey what happens if I add here let's say an 03 or a cloud model to GPD4.1 one all the prompts for this single steps that you just noticed here you have here in detail in the annex here of the documentation of this new research paper like here I show you the revised ODS react agent prompt here for calling here a tool so a lot of additional data I just want to give you the result this is it again you see one two three dimension and then all the different sub indicator in this dimension. Yeah, I think we can forget about the opensource research system from 2014.  

And there we have it. A new research paper by Stanford University UC Berkeley about AI doing here the research the literature review here on brand new topics live in a multi- aent network deep scholar to research and synthesize knowledge. And if you are in science this could be a real benefit, a real help.  

85. So I give you here my summary. OpenI is really here the leader in crafting a well ststructured and seemingly comprehensive narrative. Open deep research really excels in this synthesis and in the retrieval task and we have here the numerical data for the highest organization score 85 and the highest nugget coverage 39. output are well organized and capture really the essential fact better than the competitors can perform. And for the retrieval quality also the winner here. If you look here at the relevance rate 62 covers the most important human-sided reference with a reference coins of8 and finds the most impactful paper.1 but and this is now a big but the performance of open research collapses in the third dimension and this is the most crucial aspect if you have some scientific writing trust and verifiability. If you just write something that is not verifiable.  

Not up to challenge anymore. Let's look at a commercial system before we go to the pure search. AI commercial system is they tested here one. They said this is kind of one of the best OpenIP research. And if you look at the knowledge synthesis, wow, this is really impressive here.  
    
------------------------------------------------------------
